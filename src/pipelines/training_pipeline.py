"""WMTP í†µí•© í›ˆë ¨ íŒŒì´í”„ë¼ì¸ - ëª¨ë“  ì•Œê³ ë¦¬ì¦˜ì˜ í•µì‹¬ ì‹¤í–‰ ì—”ì§„.

ì´ íŒŒì´í”„ë¼ì¸ì€ WMTPì˜ í•µì‹¬ ì•„ì´ë””ì–´ë¥¼ ì‹¤í˜„í•˜ëŠ” í†µí•© ì‹¤í–‰ ì—”ì§„ì…ë‹ˆë‹¤.
ì„¸ ê°€ì§€ ì•Œê³ ë¦¬ì¦˜(mtp-baseline, critic-wmtp, rho1-wmtp) ëª¨ë‘ê°€ ë™ì¼í•œ
íŒŒì´í”„ë¼ì¸ì„ ì‚¬ìš©í•˜ë˜, Factory íŒ¨í„´ì„ í†µí•´ ë‹¤ë¥¸ ì»´í¬ë„ŒíŠ¸ë¥¼ ì¡°í•©í•©ë‹ˆë‹¤.

íŒŒì´í”„ë¼ì¸ ì„¤ê³„ ì›ì¹™:
  1. ì–´ì…ˆë¸”ë¦¬ ì „ìš©: ë³µì¡í•œ ë¡œì§ì€ Factoryì™€ Registryì— ìœ„ì„
  2. ëª¨ë“ˆí™”ëœ ì»´í¬ë„ŒíŠ¸: ê° ì•Œê³ ë¦¬ì¦˜ë³„ íŠ¹í™”ëœ Scorer, Trainer ì‚¬ìš©
  3. ì¡°ê±´ë¶€ ëª¨ë¸ ë¡œë”©: ì•Œê³ ë¦¬ì¦˜ì— ë”°ë¼ í•„ìš”í•œ ëª¨ë¸ë§Œ ì„ íƒì  ë¡œë“œ
  4. ë‹¨ê³„ì  ì‹¤í–‰: Stage1(ì„ íƒì ) â†’ Stage2(ë©”ì¸ í›ˆë ¨) â†’ ê²°ê³¼ ë°˜í™˜

ì•Œê³ ë¦¬ì¦˜ë³„ ì»´í¬ë„ŒíŠ¸ ì¡°í•©:
  - mtp-baseline: Base Model + No Scorer + Uniform Weighting
  - critic-wmtp: Base + RM + CriticScorer + Stage1 Pretraining
  - rho1-wmtp: Base + Ref + Rho1Scorer + Dynamic Weighting

ì´ í†µí•© ì ‘ê·¼ë²•ìœ¼ë¡œ ì—°êµ¬ìëŠ” ì•Œê³ ë¦¬ì¦˜ ê°„ ì„±ëŠ¥ì„ ê³µì •í•˜ê²Œ ë¹„êµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""

from __future__ import annotations  # Python 3.10+ íƒ€ì… íŒíŠ¸ í˜¸í™˜ì„±

from dataclasses import dataclass  # ê°„ë‹¨í•œ ë°ì´í„° í´ë˜ìŠ¤ ìƒì„±ìš©
from typing import Any  # ë²”ìš© íƒ€ì… íŒíŠ¸

from torch.utils.data import DataLoader  # ë°ì´í„°ì…‹ì„ ë°°ì¹˜ë¡œ ë¡œë“œí•˜ëŠ” ë„êµ¬
from torch.utils.data.distributed import DistributedSampler  # ë¶„ì‚° í›ˆë ¨ì„ ìœ„í•œ ë°ì´í„° ë¶„ë°°ê¸°
from transformers import default_data_collator  # HuggingFaceì˜ ê¸°ë³¸ ë°ì´í„° ë°°ì¹˜ ìƒì„±ê¸°

from src.factory.component_factory import ComponentFactory  # ì•Œê³ ë¦¬ì¦˜ë³„ ì»´í¬ë„ŒíŠ¸ ìƒì„± íŒ©í† ë¦¬
from src.settings import Config, Recipe  # Pydantic ê¸°ë°˜ ì„¤ì • ëª¨ë¸ë“¤
from src.utils import create_mlflow_manager, set_seed  # MLflow ì¶”ì ê³¼ ì¬í˜„ì„± ë³´ì¥ ìœ í‹¸


@dataclass
class RunOutputs:
    """íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ê²°ê³¼ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤.

    í›ˆë ¨ ì™„ë£Œ í›„ ë©”íŠ¸ë¦­ì„ í¬í•¨í•œ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ì—¬
    CLIë‚˜ ë‹¤ë¥¸ ëª¨ë“ˆì—ì„œ í›ˆë ¨ ì„±ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    Attributes:
        trainer_metrics: í›ˆë ¨ ê³¼ì •ì—ì„œ ìˆ˜ì§‘ëœ ê°ì¢… ë©”íŠ¸ë¦­ (loss, accuracy ë“±)
    """

    trainer_metrics: dict[str, Any]  # í›ˆë ¨ ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬


def run_training_pipeline(
    config: Config,  # í™˜ê²½ ì„¤ì • (GPU, ë¶„ì‚°í›ˆë ¨, S3 ë“±)
    recipe: Recipe,  # í›ˆë ¨ ë ˆì‹œí”¼ (ì•Œê³ ë¦¬ì¦˜, ëª¨ë¸, ë°ì´í„°ì…‹)
    dry_run: bool = False,  # ê²€ì¦ ëª¨ë“œ (ì‹¤ì œ í›ˆë ¨ X)
    resume_checkpoint: str | Path | None = None,  # ì¬ê°œìš© ì²´í¬í¬ì¸íŠ¸ (ì„ íƒì )
) -> RunOutputs:
    """WMTP í†µí•© í›ˆë ¨ íŒŒì´í”„ë¼ì¸ - ëª¨ë“  ì•Œê³ ë¦¬ì¦˜ì˜ ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜.

    Returns:
        RunOutputs: í›ˆë ¨ ë©”íŠ¸ë¦­ì´ í¬í•¨ëœ ê²°ê³¼ ê°ì²´

    Raises:
        ValueError: ì˜ëª»ëœ ì„¤ì •ê°’ì´ë‚˜ ì§€ì›ë˜ì§€ ì•ŠëŠ” ì•Œê³ ë¦¬ì¦˜
        RuntimeError: ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ë‚˜ í›ˆë ¨ ì¤‘ ì˜¤ë¥˜
    """
    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ë‹¨ê³„ ì¶”ì  ì‹œì‘
    console.print("[bold green]ğŸš€ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œì‘[/bold green]")
    console.print(f"[dim]ğŸ” íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ ì¶”ì  ì‹œì‘...[/dim]")

    # ------------------------------------------------------------

    # Step 0: ì‹¤í—˜ ì¶”ì  ë° ì¬í˜„ì„± ì„¤ì •
    set_seed(config.seed)  # ë™ì¼í•œ ì‹œë“œë¡œ ì¬í˜„ ê°€ëŠ¥í•œ ì‹¤í—˜ ë³´ì¥

    # ì¬ê°œ ì²˜ë¦¬ ë¡œì§ - ComponentFactory í†µí•© (í•œ ë²ˆë§Œ ë¡œë”©)
    start_epoch = 0
    start_step = 0
    resume_run_id = None
    checkpoint_data = None

    if resume_checkpoint:
        # ì²´í¬í¬ì¸íŠ¸ ì „ìš© ë¡œë” ìƒì„± - í•œ ë²ˆë§Œ ë¡œë“œí•˜ì—¬ ëª¨ë“  ì •ë³´ ì¶”ì¶œ
        checkpoint_loader = ComponentFactory.create_checkpoint_loader(config)
        checkpoint_loader.setup({})

        # ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ë° ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        checkpoint_result = checkpoint_loader.run({
            "model_path": resume_checkpoint,
            "load_metadata": True
        })

        # ë©”íƒ€ë°ì´í„°ì™€ ì²´í¬í¬ì¸íŠ¸ ë°ì´í„° ëª¨ë‘ ì¶”ì¶œ
        if checkpoint_result.get("checkpoint_data") is not None:
            checkpoint_data = checkpoint_result["checkpoint_data"]
            start_epoch = checkpoint_result.get("epoch", 0)
            start_step = checkpoint_result.get("step", 0)
            resume_run_id = checkpoint_result.get("mlflow_run_id")

    console.print(f"[dim]ğŸ” ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì™„ë£Œ: epoch={start_epoch}, step={start_step}[/dim]")
    console.print(f"[dim]ğŸ” MLflow Run ID: {resume_run_id}[/dim]")

    # ------------------------------------------------------------

    # Step 1: MLflow ì‹¤í—˜ ì¶”ì  ì´ˆê¸°í™”
    # ì‹¤í—˜ ë©”íŠ¸ë¦­ê³¼ ì•„í‹°íŒ©íŠ¸ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ì¶”ì í•˜ê¸° ìœ„í•œ MLflow ì„¤ì •
    mlflow = create_mlflow_manager(config.model_dump())
    tag_map = {
        str(i): t for i, t in enumerate(recipe.run.tags)
    }  # íƒœê·¸ë¥¼ MLflow í˜•ì‹ìœ¼ë¡œ ë³€í™˜

    if resume_run_id:
        mlflow.start_run(run_id=resume_run_id, resume=True)
    else:
        mlflow.start_run(run_name=recipe.run.name, tags=tag_map)

    console.print(f"[dim]ğŸ” MLflow ì‹¤í—˜ ì¶”ì  ì´ˆê¸°í™” ì™„ë£Œ: run_name={recipe.run.name}[/dim]")

    # Step 2: Base ëª¨ë¸ ë¡œë”©
    # Facebook native MTP ëª¨ë¸ - 4ê°œ headê°€ ë‚´ì¥ëœ WMTPì˜ í•µì‹¬ ì•„í‚¤í…ì²˜
    base_loader = ComponentFactory.create_model_loader(config, recipe)
    base_loader.setup({})
    base_result = base_loader.run({
        "model_path": str(config.paths.models.base)
    })
    base = base_result["model"]

    console.print(f"[dim]ğŸ” Base ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {config.paths.models.base}[/dim]")

    # Step 3: í† í¬ë‚˜ì´ì € ìƒì„±
    # HuggingFace í˜¸í™˜ í†µí•© í† í¬ë‚˜ì´ì € - ëª¨ë“  WMTP ëª¨ë¸ì´ ê³µìœ í•˜ëŠ” ì–´íœ˜ ì²´ê³„
    tokenizer_component = ComponentFactory.create_tokenizer(recipe, config)
    tokenizer_component.setup({"config": config})
    tokenizer_result = tokenizer_component.run({})
    tokenizer = tokenizer_result["tokenizer"]

    console.print(f"[dim]ğŸ” í† í¬ë‚˜ì´ì € ìƒì„± ì™„ë£Œ: {config.paths.models.base}[/dim]")

    # Step 4: ì•Œê³ ë¦¬ì¦˜ë³„ ì¶”ê°€ ëª¨ë¸ ë¡œë”© (ì¡°ê±´ë¶€)
    # ê° WMTP ì•Œê³ ë¦¬ì¦˜ì€ ì„œë¡œ ë‹¤ë¥¸ ë³´ì¡° ëª¨ë¸ì„ í•„ìš”ë¡œ í•¨
    ref_model = None  # Rho-1ì—ì„œ ì‚¬ìš©í•  ì°¸ì¡° ëª¨ë¸
    rm_model = None  # Criticì—ì„œ ì‚¬ìš©í•  ë³´ìƒ ëª¨ë¸

    if recipe.train.algo == "rho1-wmtp":
        # Rho-1: Reference Model ë¡œë”© - |CE^ref_t - CE^base_t| ê³„ì‚°ìš©
        ref_loader = ComponentFactory.create_aux_model_loader(recipe, config, "ref")
        ref_loader.setup({})
        ref_result = ref_loader.run({
            "model_path": str(config.paths.models.ref)
        })
        ref_model = ref_result["model"]

    elif recipe.train.algo == "critic-wmtp":
        # Critic: Reward Model ë¡œë”© - Stage1 Value Head í›ˆë ¨ìš©
        rm_loader = ComponentFactory.create_aux_model_loader(recipe, config, "rm")
        rm_loader.setup({})
        rm_result = rm_loader.run({
            "model_path": str(config.paths.models.rm)
        })
        rm_model = rm_result["model"]

    # mtp-baselineì€ ì¶”ê°€ ëª¨ë¸ ë¶ˆí•„ìš” - Base ëª¨ë¸ë§Œìœ¼ë¡œ ê· ë“± ê°€ì¤‘ì¹˜ MTP ìˆ˜í–‰

    console.print(f"[dim]ğŸ” ì•Œê³ ë¦¬ì¦˜ë³„ ì¶”ê°€ ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {recipe.train.algo}[/dim]")

    # Step 5: ì˜µí‹°ë§ˆì´ì € ì„¤ì • (ì˜ˆì™¸: .run() ì—†ëŠ” íŒ¨í„´)
    # AdamW + BF16 + FSDP ì¡°í•©ìœ¼ë¡œ ëŒ€ê·œëª¨ ëª¨ë¸ í›ˆë ¨ ìµœì í™”
    optimizer = ComponentFactory.create_optimizer(recipe, base.parameters())
    optimizer.setup({
        "num_training_steps": recipe.train.max_steps or 0
    })

    console.print(f"[dim]ğŸ” ì˜µí‹°ë§ˆì´ì € ì„¤ì • ì™„ë£Œ: {recipe.train.algo}[/dim]")

    # Step 6: ë°ì´í„°ì…‹ ë¡œë”©
    # MBPP, CodeContests, HumanEval ë“± ì½”ë“œ ìƒì„± ë²¤ì¹˜ë§ˆí¬ ì§€ì›
    train_loader_comp = ComponentFactory.create_data_loader(recipe, config)
    train_loader_comp.setup({})
    train_ds = train_loader_comp.run({
        "split": "train",
        "max_length": recipe.data.train.max_length,
        "add_solution": True,
    })["dataset"]

    console.print(f"[dim]ğŸ” ë°ì´í„°ì…‹ í† í¬ë‚˜ì´ì§• ì™„ë£Œ: {recipe.train.algo}[/dim]")

    # Step 7: ë°ì´í„°ì…‹ í† í¬ë‚˜ì´ì§•
    # HuggingFace í˜¸í™˜ í† í¬ë‚˜ì´ì €ë¡œ í…ìŠ¤íŠ¸ë¥¼ ëª¨ë¸ ì…ë ¥ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    tokenized = tokenizer.tokenize_dataset(
        dataset=train_ds,
        max_length=recipe.data.train.max_length,
        remove_columns=train_ds.column_names,
        load_from_cache_file=True,
    )

    console.print(f"[dim]ğŸ” ë¶„ì‚° í›ˆë ¨ìš© ë°ì´í„° ìƒ˜í”ŒëŸ¬ ì„¤ì • ì™„ë£Œ: {recipe.train.algo}[/dim]")

    # Step 8: ë¶„ì‚° í›ˆë ¨ìš© ë°ì´í„° ìƒ˜í”ŒëŸ¬ ì„¤ì •
    # ë‹¤ì¤‘ GPU í™˜ê²½ì—ì„œ ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ë¶„ë°°í•˜ê¸° ìœ„í•œ ìƒ˜í”ŒëŸ¬ êµ¬ì„±
    sampler = None  # ë¶„ì‚° í›ˆë ¨ìš© ë°ì´í„° ìƒ˜í”ŒëŸ¬ (ë‹¨ì¼ GPUì—ì„œëŠ” None)
    try:
        import torch
        import torch.distributed as dist
        if dist.is_available() and dist.is_initialized():
            sampler = DistributedSampler(tokenized, shuffle=True)
    except Exception:
        sampler = None

    console.print(f"[dim]ğŸ” ë¶„ì‚° í›ˆë ¨ìš© ë°ì´í„° ìƒ˜í”ŒëŸ¬ ì„¤ì • ì™„ë£Œ: {recipe.train.algo}[/dim]")

    # Step 9: PyTorch DataLoader ìƒì„±
    # í† í°í™”ëœ ë°ì´í„°ë¥¼ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ëª¨ë¸ì— ê³µê¸‰í•˜ê¸° ìœ„í•œ ë°ì´í„° ë¡œë” êµ¬ì„±
    train_dl = DataLoader(
        tokenized,
        batch_size=recipe.data.train.batch_size or 1,
        shuffle=(sampler is None),
        sampler=sampler,
        collate_fn=default_data_collator,
        num_workers=2,
        pin_memory=torch.cuda.is_available(),
    )

    console.print(f"[dim]ğŸ” PyTorch DataLoader ìƒì„± ì™„ë£Œ: {recipe.train.algo}[/dim]")

    # Step 10: Stage1 ì‚¬ì „í›ˆë ¨ (Critic ì „ìš©, ì¡°ê±´ë¶€)
    # Critic ì•Œê³ ë¦¬ì¦˜ì˜ íŠ¹ë³„í•œ 2ë‹¨ê³„ í•™ìŠµ - Value Head í›ˆë ¨ì„ S3ì— ì§ì ‘ ì €ì¥
    if recipe.train.algo == "critic-wmtp" and rm_model is not None and not dry_run:
        pretrainer = ComponentFactory.create_pretrainer(recipe)
        pretrainer.setup({})
        pretrainer.run({
            "base_model": base,
            "rm_model": rm_model,
            "train_dataloader": train_dl,
            "run_name": recipe.run.name or "default",  # S3 ê²½ë¡œ ìƒì„±ìš© ì‹¤í–‰ ì´ë¦„
        })

    console.print(f"[dim]ğŸ” Stage1 ì‚¬ì „í›ˆë ¨ ì™„ë£Œ: {recipe.train.algo}[/dim]")

    # Step 11: ë©”ì¸ Trainer ìƒì„± ë° ì´ˆê¸°í™”
    # ëª¨ë“  WMTP ì•Œê³ ë¦¬ì¦˜ì˜ í†µí•© ì‹¤í–‰ ì—”ì§„ - scorerì— ë”°ë¼ ê°€ì¤‘ì¹˜ ë°©ì‹ ê²°ì •
    trainer = ComponentFactory.create_trainer(recipe, config)
    trainer.setup({
        "model": base,
        "optimizer": optimizer,
        "mlflow_manager": mlflow,
        "ref_model": ref_model,
        "base_tokenizer": tokenizer,
        "rm_model": rm_model,
        "recipe": recipe,
        # ì¤‘ë³µ ì œê±°: ì´ë¯¸ ë¡œë“œëœ ì²´í¬í¬ì¸íŠ¸ ë°ì´í„°ì™€ ë©”íƒ€ë°ì´í„° ì „ë‹¬
        "checkpoint_data": checkpoint_data,
        "start_epoch": start_epoch,
        "start_step": start_step,
    })

    console.print(f"[dim]ğŸ” ë©”ì¸ Trainer ìƒì„± ë° ì´ˆê¸°í™” ì™„ë£Œ: {recipe.train.algo}[/dim]")

    # Step 12: ì‹¤í–‰ ëª¨ë“œ ë¶„ê¸°
    # Dry run ëª¨ë“œì—ì„œëŠ” ì„¤ì • ê²€ì¦ë§Œ ìˆ˜í–‰í•˜ê³  ì‹¤ì œ í›ˆë ¨ì€ ê±´ë„ˆë›°ê¸°
    if dry_run:
        mlflow.end_run("FINISHED")
        return RunOutputs(trainer_metrics={"dry_run": True})

    console.print(f"[dim]ğŸ” ì‹¤í–‰ ëª¨ë“œ ë¶„ê¸° ì™„ë£Œ: {recipe.train.algo}[/dim]")

    # Step 13: ë©”ì¸ WMTP í›ˆë ¨ ì‹¤í–‰
    # L_WMTP = Î£ w_{t+k} Ã— CE_k ê³µì‹ìœ¼ë¡œ í† í°ë³„ ì¤‘ìš”ë„ ë°˜ì˜ í›ˆë ¨
    metrics = trainer.run({
        "train_dataloader": train_dl,
        "max_steps": recipe.train.max_steps
    })

    console.print(f"[dim]ğŸ” ë©”ì¸ WMTP í›ˆë ¨ ì‹¤í–‰ ì™„ë£Œ: {recipe.train.algo}[/dim]")

    # Step 14: ì‹¤í—˜ ì¢…ë£Œ ë° ê²°ê³¼ ë°˜í™˜
    # MLflow ì¶”ì  ì¢…ë£Œ ë° í›ˆë ¨ ë©”íŠ¸ë¦­ ë°˜í™˜
    mlflow.end_run("FINISHED")

    console.print("[bold green]ğŸ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ[/bold green]")
    console.print(f"[dim]ğŸ” íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ê²°ê³¼: {metrics}[/dim]")

    return RunOutputs(trainer_metrics=metrics)