"""WMTP í†µí•© í›ˆë ¨ íŒŒì´í”„ë¼ì¸ - ëª¨ë“  ì•Œê³ ë¦¬ì¦˜ì˜ í•µì‹¬ ì‹¤í–‰ ì—”ì§„.

ì—°êµ¬ ì² í•™ êµ¬í˜„: "Not All Tokens Are What You Need"
===============================================

ì´ íŒŒì´í”„ë¼ì¸ì€ WMTPì˜ í•µì‹¬ ì•„ì´ë””ì–´ë¥¼ ì‹¤í˜„í•˜ëŠ” í†µí•© ì‹¤í–‰ ì—”ì§„ì…ë‹ˆë‹¤.
ì„¸ ê°€ì§€ ì•Œê³ ë¦¬ì¦˜(mtp-baseline, critic-wmtp, rho1-wmtp) ëª¨ë‘ê°€ ë™ì¼í•œ
íŒŒì´í”„ë¼ì¸ì„ ì‚¬ìš©í•˜ë˜, Factory íŒ¨í„´ì„ í†µí•´ ë‹¤ë¥¸ ì»´í¬ë„ŒíŠ¸ë¥¼ ì¡°í•©í•©ë‹ˆë‹¤.

íŒŒì´í”„ë¼ì¸ ì„¤ê³„ ì›ì¹™:
  1. ì–´ì…ˆë¸”ë¦¬ ì „ìš©: ë³µì¡í•œ ë¡œì§ì€ Factoryì™€ Registryì— ìœ„ì„
  2. ëª¨ë“ˆí™”ëœ ì»´í¬ë„ŒíŠ¸: ê° ì•Œê³ ë¦¬ì¦˜ë³„ íŠ¹í™”ëœ Scorer, Trainer ì‚¬ìš©
  3. ì¡°ê±´ë¶€ ëª¨ë¸ ë¡œë”©: ì•Œê³ ë¦¬ì¦˜ì— ë”°ë¼ í•„ìš”í•œ ëª¨ë¸ë§Œ ì„ íƒì  ë¡œë“œ
  4. ë‹¨ê³„ì  ì‹¤í–‰: Stage1(ì„ íƒì ) â†’ Stage2(ë©”ì¸ í›ˆë ¨) â†’ ê²°ê³¼ ë°˜í™˜

ì•Œê³ ë¦¬ì¦˜ë³„ ì»´í¬ë„ŒíŠ¸ ì¡°í•©:
  - mtp-baseline: Base Model + No Scorer + Uniform Weighting
  - critic-wmtp: Base + RM + CriticScorer + Stage1 Pretraining
  - rho1-wmtp: Base + Ref + Rho1Scorer + Dynamic Weighting

ì´ í†µí•© ì ‘ê·¼ë²•ìœ¼ë¡œ ì—°êµ¬ìëŠ” ì•Œê³ ë¦¬ì¦˜ ê°„ ì„±ëŠ¥ì„ ê³µì •í•˜ê²Œ ë¹„êµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""

from __future__ import annotations  # Python 3.10+ íƒ€ì… íŒíŠ¸ í˜¸í™˜ì„±

from dataclasses import dataclass  # ê°„ë‹¨í•œ ë°ì´í„° í´ë˜ìŠ¤ ìƒì„±ìš©
from typing import Any  # ë²”ìš© íƒ€ì… íŒíŠ¸

from torch.utils.data import DataLoader  # ë°ì´í„°ì…‹ì„ ë°°ì¹˜ë¡œ ë¡œë“œí•˜ëŠ” ë„êµ¬
from torch.utils.data.distributed import (
    DistributedSampler,  # ë¶„ì‚° í›ˆë ¨ì„ ìœ„í•œ ë°ì´í„° ë¶„ë°°ê¸°
)
from transformers import default_data_collator  # HuggingFaceì˜ ê¸°ë³¸ ë°ì´í„° ë°°ì¹˜ ìƒì„±ê¸°

from src.factory.component_factory import (
    ComponentFactory,  # ì•Œê³ ë¦¬ì¦˜ë³„ ì»´í¬ë„ŒíŠ¸ ìƒì„± íŒ©í† ë¦¬
)
from src.settings import Config, Recipe  # Pydantic ê¸°ë°˜ ì„¤ì • ëª¨ë¸ë“¤
from src.utils import create_mlflow_manager, set_seed  # MLflow ì¶”ì ê³¼ ì¬í˜„ì„± ë³´ì¥ ìœ í‹¸


@dataclass
class RunOutputs:
    """íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ê²°ê³¼ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤.

    í›ˆë ¨ ì™„ë£Œ í›„ ë©”íŠ¸ë¦­ì„ í¬í•¨í•œ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ì—¬
    CLIë‚˜ ë‹¤ë¥¸ ëª¨ë“ˆì—ì„œ í›ˆë ¨ ì„±ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    Attributes:
        trainer_metrics: í›ˆë ¨ ê³¼ì •ì—ì„œ ìˆ˜ì§‘ëœ ê°ì¢… ë©”íŠ¸ë¦­ (loss, accuracy ë“±)
    """

    trainer_metrics: dict[str, Any]  # í›ˆë ¨ ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬


def run_training_pipeline(
    config: Config,  # í™˜ê²½ ì„¤ì • (GPU, ë¶„ì‚°í›ˆë ¨, S3 ë“±)
    recipe: Recipe,  # í›ˆë ¨ ë ˆì‹œí”¼ (ì•Œê³ ë¦¬ì¦˜, ëª¨ë¸, ë°ì´í„°ì…‹)
    run_name: str | None = None,  # MLflow ì‹¤í—˜ ì´ë¦„ (ì„ íƒì )
    tags: list[str] | None = None,  # ì‹¤í—˜ ë¶„ë¥˜ìš© íƒœê·¸ (ì„ íƒì )
    dry_run: bool = False,  # ê²€ì¦ ëª¨ë“œ (ì‹¤ì œ í›ˆë ¨ X)
    max_steps: int | None = None,  # ìµœëŒ€ í›ˆë ¨ ìŠ¤í… (ì œí•œìš©)
    resume_checkpoint: Path | None = None,  # ì¬ê°œìš© ì²´í¬í¬ì¸íŠ¸ (ì„ íƒì )
) -> RunOutputs:
    """WMTP í†µí•© í›ˆë ¨ íŒŒì´í”„ë¼ì¸ - ëª¨ë“  ì•Œê³ ë¦¬ì¦˜ì˜ ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜.

    ì—°êµ¬ ì² í•™ "Not All Tokens Are What You Need" êµ¬í˜„:
        ì´ í•¨ìˆ˜ëŠ” ì„¸ ê°€ì§€ WMTP ì•Œê³ ë¦¬ì¦˜ì„ í†µí•©ëœ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.
        ê° ì•Œê³ ë¦¬ì¦˜ì€ í† í° ê°€ì¤‘ì¹˜ ê³„ì‚° ë°©ì‹ì´ ë‹¤ë¥´ì§€ë§Œ, ë™ì¼í•œ êµ¬ì¡°ë¡œ
        ê³µì •í•œ ì„±ëŠ¥ ë¹„êµê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.

    íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ë‹¨ê³„:
        1. ì‹¤í—˜ ì¶”ì  ì„¤ì • (MLflow + ì‹œë“œ ê³ ì •)
        2. ì•Œê³ ë¦¬ì¦˜ë³„ ëª¨ë¸ ë¡œë”©:
           - Base: í•­ìƒ ë¡œë“œ (Facebook native MTP)
           - Ref: rho1-wmtpì—ì„œë§Œ ì‚¬ìš©
           - RM: critic-wmtpì—ì„œë§Œ ì‚¬ìš©
        3. ì˜µí‹°ë§ˆì´ì € ì„¤ì • (ëŒ€ë¶€ë¶„ AdamW + BF16)
        4. ë°ì´í„°ì…‹ ë¡œë”© ë° í† í¬ë‚˜ì´ì§•
        5. ë¶„ì‚° í›ˆë ¨ìš© ë°ì´í„°ë¡œë” ì„¤ì •
        6. Stage1 ì‚¬ì „í›ˆë ¨ (critic-wmtpë§Œ í•´ë‹¹)
        7. ë©”ì¸ í›ˆë ¨ ì‹¤í–‰ (ëª¨ë“  ì•Œê³ ë¦¬ì¦˜ ê³µí†µ)
        8. ê²°ê³¼ ë°˜í™˜ ë° ì‹¤í—˜ ì¢…ë£Œ

    Args:
        config: GPU, ë©”ëª¨ë¦¬, S3 ë“± í™˜ê²½ ì„¤ì •
        recipe: ì•Œê³ ë¦¬ì¦˜, í•˜ì´í¼íŒŒë¼ë¯¸í„°, ë°ì´í„° ì„¤ì •
        run_name: MLflow ì‹¤í—˜ëª… (Noneì‹œ recipe.run.name ì‚¬ìš©)
        tags: ì‹¤í—˜ ë¶„ë¥˜ìš© íƒœê·¸ë“¤ (ì˜ˆ: ["exp1", "critic", "mbpp"])
        dry_run: Trueì‹œ ì„¤ì • ê²€ì¦ë§Œ í•˜ê³  ì‹¤ì œ í›ˆë ¨ì€ skip
        max_steps: í›ˆë ¨ ìŠ¤í… ì œí•œ (Noneì‹œ recipe ì„¤ì • ë”°ë¦„)

    Returns:
        RunOutputs: í›ˆë ¨ ë©”íŠ¸ë¦­ì´ í¬í•¨ëœ ê²°ê³¼ ê°ì²´

    Raises:
        ValueError: ì˜ëª»ëœ ì„¤ì •ê°’ì´ë‚˜ ì§€ì›ë˜ì§€ ì•ŠëŠ” ì•Œê³ ë¦¬ì¦˜
        RuntimeError: ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ë‚˜ í›ˆë ¨ ì¤‘ ì˜¤ë¥˜
    """
    # Step 1: ì‹¤í—˜ ì¶”ì  ë° ì¬í˜„ì„± ì„¤ì •
    set_seed(config.seed)  # ë™ì¼í•œ ì‹œë“œë¡œ ì¬í˜„ ê°€ëŠ¥í•œ ì‹¤í—˜ ë³´ì¥

    # ì¬ê°œ ì²˜ë¦¬ ë¡œì§
    start_epoch = 0
    start_step = 0
    resume_run_id = None

    if resume_checkpoint and resume_checkpoint.exists():
        import torch
        from rich.console import Console

        console = Console()
        checkpoint_data = torch.load(resume_checkpoint, map_location="cpu")
        start_epoch = checkpoint_data.get("epoch", 0)
        start_step = checkpoint_data.get("step", 0)
        resume_run_id = checkpoint_data.get("mlflow_run_id")

        console.print(
            f"[green]Resuming from epoch {start_epoch}, step {start_step}[/green]"
        )

    # MLflow ì‹¤í—˜ ì¶”ì  ë§¤ë‹ˆì € ì´ˆê¸°í™” ë° ì‹¤í–‰ ì‹œì‘/ì¬ê°œ
    mlflow = create_mlflow_manager(config.model_dump())
    tag_map = {
        str(i): t for i, t in enumerate(tags or [])
    }  # íƒœê·¸ë¥¼ MLflow í˜•ì‹ìœ¼ë¡œ ë³€í™˜

    if resume_run_id:
        mlflow.start_run(run_id=resume_run_id, resume=True)
    else:
        mlflow.start_run(run_name=run_name or recipe.run.name, tags=tag_map)

    # Step 2: ê¸°ë³¸ ëª¨ë¸ ë¡œë”© (ëª¨ë“  ì•Œê³ ë¦¬ì¦˜ì—ì„œ ê³µí†µìœ¼ë¡œ í•„ìš”)
    # Facebookì˜ native MTP ëª¨ë¸ - 4ê°œ headê°€ ë‚´ì¥ëœ ì•„í‚¤í…ì²˜ ì‚¬ìš©
    base_loader = ComponentFactory.create_model_loader(config, recipe)
    base_loader.setup({})  # ë¡œë” ì´ˆê¸°í™”

    # Base ëª¨ë¸ì€ í•­ìƒ í•„ìš” - WMTPì˜ í•µì‹¬ì´ ë˜ëŠ” Multi-Token Prediction ëª¨ë¸
    base_result = base_loader.run(
        {
            "model_path": str(config.paths.models.base_local)  # ë¡œì»¬ì— ìºì‹œëœ ëª¨ë¸ ê²½ë¡œ
        }
    )
    base = base_result["model"]  # Facebook MTP ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
    tokenizer = base_result["tokenizer"]  # ëª¨ë¸ê³¼ í˜¸í™˜ë˜ëŠ” í† í¬ë‚˜ì´ì €

    # Step 3: ì•Œê³ ë¦¬ì¦˜ë³„ ì¶”ê°€ ëª¨ë¸ ë¡œë”© (ì¡°ê±´ë¶€)
    # ê° WMTP ì•Œê³ ë¦¬ì¦˜ì€ ì„œë¡œ ë‹¤ë¥¸ ë³´ì¡° ëª¨ë¸ì„ í•„ìš”ë¡œ í•¨
    ref_model = None  # Rho-1ì—ì„œ ì‚¬ìš©í•  ì°¸ì¡° ëª¨ë¸
    rm_model = None  # Criticì—ì„œ ì‚¬ìš©í•  ë³´ìƒ ëª¨ë¸

    if recipe.train.algo == "rho1-wmtp":
        # Rho-1 ì•Œê³ ë¦¬ì¦˜: Reference Modelì´ í•„ìš”
        # |CE^ref_t - CE^base_t| ê³„ì‚°ì„ ìœ„í•´ ì°¸ì¡° ëª¨ë¸ì˜ CE ê°’ í•„ìš”
        ref_loader = ComponentFactory.create_model_loader(
            config
        )  # Recipe ì—†ìœ¼ë©´ HF ë¡œë”
        ref_loader.setup({})
        ref_result = ref_loader.run(
            {
                "model_path": str(
                    config.paths.models.ref_local
                )  # CodeLlama ë“± ì°¸ì¡° ëª¨ë¸
            }
        )
        ref_model = ref_result["model"]

    elif recipe.train.algo == "critic-wmtp":
        # Critic ì•Œê³ ë¦¬ì¦˜: Reward Modelì´ í•„ìš”
        # Stage1ì—ì„œ ì‹œí€€ìŠ¤ ë ˆë²¨ ë³´ìƒ ê³„ì‚° ë° Value Head í›ˆë ¨ì— ì‚¬ìš©
        rm_loader = ComponentFactory.create_model_loader(
            config
        )  # Recipe ì—†ìœ¼ë©´ HF ë¡œë”
        rm_loader.setup({})
        rm_result = rm_loader.run(
            {
                "model_path": str(config.paths.models.rm_local)  # Llama RM ë“± ë³´ìƒ ëª¨ë¸
            }
        )
        rm_model = rm_result["model"]

    # mtp-baselineì€ ì¶”ê°€ ëª¨ë¸ ë¶ˆí•„ìš” - Base ëª¨ë¸ë§Œìœ¼ë¡œ ê· ë“± ê°€ì¤‘ì¹˜ MTP ìˆ˜í–‰

    # Step 4: ì˜µí‹°ë§ˆì´ì € ì„¤ì •
    # ëŒ€ë¶€ë¶„ì˜ ê²½ìš° AdamW + BF16 + FSDP ì¡°í•© ì‚¬ìš©
    optimizer = ComponentFactory.create_optimizer(recipe, base.parameters())
    optimizer.setup(
        {"num_training_steps": max_steps or 0}
    )  # ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ìœ„í•œ ì´ ìŠ¤í… ìˆ˜

    # ğŸ“ ì¤‘ìš”: Facebook native MTP ëª¨ë¸ì€ 4ê°œì˜ horizon headê°€ ë‚´ì¥ë˜ì–´ ìˆìŒ
    # ë³„ë„ì˜ MTPWrapper ë¶ˆí•„ìš” - native implementation ì§ì ‘ ì‚¬ìš©
    # ì´ëŠ” ì„±ëŠ¥ê³¼ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ì¸¡ë©´ì—ì„œ ìœ ë¦¬

    # Step 5: ë°ì´í„°ì…‹ ë¡œë”© ë° ì „ì²˜ë¦¬
    # ì§€ì› ë°ì´í„°ì…‹: MBPP, CodeContests, HumanEval, Custom
    train_source = recipe.data.train.sources[0]  # ì²« ë²ˆì§¸ í›ˆë ¨ ì†ŒìŠ¤ ì‚¬ìš©
    train_loader_comp = ComponentFactory.create_data_loader(train_source, config)
    train_loader_comp.setup({})

    # í›ˆë ¨ ë°ì´í„°ì…‹ ë¡œë“œ - ë¬¸ì œì™€ ì†”ë£¨ì…˜ì´ í¬í•¨ëœ í˜•íƒœ
    train_ds = train_loader_comp.run(
        {
            "split": "train",  # í›ˆë ¨ ë¶„í•  ì‚¬ìš©
            "max_length": recipe.data.train.max_length,  # ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
            "add_solution": True,  # ì†”ë£¨ì…˜ í¬í•¨ (ì½”ë“œ ìƒì„± íƒœìŠ¤í¬)
        }
    )["dataset"]

    # Step 6: í† í¬ë‚˜ì´ì§• - í…ìŠ¤íŠ¸ë¥¼ ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” ìˆ«ìë¡œ ë³€í™˜
    def _tokenize_function(example: dict[str, Any]) -> dict[str, Any]:
        """ê°œë³„ ë°ì´í„° ìƒ˜í”Œì„ í† í°í™”í•˜ëŠ” ë‚´ë¶€ í•¨ìˆ˜.

        Args:
            example: ë°ì´í„°ì…‹ì˜ í•œ ìƒ˜í”Œ (ë”•ì…”ë„ˆë¦¬ í˜•íƒœ)

        Returns:
            í† í°í™”ëœ ê²°ê³¼ (input_ids, attention_mask, labels í¬í•¨)
        """
        # í…ìŠ¤íŠ¸ ì¶”ì¶œ - ë°ì´í„°ì…‹ í˜•ì‹ì— ë”°ë¼ ë‹¤ë¥¸ í‚¤ ì‚¬ìš© ê°€ëŠ¥
        text = example.get("full_text") or example.get("prompt") or ""

        # í† í¬ë‚˜ì´ì €ë¡œ í…ìŠ¤íŠ¸ë¥¼ ìˆ«ì ì‹œí€€ìŠ¤ë¡œ ë³€í™˜
        tok = tokenizer(
            text,
            truncation=True,  # ìµœëŒ€ ê¸¸ì´ ì´ˆê³¼ì‹œ ìë¥´ê¸°
            max_length=recipe.data.train.max_length,  # ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
            padding=False,  # ë°°ì¹˜ì—ì„œ íŒ¨ë”© (ì—¬ê¸°ì„œëŠ” í•˜ì§€ ì•ŠìŒ)
        )
        # ë¼ë²¨ì€ input_idsì™€ ë™ì¼ (ì–¸ì–´ëª¨ë¸ì€ ë‹¤ìŒ í† í° ì˜ˆì¸¡)
        tok["labels"] = tok["input_ids"].copy()
        return tok

    # ì „ì²´ ë°ì´í„°ì…‹ì— í† í¬ë‚˜ì´ì§• ì ìš©
    tokenized = train_ds.map(
        _tokenize_function,
        remove_columns=train_ds.column_names,  # ì›ë³¸ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì œê±° (ë©”ëª¨ë¦¬ ì ˆì•½)
        desc="í›ˆë ¨ ë°ì´í„° í† í¬ë‚˜ì´ì§•",  # ì§„í–‰ë¥  í‘œì‹œìš© ì„¤ëª…
        load_from_cache_file=True,  # ìºì‹œ ì‚¬ìš©ìœ¼ë¡œ ì¬ì‹¤í–‰ì‹œ ì†ë„ í–¥ìƒ
    )

    # Step 7: ë¶„ì‚° í›ˆë ¨ì„ ìœ„í•œ ë°ì´í„° ìƒ˜í”ŒëŸ¬ ì„¤ì •
    sampler = None  # ê¸°ë³¸ê°’: ìƒ˜í”ŒëŸ¬ ì—†ìŒ
    try:
        import torch.distributed as dist

        # ë¶„ì‚° í›ˆë ¨ì´ í™œì„±í™”ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        if dist.is_available() and dist.is_initialized():
            # DistributedSampler: ê° GPUê°€ ë‹¤ë¥¸ ë°ì´í„° ë¶€ë¶„ì„ ì²˜ë¦¬í•˜ë„ë¡ ë¶„ë°°
            sampler = DistributedSampler(tokenized, shuffle=True)
    except Exception:
        # ë¶„ì‚° í›ˆë ¨ì´ ì„¤ì •ë˜ì§€ ì•Šì€ ê²½ìš° None ìœ ì§€
        sampler = None

    # Step 8: PyTorch DataLoader ìƒì„± - ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë°ì´í„° ê³µê¸‰
    train_dl = DataLoader(
        tokenized,  # í† í°í™”ëœ ë°ì´í„°ì…‹
        batch_size=recipe.data.train.batch_size or 1,  # ë°°ì¹˜ í¬ê¸° (ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì •)
        shuffle=(sampler is None),  # ë¶„ì‚° í›ˆë ¨ì´ ì•„ë‹ ë•Œë§Œ ì…”í”Œ
        sampler=sampler,  # ë¶„ì‚° í›ˆë ¨ìš© ìƒ˜í”ŒëŸ¬ (ìˆëŠ” ê²½ìš°)
        collate_fn=default_data_collator,  # HuggingFaceì˜ ê¸°ë³¸ ë°°ì¹˜ ìƒì„±ê¸°
        num_workers=2,  # ë°ì´í„° ë¡œë”©ìš© ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ ìˆ˜
        pin_memory=torch.cuda.is_available(),  # GPU ì‚¬ìš©ì‹œ ë©”ëª¨ë¦¬ í•€ë‹ìœ¼ë¡œ ì†ë„ í–¥ìƒ
    )

    # Step 9: Stage1 ì‚¬ì „í›ˆë ¨ (critic-wmtp ì „ìš©)
    # Critic ì•Œê³ ë¦¬ì¦˜ë§Œì˜ íŠ¹ë³„í•œ 2ë‹¨ê³„ í•™ìŠµ ê³¼ì •
    if recipe.train.algo == "critic-wmtp" and rm_model is not None and not dry_run:
        from pathlib import Path

        from src.components.registry import trainer_registry

        # Stage1 ì„¤ì •: Value Head í›ˆë ¨ì„ ìœ„í•œ íŒŒë¼ë¯¸í„°ë“¤
        pre_cfg = {
            # ë³´ìƒ íƒ€ê²Ÿ: "rm_sequence" (ì‹œí€€ìŠ¤ ë ˆë²¨ ë³´ìƒ ì‚¬ìš©)
            "target": getattr(recipe.critic, "target", "rm_sequence")
            if hasattr(recipe, "critic")
            else "rm_sequence",
            # í† í° í™•ì‚° ë°©ì‹: "gae" (Generalized Advantage Estimation)
            "token_spread": getattr(recipe.critic, "token_spread", "gae")
            if hasattr(recipe, "critic")
            else "gae",
            # ë¸íƒ€ ê³„ì‚° ëª¨ë“œ: "td" (Temporal Difference)
            "delta_mode": getattr(recipe.critic, "delta_mode", "td")
            if hasattr(recipe, "critic")
            else "td",
            # ì •ê·œí™” ë°©ì‹: "zscore" (í‘œì¤€í™”)
            "normalize": getattr(recipe.critic, "normalize", "zscore")
            if hasattr(recipe, "critic")
            else "zscore",
            "temperature": recipe.loss.temperature,  # ì†Œí”„íŠ¸ë§¥ìŠ¤ ì˜¨ë„
            "lr": 1e-4,  # Stage1 ì „ìš© í•™ìŠµë¥  (ë³´í†µ ë©”ì¸ë³´ë‹¤ ë‚®ìŒ)
        }

        # Stage1 ì „ìš© trainer ìƒì„± ë° ì‹¤í–‰
        pretrainer = trainer_registry.create("critic-stage1-pretrainer-v1", pre_cfg)
        cache_root = (
            Path(config.paths.cache) / "critic" / (recipe.run.name or "default")
        )
        pretrainer.setup({})

        # Stage1 ì‹¤í–‰: Value Head í›ˆë ¨
        # RM ëª¨ë¸ë¡œë¶€í„° ì‹œí€€ìŠ¤ ë ˆë²¨ ë³´ìƒì„ ë°›ì•„ Value Function í•™ìŠµ
        pretrainer.run(
            {
                "base_model": base,  # ê¸°ë³¸ MTP ëª¨ë¸
                "rm_model": rm_model,  # ë³´ìƒ ì ìˆ˜ ì œê³µ ëª¨ë¸
                "train_dataloader": train_dl,  # í›ˆë ¨ ë°ì´í„°
                "cache_root": cache_root,  # Value Head ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ìœ„ì¹˜
            }
        )

    # Step 10: ë©”ì¸ Trainer ìƒì„± - ì•Œê³ ë¦¬ì¦˜ë³„ ë‹¤ë¥¸ ì„¤ì •
    # WMTPì˜ í•µì‹¬: ë™ì¼í•œ íŠ¸ë ˆì´ë„ˆ êµ¬ì¡°ì— ë‹¤ë¥¸ Scorer ì¡°í•©
    if recipe.train.algo == "mtp-baseline":
        # Baseline: Scorer ì—†ìŒ - ìˆœìˆ˜ MTP (ê· ë“± ê°€ì¤‘ì¹˜)
        # ëª¨ë“  í† í°ì— ë™ì¼í•œ ê°€ì¤‘ì¹˜ 1.0 ì ìš©
        scorer = None
        trainer = ComponentFactory.create_trainer(recipe, config, scorer)
    else:
        # Weighted ë°©ì‹: Scorer ì‚¬ìš© - í† í°ë³„ ì¤‘ìš”ë„ ê³„ì‚°
        # critic-wmtp ë˜ëŠ” rho1-wmtpì—ì„œ ë™ì  ê°€ì¤‘ì¹˜ ì ìš©
        scorer = ComponentFactory.create_scorer(recipe)

        # Criticì˜ ê²½ìš°: Stage1ì—ì„œ í›ˆë ¨ëœ Value Head ê²½ë¡œ ì œê³µ
        try:
            from pathlib import Path

            # Stage1ì—ì„œ ì €ì¥ëœ value_head.pt íŒŒì¼ ê²½ë¡œ
            vh_path = (
                Path(config.paths.cache)
                / "critic"
                / (recipe.run.name or "default")
                / "value_head.pt"
            )
            if vh_path.exists():
                # Value Headê°€ ì¡´ì¬í•˜ë©´ Scorerì— ê²½ë¡œ ì œê³µ
                scorer.setup({"value_head_path": vh_path})
            else:
                # Value Headê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì§„í–‰
                scorer.setup({})
        except Exception:
            # ì˜¤ë¥˜ ë°œìƒì‹œ ê¸°ë³¸ ì„¤ì • ì‚¬ìš©
            scorer.setup({})

        # ìµœì¢… Trainer ìƒì„± - Scorerê°€ í¬í•¨ëœ ê°€ì¤‘ì¹˜ ê¸°ë°˜ í›ˆë ¨
        trainer = ComponentFactory.create_trainer(recipe, config, scorer)
    # Step 11: Trainer ì´ˆê¸°í™” - ëª¨ë“  í•„ìš”í•œ ì»´í¬ë„ŒíŠ¸ ì—°ê²°
    trainer.setup(
        {
            "model": base,  # Facebook native MTP ëª¨ë¸
            "optimizer": optimizer,  # AdamW ë“± ìµœì í™”ê¸°
            "mlflow_manager": mlflow,  # ì‹¤í—˜ ì¶”ì  ë§¤ë‹ˆì €
            "ref_model": ref_model,  # Rho-1ìš© ì°¸ì¡° ëª¨ë¸ (í•´ë‹¹ì‹œ)
            "base_tokenizer": tokenizer,  # í† í¬ë‚˜ì´ì €
            "rm_model": rm_model,  # Criticìš© ë³´ìƒ ëª¨ë¸ (í•´ë‹¹ì‹œ)
            "recipe": recipe,  # ì²´í¬í¬ì¸íŠ¸ ì„¤ì •ì„ ìœ„í•œ Recipe ì „ë‹¬
            "resume_checkpoint": resume_checkpoint,  # ì¬ê°œìš© ì²´í¬í¬ì¸íŠ¸
        }
    )

    # Step 12: ì‹¤í–‰ ëª¨ë“œì— ë”°ë¥¸ ì²˜ë¦¬
    if dry_run:
        # ê²€ì¦ ëª¨ë“œ: ì„¤ì •ë§Œ í™•ì¸í•˜ê³  ì‹¤ì œ í›ˆë ¨ì€ ê±´ë„ˆë›°ê¸°
        mlflow.end_run("FINISHED")  # MLflow ì‹¤í–‰ ì¢…ë£Œ
        return RunOutputs(trainer_metrics={"dry_run": True})

    # Step 13: ë©”ì¸ í›ˆë ¨ ì‹¤í–‰
    # ì—¬ê¸°ì„œ ì‹¤ì œ WMTP í›ˆë ¨ì´ ìˆ˜í–‰ë¨ - ì•Œê³ ë¦¬ì¦˜ì— ë”°ë¼ ë‹¤ë¥¸ ê°€ì¤‘ì¹˜ ì ìš©
    # L_WMTP = Î£ w_{t+k} Ã— CE_k (k=1,2,3,4)
    metrics = trainer.run({"train_dataloader": train_dl, "max_steps": max_steps})

    # Step 14: ì‹¤í—˜ ì¢…ë£Œ ë° ê²°ê³¼ ë°˜í™˜
    mlflow.end_run("FINISHED")  # MLflow ì¶”ì  ì¢…ë£Œ
    return RunOutputs(trainer_metrics=metrics)  # í›ˆë ¨ ë©”íŠ¸ë¦­ ë°˜í™˜
