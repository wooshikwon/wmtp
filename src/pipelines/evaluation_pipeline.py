"""WMTP í†µí•© í‰ê°€ íŒŒì´í”„ë¼ì¸ - Meta 2024 ë…¼ë¬¸ ê¸°ì¤€ í‰ê°€ ì—”ì§„.

ì´ íŒŒì´í”„ë¼ì¸ì€ training_pipeline.pyì™€ ë™ì¼í•œ êµ¬ì¡°ë¡œ ì„¤ê³„ë˜ì–´
Meta MTP ë…¼ë¬¸ì˜ ëª¨ë“  í‰ê°€ ë©”íŠ¸ë¦­ì„ ì¬í˜„í•  ìˆ˜ ìˆëŠ” í†µí•© í‰ê°€ ì—”ì§„ì…ë‹ˆë‹¤.

íŒŒì´í”„ë¼ì¸ ì„¤ê³„ ì›ì¹™:
  1. ì–´ì…ˆë¸”ë¦¬ ì „ìš©: ë³µì¡í•œ ë¡œì§ì€ Factoryì™€ Registryì— ìœ„ì„
  2. ëª¨ë“ˆí™”ëœ ì»´í¬ë„ŒíŠ¸: ê° í‰ê°€ íƒ€ì…ë³„ íŠ¹í™”ëœ Evaluator ì‚¬ìš©
  3. ì¡°ê±´ë¶€ ë°ì´í„° ë¡œë”©: í‰ê°€ íƒ€ì…ì— ë”°ë¼ í•„ìš”í•œ ë°ì´í„°ì…‹ë§Œ ì„ íƒì  ë¡œë“œ
  4. ë‹¨ê³„ì  ì‹¤í–‰: ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ â†’ í‰ê°€ ì‹¤í–‰ â†’ ê²°ê³¼ ë°˜í™˜

í‰ê°€ íƒ€ì…ë³„ ì»´í¬ë„ŒíŠ¸ ì¡°í•©:
  - meta-mtp: Pass@k ë©”íŠ¸ë¦­ (HumanEval, MBPP, CodeContests)
  - inference-speed: MTP vs NTP ì¶”ë¡  ì†ë„ ë¹„êµ
  - per-head-analysis: í—¤ë“œë³„(t+1~t+4) ì„±ëŠ¥ ë¶„ì„
  - token-accuracy: í† í° ìœ„ì¹˜ë³„ ì˜ˆì¸¡ ì •í™•ë„

ì´ í†µí•© ì ‘ê·¼ë²•ìœ¼ë¡œ ì—°êµ¬ìëŠ” Meta ë…¼ë¬¸ ê²°ê³¼ë¥¼ ì™„ë²½í•˜ê²Œ ì¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""

from __future__ import annotations

from dataclasses import dataclass
from pathlib import Path
from typing import Any

import torch
from rich.console import Console

from src.factory.component_factory import ComponentFactory
from src.settings import Config, Recipe
from src.utils import create_mlflow_manager, set_seed

console = Console()


@dataclass
class EvaluationOutputs:
    """íŒŒì´í”„ë¼ì¸ í‰ê°€ ê²°ê³¼ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤.

    í‰ê°€ ì™„ë£Œ í›„ ë©”íŠ¸ë¦­ì„ í¬í•¨í•œ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ì—¬
    CLIë‚˜ ë‹¤ë¥¸ ëª¨ë“ˆì—ì„œ í‰ê°€ ì„±ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    Attributes:
        metrics: í‰ê°€ ê³¼ì •ì—ì„œ ìˆ˜ì§‘ëœ ê°ì¢… ë©”íŠ¸ë¦­
        algorithm: í‰ê°€ëœ ì•Œê³ ë¦¬ì¦˜ íƒ€ì…
        checkpoint: í‰ê°€ì— ì‚¬ìš©ëœ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
    """

    metrics: dict[str, Any]
    algorithm: str
    checkpoint: str


def run_evaluation_pipeline(
    config: Config,
    recipe: Recipe,
    checkpoint_path: Path,
    eval_types: list[str] | None = None,
    dry_run: bool = False,
) -> EvaluationOutputs:
    """WMTP í†µí•© í‰ê°€ íŒŒì´í”„ë¼ì¸ - Meta 2024 ë…¼ë¬¸ ê¸°ì¤€ ë©”ì¸ í‰ê°€ í•¨ìˆ˜.

    training_pipeline.pyì™€ ë™ì¼í•œ êµ¬ì¡°ë¡œ ì„¤ê³„ë˜ì–´ ì¼ê´€ì„±ì„ ìœ ì§€í•©ë‹ˆë‹¤.
    Factory íŒ¨í„´ì„ í†µí•´ ë‹¤ì–‘í•œ í‰ê°€ íƒ€ì…ì„ ë™ì ìœ¼ë¡œ ì¡°í•©í•©ë‹ˆë‹¤.

    Args:
        config: í™˜ê²½ ì„¤ì • (GPU, ì €ì¥ì†Œ, MLflow)
        recipe: í‰ê°€ ë ˆì‹œí”¼ (ëª¨ë¸, í‰ê°€ í”„ë¡œí† ì½œ)
        checkpoint_path: í‰ê°€í•  ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
        eval_types: í‰ê°€ íƒ€ì… ë¦¬ìŠ¤íŠ¸ (None = ["meta-mtp"])
        dry_run: ê²€ì¦ ëª¨ë“œ (ì‹¤ì œ í‰ê°€ X)

    Returns:
        EvaluationOutputs: í‰ê°€ ë©”íŠ¸ë¦­ì´ í¬í•¨ëœ ê²°ê³¼ ê°ì²´

    Raises:
        ValueError: ì˜ëª»ëœ ì„¤ì •ê°’ì´ë‚˜ ì§€ì›ë˜ì§€ ì•ŠëŠ” í‰ê°€ íƒ€ì…
        RuntimeError: ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ë‚˜ í‰ê°€ ì¤‘ ì˜¤ë¥˜
    """
    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ë‹¨ê³„ ì¶”ì  ì‹œì‘
    console.print("[bold blue]ğŸš€ í‰ê°€ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œì‘[/bold blue]")
    console.print(f"[dim]ğŸ” ì²´í¬í¬ì¸íŠ¸: {checkpoint_path}[/dim]")

    # Step 0: ì‹¤í—˜ ì¶”ì  ë° ì¬í˜„ì„± ì„¤ì •
    set_seed(config.seed)

    # Step 1: MLflow ì‹¤í—˜ ì¶”ì  ì´ˆê¸°í™”
    mlflow = create_mlflow_manager(config.model_dump())
    tag_map = {str(i): t for i, t in enumerate(recipe.run.tags)}
    mlflow.start_run(run_name=f"eval_{recipe.run.name}", tags=tag_map)

    console.print(f"[dim]ğŸ” MLflow ì‹¤í—˜ ì¶”ì  ì´ˆê¸°í™” ì™„ë£Œ: run_name=eval_{recipe.run.name}[/dim]")

    # Step 2: ì²´í¬í¬ì¸íŠ¸ ë¡œë”©
    checkpoint_loader = ComponentFactory.create_checkpoint_loader(config)
    checkpoint_loader.setup({})
    checkpoint_result = checkpoint_loader.run({
        "model_path": str(checkpoint_path),
        "load_metadata": True
    })

    model = checkpoint_result["model"]
    model.eval()  # í‰ê°€ ëª¨ë“œ ì„¤ì •

    console.print(f"[dim]ğŸ” ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì™„ë£Œ: {checkpoint_path}[/dim]")

    # Step 3: í† í¬ë‚˜ì´ì € ìƒì„±
    tokenizer_component = ComponentFactory.create_tokenizer(recipe, config)
    tokenizer_component.setup({"config": config})
    tokenizer_result = tokenizer_component.run({})
    tokenizer = tokenizer_result["tokenizer"]

    console.print(f"[dim]ğŸ” í† í¬ë‚˜ì´ì € ìƒì„± ì™„ë£Œ[/dim]")

    # Step 4: í‰ê°€ íƒ€ì… ê²°ì •
    if eval_types is None:
        eval_types = ["meta-mtp"]  # ê¸°ë³¸ê°’: Meta MTP ë…¼ë¬¸ í‰ê°€
    elif "all" in eval_types:
        eval_types = ["meta-mtp", "inference-speed", "per-head-analysis", "token-accuracy"]

    console.print(f"[dim]ğŸ” í‰ê°€ íƒ€ì…: {eval_types}[/dim]")

    # Step 5: ì‹¤í–‰ ëª¨ë“œ ë¶„ê¸° (dry run)
    if dry_run:
        mlflow.end_run("FINISHED")
        return EvaluationOutputs(
            metrics={"dry_run": True},
            algorithm=recipe.train.algo,
            checkpoint=str(checkpoint_path)
        )

    # Step 6: í‰ê°€ íƒ€ì…ë³„ ë°ì´í„°ì…‹ ë¡œë”© (ì¡°ê±´ë¶€)
    datasets = {}

    # meta-mtp í‰ê°€ì‹œ ë°ì´í„°ì…‹ ë¡œë”©
    if "meta-mtp" in eval_types:
        # MBPP ë°ì´í„°ì…‹ ë¡œë”©
        if "mbpp" in recipe.data.eval.sources:
            mbpp_recipe = recipe.model_copy(deep=True)
            mbpp_recipe.data.train.sources = ["mbpp"]
            mbpp_loader = ComponentFactory.create_data_loader(mbpp_recipe, config)
            mbpp_loader.setup({})
            mbpp_result = mbpp_loader.run({
                "split": "test",
                "max_length": recipe.data.eval.max_length
            })
            datasets["mbpp_dataset"] = mbpp_result["dataset"]
            console.print(f"[dim]ğŸ” MBPP ë°ì´í„°ì…‹ ë¡œë”© ì™„ë£Œ[/dim]")

        # CodeContests ë°ì´í„°ì…‹ ë¡œë”©
        if "codecontests" in recipe.data.eval.sources:
            contest_recipe = recipe.model_copy(deep=True)
            contest_recipe.data.train.sources = ["contest"]
            contest_loader = ComponentFactory.create_data_loader(contest_recipe, config)
            contest_loader.setup({})
            contest_result = contest_loader.run({
                "split": "test",
                "max_length": recipe.data.eval.max_length
            })
            datasets["contest_dataset"] = contest_result["dataset"]
            console.print(f"[dim]ğŸ” CodeContests ë°ì´í„°ì…‹ ë¡œë”© ì™„ë£Œ[/dim]")

    # Step 7: í‰ê°€ íƒ€ì…ë³„ ì‹¤í–‰ ë° ë©”íŠ¸ë¦­ ìˆ˜ì§‘
    all_metrics = {}

    for eval_type in eval_types:
        console.print(f"[cyan]âš¡ {eval_type} í‰ê°€ ì‹¤í–‰ ì¤‘...[/cyan]")

        # í‰ê°€ê¸° ìƒì„±
        evaluator = ComponentFactory.create_evaluator_by_type(eval_type, recipe, config)

        # í‰ê°€ê¸° ì´ˆê¸°í™”
        evaluator.setup({
            "device": "cuda" if torch.cuda.is_available() else "cpu",
            "sampling": recipe.eval.sampling.model_dump()
        })

        # í‰ê°€ ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„
        eval_context = {
            "model": model,
            "tokenizer": tokenizer,
            **datasets  # í•„ìš”í•œ ë°ì´í„°ì…‹ í¬í•¨
        }

        # í‰ê°€ ì‹¤í–‰
        eval_result = evaluator.run(eval_context)

        # ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        if "metrics" in eval_result:
            for metric_name, metric_value in eval_result["metrics"].items():
                all_metrics[f"{eval_type}.{metric_name}"] = metric_value

                # MLflowì— ë©”íŠ¸ë¦­ ê¸°ë¡
                if isinstance(metric_value, (int, float)):
                    mlflow.log_metric(f"{eval_type}.{metric_name}", metric_value)

        console.print(f"[green]âœ“ {eval_type} í‰ê°€ ì™„ë£Œ[/green]")

    # Step 8: MLflow íŒŒë¼ë¯¸í„° ê¸°ë¡
    mlflow.log_params({
        "checkpoint": str(checkpoint_path),
        "algorithm": recipe.train.algo,
        "model_id": recipe.model.base_id,
        "mtp_heads": recipe.model.mtp.n_heads,
        "eval_protocol": recipe.eval.protocol,
        "eval_types": ",".join(eval_types)
    })

    # Step 9: ì‹¤í—˜ ì¢…ë£Œ ë° ê²°ê³¼ ë°˜í™˜
    mlflow.end_run("FINISHED")

    console.print("[bold green]ğŸ í‰ê°€ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ[/bold green]")
    console.print(f"[dim]ğŸ” ìˆ˜ì§‘ëœ ë©”íŠ¸ë¦­ ìˆ˜: {len(all_metrics)}[/dim]")

    return EvaluationOutputs(
        metrics=all_metrics,
        algorithm=recipe.train.algo,
        checkpoint=str(checkpoint_path)
    )