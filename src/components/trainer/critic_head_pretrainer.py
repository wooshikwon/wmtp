"""
Critic Head Pretrainer - Value Head ì‚¬ì „í•™ìŠµ ì»´í¬ë„ŒíŠ¸ (v2.0.0).

RM(Reward Model)ì˜ ì‹œí€€ìŠ¤ ë³´ìƒì„ ì‚¬ìš©í•˜ì—¬ ê°€ì¹˜í•¨ìˆ˜ í—¤ë“œë¥¼ í•™ìŠµí•˜ê³ ,
Critic WMTPì˜ Stage 2ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì €ì¥í•©ë‹ˆë‹¤.

[ë¦¬íŒ©í† ë§ v2.0.0]
- CriticDeltaScorer ì œê±° í›„ GAE ë¡œì§ ì§ì ‘ êµ¬í˜„
- ì—°êµ¬ì œì•ˆì„œ ìˆ˜ì‹ ì§ì ‘ êµ¬í˜„: L_VF(Ï•) = E_t[(V_Ï•(s_t) - R_t)Â²]
- Value Head ì§ì ‘ ê´€ë¦¬ ë° í•™ìŠµ

ì—°êµ¬ ì² í•™:
Stage 1: Value Headê°€ RMì˜ ë³´ìƒ ì‹ í˜¸ë¡œë¶€í„° ê° í† í°ì˜ ê°€ì¹˜ë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµ
Stage 2: í•™ìŠµëœ Value Headë¡œ TD error ê³„ì‚°í•˜ì—¬ í† í° ì¤‘ìš”ë„ ê°€ì¤‘ì¹˜ ìƒì„±
"""

from __future__ import annotations

from pathlib import Path
from typing import Any

import numpy as np
import torch
import torch.nn as nn
from rich.console import Console
from rich.progress import track

from src.components.base import BaseComponent
from src.components.registry import pretrainer_registry
from src.utils.reward_utils import compute_sequence_rewards

console = Console()


@pretrainer_registry.register(
    "critic-head-pretrainer", category="pretrainer", version="2.0.0"
)
class CriticHeadPretrainer(BaseComponent):
    """Critic WMTPë¥¼ ìœ„í•œ Value Head ì‚¬ì „í•™ìŠµ íŠ¸ë ˆì´ë„ˆ.

    ì—°êµ¬ì œì•ˆì„œ Stage 1 êµ¬í˜„:
    1. RMì´ ì‹œí€€ìŠ¤ ë ˆë²¨ ë³´ìƒ R ì œê³µ
    2. GAEë¡œ í† í°ë³„ ê°€ì¹˜ ëª©í‘œê°’ VÌ‚_t ê³„ì‚°
    3. Value Head V_Ï•(h_t)ê°€ VÌ‚_të¥¼ ì˜ˆì¸¡í•˜ë„ë¡ MSE lossë¡œ í•™ìŠµ
    """

    def __init__(self, config: dict[str, Any] | None = None):
        """Value Head Pretrainer ì´ˆê¸°í™”.

        Args:
            config: í•™ìŠµ ì„¤ì •
                - lr: í•™ìŠµë¥  (ê¸°ë³¸ê°’: 1e-4)
                - num_epochs: í•™ìŠµ ì—í¬í¬ ìˆ˜ (ê¸°ë³¸ê°’: 3)
                - gamma: GAE í• ì¸ìœ¨ (ê¸°ë³¸ê°’: 0.99)
                - gae_lambda: GAE Î» (ê¸°ë³¸ê°’: 0.95)
                - max_steps: ìµœëŒ€ í•™ìŠµ ìŠ¤í… (ê¸°ë³¸ê°’: 1000)
        """
        super().__init__(config)

        # í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°
        self.lr = self.config.get("lr", 1e-4)
        self.num_epochs = self.config.get("num_epochs", 3)
        self.gamma = self.config.get("gamma", 0.99)  # GAE discount
        self.gae_lambda = self.config.get("gae_lambda", 0.95)  # GAE lambda
        self.max_steps = self.config.get("max_steps", 1000)

        # Value HeadëŠ” run()ì—ì„œ ìƒì„±
        self.value_head: nn.Module | None = None

    def spread_reward_to_tokens(
        self, sequence_reward: float, seq_length: int
    ) -> np.ndarray:
        """ì‹œí€€ìŠ¤ ë³´ìƒì„ í† í°ë³„ë¡œ ê· ë“± ë¶„ë°°.

        ê°„ë‹¨í•œ ê· ë“± ë¶„ë°° ë°©ì‹. í–¥í›„ ë” ì •êµí•œ ë°©ë²• ê³ ë ¤ ê°€ëŠ¥.

        Args:
            sequence_reward: ì‹œí€€ìŠ¤ ì „ì²´ ë³´ìƒ
            seq_length: ì‹œí€€ìŠ¤ ê¸¸ì´

        Returns:
            í† í°ë³„ ë³´ìƒ ë°°ì—´ [seq_length]
        """
        # ê· ë“± ë¶„ë°° (ê°€ì¥ ê°„ë‹¨í•œ ë°©ì‹)
        return np.full(seq_length, sequence_reward / seq_length, dtype=np.float32)

    def compute_gae_returns(
        self, rewards: np.ndarray, values: np.ndarray, next_value: float = 0.0
    ) -> np.ndarray:
        """GAE(Generalized Advantage Estimation)ë¡œ ê°€ì¹˜ ëª©í‘œê°’ ê³„ì‚°.

        ì—°êµ¬ì œì•ˆì„œì˜ í† í°ë³„ ê°€ì¹˜ ë¶„ë°° êµ¬í˜„.

        Args:
            rewards: í† í°ë³„ ë³´ìƒ [T]
            values: í˜„ì¬ ê°€ì¹˜ ì˜ˆì¸¡ê°’ [T]
            next_value: ë§ˆì§€ë§‰ ìƒíƒœì˜ ê°€ì¹˜ (ë³´í†µ 0)

        Returns:
            GAE ê¸°ë°˜ ê°€ì¹˜ ëª©í‘œê°’ [T]
        """
        T = len(rewards)
        advantages = np.zeros(T, dtype=np.float32)
        returns = np.zeros(T, dtype=np.float32)

        # GAE ê³„ì‚° (ì—­ë°©í–¥)
        gae = 0
        for t in reversed(range(T)):
            next_val = next_value if t == T - 1 else values[t + 1]

            # TD error: Î´_t = r_t + Î³V(s_{t+1}) - V(s_t)
            delta = rewards[t] + self.gamma * next_val - values[t]

            # GAE: A_t = Î´_t + (Î³Î»)Î´_{t+1} + (Î³Î»)Â²Î´_{t+2} + ...
            gae = delta + self.gamma * self.gae_lambda * gae
            advantages[t] = gae

            # Return = Value + Advantage
            returns[t] = values[t] + advantages[t]

        return returns

    def run(self, ctx: dict[str, Any]) -> dict[str, Any]:
        """Value Head ì‚¬ì „í•™ìŠµ ì‹¤í–‰.

        Args:
            ctx: ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸
                - base_model: ë² ì´ìŠ¤ ì–¸ì–´ ëª¨ë¸
                - rm_model: Reward Model
                - train_dataloader: í›ˆë ¨ ë°ì´í„° ë¡œë”
                - run_name: ì‹¤í–‰ ì´ë¦„ (S3 ê²½ë¡œìš©)

        Returns:
            í•™ìŠµ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        self.validate_initialized()

        base_model = ctx["base_model"]
        rm_model = ctx.get("rm_model")
        train_loader = ctx["train_dataloader"]
        run_name = ctx.get("run_name", "default")

        if rm_model is None:
            console.print(
                "[yellow]âš  No RM model provided for Stage 1 training[/yellow]"
            )
            return {"skipped": True, "message": "No RM model"}

        # Hidden size ì¶”ì¶œ
        hidden_size = None
        if hasattr(base_model, "config"):
            # HuggingFace ìŠ¤íƒ€ì¼ ëª¨ë¸
            config = base_model.config
            hidden_size = getattr(
                config, "hidden_size", getattr(config, "n_embd", None)
            )

        if hidden_size is None:
            raise ValueError(
                f"Failed to extract hidden_size from base model. "
                f"Model config attributes: {dir(base_model.config) if hasattr(base_model, 'config') else 'No config'}"
            )

        # ğŸ¯ Value Head ìƒì„± (ì—°êµ¬ì œì•ˆì„œ: V_Ï•(h_t) â†’ scalar)
        self.value_head = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),  # ì€ë‹‰ì¸µ
            nn.ReLU(),  # í™œì„±í™”
            nn.Linear(hidden_size // 2, 1),  # ìŠ¤ì¹¼ë¼ ê°€ì¹˜ ì¶œë ¥
        )

        # Device ì„¤ì •
        device = next(base_model.parameters()).device
        self.value_head = self.value_head.to(device)

        # Optimizerì™€ Loss function
        optimizer = torch.optim.AdamW(self.value_head.parameters(), lr=self.lr)
        loss_fn = nn.MSELoss()

        # Hidden states ì¶œë ¥ í™œì„±í™”
        orig_flag = getattr(
            getattr(base_model, "config", object()), "output_hidden_states", False
        )
        if hasattr(base_model, "config"):
            base_model.config.output_hidden_states = True

        # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ (Value Headë§Œ í•™ìŠµ)
        base_model.eval()
        self.value_head.train()

        console.print("[cyan]Starting Stage 1: Value Head Pretraining[/cyan]")
        console.print(f"  - Hidden size: {hidden_size}")
        console.print(f"  - Learning rate: {self.lr}")
        console.print(f"  - Max steps: {self.max_steps}")

        total_loss = 0.0
        step_count = 0

        # ğŸ”„ Training loop
        for epoch in range(self.num_epochs):
            console.print(f"\n[bold]Epoch {epoch + 1}/{self.num_epochs}[/bold]")

            for step, batch in enumerate(track(train_loader, description="Training")):
                if step >= self.max_steps:
                    break

                input_ids = batch.get("input_ids")
                attention_mask = batch.get("attention_mask")

                if input_ids is None:
                    continue

                # Deviceë¡œ ì´ë™
                input_ids = input_ids.to(device)
                if attention_mask is not None:
                    attention_mask = attention_mask.to(device)

                # ğŸ“Š Hidden states ì¶”ì¶œ (gradient ë¶ˆí•„ìš”)
                with (
                    torch.no_grad(),
                    torch.autocast(
                        device_type=("cuda" if torch.cuda.is_available() else "cpu"),
                        dtype=torch.bfloat16,
                    ),
                ):
                    outputs = base_model(
                        input_ids=input_ids,
                        attention_mask=attention_mask,
                        output_hidden_states=True,
                    )
                    # ì•ˆì „í•œ hidden_states ì¶”ì¶œ
                    from src.utils.model_utils import extract_hidden_states

                    hidden_states = extract_hidden_states(outputs)

                # ğŸ RMìœ¼ë¡œë¶€í„° ì‹œí€€ìŠ¤ ë³´ìƒ ê³„ì‚° (ê³µí†µ ìœ í‹¸ë¦¬í‹° ì‚¬ìš©)
                reward_tensor = compute_sequence_rewards(
                    rm_model, input_ids, attention_mask, amp_dtype=torch.bfloat16
                )
                rewards = reward_tensor.tolist()  # list[float]ë¡œ ë³€í™˜

                # ğŸ“ í† í°ë³„ ê°€ì¹˜ ëª©í‘œê°’ ê³„ì‚° (GAE)
                B, S, H = hidden_states.shape
                value_targets = []

                for b in range(B):
                    # ì‹œí€€ìŠ¤ ë³´ìƒì„ í† í°ë³„ë¡œ ë¶„ë°°
                    seq_reward = float(rewards[b])
                    token_rewards = self.spread_reward_to_tokens(seq_reward, S)

                    # ì´ˆê¸° ê°€ì¹˜ ì˜ˆì¸¡ (0ìœ¼ë¡œ ì‹œì‘)
                    init_values = np.zeros(S, dtype=np.float32)

                    # GAEë¡œ ê°€ì¹˜ ëª©í‘œê°’ ê³„ì‚°
                    value_target = self.compute_gae_returns(
                        token_rewards, init_values, next_value=0.0
                    )
                    value_targets.append(value_target)

                # ğŸ¯ Value Head í•™ìŠµ
                # Flatten: [B, S, H] â†’ [B*S, H]
                hs_flat = hidden_states.reshape(B * S, H)

                # ê°€ì¹˜ ëª©í‘œê°’ í…ì„œ ìƒì„±
                vt_flat = torch.tensor(
                    np.concatenate(value_targets, axis=0),
                    device=device,
                    dtype=hs_flat.dtype,
                ).view(B * S, 1)

                # Forward pass
                pred_values = self.value_head(hs_flat)

                # MSE Loss: L_VF(Ï•) = E_t[(V_Ï•(s_t) - R_t)Â²]
                loss = loss_fn(pred_values, vt_flat)

                # Backward pass - ì—°êµ¬ ì·¨ì§€ì— ë§ê²Œ ìì—°ìŠ¤ëŸ¬ìš´ í•™ìŠµ í—ˆìš©
                optimizer.zero_grad(set_to_none=True)
                loss.backward()

                # ì—°êµ¬ ì·¨ì§€ ìœ ì§€: gradient clipping ëŒ€ì‹  ëª¨ë‹ˆí„°ë§ë§Œ
                total_norm = 0.0
                for p in self.value_head.parameters():
                    if p.grad is not None:
                        param_norm = p.grad.data.norm(2)
                        total_norm += param_norm.item() ** 2
                total_norm = total_norm ** (1.0 / 2)

                if total_norm > 50.0:  # High threshold for Stage 1
                    console.print(
                        f"[yellow]âš  Stage 1 large gradient: {total_norm:.2f}[/yellow]"
                    )

                optimizer.step()

                # í†µê³„
                total_loss += loss.item()
                step_count += 1

                if step % 100 == 0:
                    avg_loss = total_loss / max(step_count, 1)
                    console.print(
                        f"  Step {step}: Loss = {loss.item():.4f}, "
                        f"Avg Loss = {avg_loss:.4f}"
                    )

        # ğŸ”š Hidden states ì„¤ì • ë³µì›
        if hasattr(base_model, "config"):
            base_model.config.output_hidden_states = orig_flag

        # ğŸ’¾ Value Head ì €ì¥
        save_location = self._save_value_head(run_name)

        avg_final_loss = total_loss / max(step_count, 1)
        console.print("\n[green]âœ… Stage 1 Training Complete[/green]")
        console.print(f"  - Final avg loss: {avg_final_loss:.4f}")
        console.print(f"  - Value Head saved to: {save_location}")

        return {
            "saved": save_location,
            "final_loss": avg_final_loss,
            "total_steps": step_count,
        }

    def _save_value_head(self, run_name: str) -> str:
        """Value Headë¥¼ ì €ì¥.

        Args:
            run_name: ì‹¤í–‰ ì´ë¦„ (ê²½ë¡œ ìƒì„±ìš©)

        Returns:
            ì €ì¥ ìœ„ì¹˜ ë¬¸ìì—´
        """
        # S3 ê²½ë¡œ ë˜ëŠ” ë¡œì»¬ ê²½ë¡œ ìƒì„±
        checkpoint_dir = Path(f"./checkpoints/critic/{run_name}")
        checkpoint_dir.mkdir(parents=True, exist_ok=True)

        # Value Head state dict ì €ì¥
        vh_path = checkpoint_dir / "value_head_stage1.pt"
        torch.save(self.value_head.state_dict(), vh_path)

        # ë©”íƒ€ë°ì´í„°ë„ ì €ì¥
        meta_path = checkpoint_dir / "value_head_meta.json"
        import json

        with open(meta_path, "w") as f:
            json.dump(
                {
                    "version": "2.0.0",
                    "hidden_size": self.value_head[0].in_features,
                    "intermediate_size": self.value_head[0].out_features,
                    "lr": self.lr,
                    "gamma": self.gamma,
                    "gae_lambda": self.gae_lambda,
                },
                f,
                indent=2,
            )

        return str(vh_path)
