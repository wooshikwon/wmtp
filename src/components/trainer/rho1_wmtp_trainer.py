"""
Rho-1 WMTP Trainer - Reference ëª¨ë¸ ë¹„êµ ê¸°ë°˜ WMTP ì•Œê³ ë¦¬ì¦˜

Microsoft Rho-1 ì—°êµ¬ì˜ "Not All Tokens Are What You Need" ì² í•™ì„ MTPì— ì ìš©í•œ í•µì‹¬ ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤.
Reference ëª¨ë¸ê³¼ Base ëª¨ë¸ì˜ Cross-Entropy ì°¨ì´ë¡œ í† í° ì¤‘ìš”ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

íŠ¹ì§•:
- Reference CE ë¹„êµ: |CE^ref - CE^base| ê¸°ë°˜ í† í° ì¤‘ìš”ë„ ì¸¡ì •
- íš¨ìœ¨ì  êµ¬í˜„: í•œ ë²ˆì˜ forward passë¡œ reference CE ê³„ì‚°
- ì •í™•í•œ ì •ë ¬: MTP í—¤ë“œë³„ë¡œ ì ì ˆí•œ time stepê³¼ ë§¤ì¹­
- ìµœê³  ì„±ëŠ¥: ì—°êµ¬ê°œì„ ì•ˆì—ì„œ ê¶Œì¥í•˜ëŠ” ê°€ì¥ íš¨ê³¼ì ì¸ ë°©ë²•

ìˆ˜í•™ì  ê³µì‹:
    CE^ref_t = CrossEntropy(ref_model(input[:t]), target[t])
    CE^base_{t+k} = CrossEntropy(base_model_head_k(input[:t]), target[t+k])
    excess_loss = |CE^ref_t - CE^base_{t+k}| (ì ì ˆí•œ time step ì •ë ¬ í›„)
    w_{t+k} = softmax(excess_loss / temperature)_k
    L_WMTP = Î£(k=0 to H-1) w_{t+k} Ã— CE_k
"""

from __future__ import annotations

import math
from typing import Any

import torch
import torch.nn.functional as F
from rich.console import Console

from src.components.trainer.base_wmtp_trainer import BaseWmtpTrainer, compute_weighted_mtp_loss
from src.components.registry import trainer_registry

console = Console()


@trainer_registry.register("rho1-wmtp", category="trainer", version="2.0.0")
class Rho1WmtpTrainer(BaseWmtpTrainer):
    """Rho-1 WMTP íŠ¸ë ˆì´ë„ˆ - Reference ëª¨ë¸ ë¹„êµ ê¸°ë°˜ WMTP ì•Œê³ ë¦¬ì¦˜.

    ì—°êµ¬ ì² í•™ "Not All Tokens Are What You Need"ì˜ í•µì‹¬ êµ¬í˜„:
        Microsoft Rho-1 ì—°êµ¬ì˜ ì„ íƒì  ì–¸ì–´ëª¨ë¸ë§ ì•„ì´ë””ì–´ë¥¼ MTPì— ì ìš©í•˜ì—¬,
        Reference ëª¨ë¸ê³¼ Base ëª¨ë¸ì´ ëª¨ë‘ ì–´ë ¤ì›Œí•˜ëŠ” í† í°ì„ ì¤‘ìš” í† í°ìœ¼ë¡œ ì‹ë³„í•˜ê³ 
        í•´ë‹¹ í† í°ì— ë†’ì€ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤.

    ğŸ”¬ í•µì‹¬ ë™ì‘:
        1. Reference ëª¨ë¸ë¡œ ê° ìœ„ì¹˜ì˜ next token CE ê³„ì‚°
        2. MTP ëª¨ë¸ì˜ í—¤ë“œë³„ CEì™€ ì •í™•í•œ time step ë§¤ì¹­
        3. Excess loss ê³„ì‚°: |CE^ref - CE^base|
        4. Softmax ê°€ì¤‘ì¹˜ ë³€í™˜ ë° WMTP ì†ì‹¤ ì ìš©

    â­ ì •ë ¬ ë°©ì‹ (í•µì‹¬):
        - Reference: t ì‹œì ì—ì„œ t+1 í† í° ì˜ˆì¸¡
        - MTP Head k: t ì‹œì ì—ì„œ t+k+1 í† í° ì˜ˆì¸¡
        - ì •ë ¬: head_k vs ref(t+k â†’ t+k+1)

    ì¥ì :
        - ì´ë¡ ì  ê·¼ê±°: Microsoft Rho-1 ì—°êµ¬ì—ì„œ ì…ì¦ëœ íš¨ê³¼
        - Pretrainer ë¶ˆí•„ìš”: ë³„ë„ í•™ìŠµ ì—†ì´ ë°”ë¡œ ê°€ì¤‘ì¹˜ ê³„ì‚°
        - ì •í™•í•œ ë¹„êµ: ë™ì¼í•œ ì˜ˆì¸¡ íƒœìŠ¤í¬ë¼ë¦¬ CE ë¹„êµ
        - ë†’ì€ ì„±ëŠ¥: ì—°êµ¬ê°œì„ ì•ˆì—ì„œ ê¶Œì¥í•˜ëŠ” ìµœê³  ì„±ëŠ¥ ì•Œê³ ë¦¬ì¦˜

    í•„ìˆ˜ ìš”êµ¬ì‚¬í•­:
        - ref_model: Reference ëª¨ë¸ (ctxì—ì„œ ì œê³µ)
        - temperature: Softmax ì˜¨ë„ íŒŒë¼ë¯¸í„° (config)
    """

    def setup(self, ctx: dict[str, Any]) -> None:
        """Rho-1 íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™” - Reference ëª¨ë¸ í•„ìˆ˜ í™•ì¸.

        Args:
            ctx: ì»¨í…ìŠ¤íŠ¸ ë”•ì…”ë„ˆë¦¬ (ref_model í¬í•¨ í•„ìš”)

        Raises:
            ValueError: Reference ëª¨ë¸ì´ ì œê³µë˜ì§€ ì•Šì€ ê²½ìš°
        """
        super().setup(ctx)

        self.ref_model: torch.nn.Module | None = ctx.get("ref_model")
        if self.ref_model is None:
            raise ValueError(
                "Rho1WmtpTrainer requires 'ref_model' in context. "
                "Please provide a reference model for CE comparison."
            )

        # Recipe ê¸°ë°˜ ì„¤ì • ë¡œë“œ (Factoryì—ì„œ ì „ë‹¬)
        self.rho1_cfg = self.config.get("rho1_config", {})
        
        # Dual mode íŒŒë¼ë¯¸í„° ë¡œë“œ 
        self.selection_mode = self.rho1_cfg.get("selection_mode", "weighted")
        self.skip_threshold_pct = float(self.rho1_cfg.get("skip_threshold_percentile", 0.3))
        
        # Weight softmax temperature (weighted modeì—ì„œ ì‚¬ìš©)
        # Backward compatibility: temperature â†’ weight_temperature
        self.temperature = float(
            self.loss_cfg.get("weight_temperature") or
            self.loss_cfg.get("temperature", 0.7)
        )
        if self.temperature <= 0:
            raise ValueError(f"Weight temperature must be positive, got {self.temperature}")
            
        # Phase 1.2: CE Difference Threshold íŒŒë¼ë¯¸í„° (ë…¸ì´ì¦ˆ í•„í„°ë§)
        self.min_ce_diff = float(self.rho1_cfg.get("min_ce_diff", 0.01))
        if self.min_ce_diff < 0:
            raise ValueError(f"min_ce_diff must be non-negative, got {self.min_ce_diff}")

        console.print(f"[green]Rho-1 WMTP initialized:[/green]")
        console.print(f"  Mode: {self.selection_mode}")
        if self.selection_mode == "token_skip":
            console.print(f"  Skip threshold: {self.skip_threshold_pct:.1%} (bottom)")
        else:
            console.print(f"  Weight temperature: {self.temperature}")
        console.print(f"  Min CE diff threshold: {self.min_ce_diff}")

    def compute_reference_ce(self, input_ids: torch.Tensor, target_ids: torch.Tensor) -> torch.Tensor:
        """íš¨ìœ¨ì  Reference CE ê³„ì‚° (í•œ ë²ˆì˜ forward pass).

        Reference ëª¨ë¸ë¡œ ê° ìœ„ì¹˜ì—ì„œ ë‹¤ìŒ í† í°ì˜ Cross-Entropyë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        MTP í—¤ë“œì™€ì˜ ì •í™•í•œ ë¹„êµë¥¼ ìœ„í•´ ìœ„ì¹˜ë³„ë¡œ CEë¥¼ ë¶„ë¦¬í•©ë‹ˆë‹¤.

        Args:
            input_ids: ì…ë ¥ í† í° ì‹œí€€ìŠ¤ [B, S]
            target_ids: íƒ€ê²Ÿ í† í° ì‹œí€€ìŠ¤ [B, S]

        Returns:
            ref_ce_all: ìœ„ì¹˜ë³„ Reference CE [B, S-1] - tìœ„ì¹˜ì—ì„œ t+1í† í° ì˜ˆì¸¡ CE
        """
        with torch.no_grad():
            # Reference ëª¨ë¸ forward pass
            ref_outputs = self.ref_model(
                input_ids=input_ids,
                attention_mask=torch.ones_like(input_ids),  # ì „ì²´ ì‹œí€€ìŠ¤ ì‚¬ìš©
            )

            # logits ì¶”ì¶œ
            if isinstance(ref_outputs, dict) and "logits" in ref_outputs:
                ref_logits = ref_outputs["logits"]  # [B, S, V]
            else:
                ref_logits = ref_outputs

            if ref_logits.ndim != 3:
                raise ValueError(f"Reference logits should be 3D [B,S,V], got {ref_logits.shape}")

            # ê° ìœ„ì¹˜ì—ì„œ ë‹¤ìŒ í† í°ì˜ CE ê³„ì‚°
            # ref_logits[:, :-1] = t ìœ„ì¹˜ì˜ logits (t+1 í† í° ì˜ˆì¸¡)
            # target_ids[:, 1:] = t+1 ìœ„ì¹˜ì˜ ì‹¤ì œ í† í°
            ref_ce_all = F.cross_entropy(
                ref_logits[:, :-1].transpose(1, 2),  # [B, V, S-1]
                target_ids[:, 1:],                   # [B, S-1]
                reduction='none'
            )  # [B, S-1] - ê° ìœ„ì¹˜ tì—ì„œ t+1 í† í°ì— ëŒ€í•œ CE

        return ref_ce_all

    def align_ref_ce_to_mtp(self, ref_ce_all: torch.Tensor, mtp_ce_heads: torch.Tensor) -> torch.Tensor:
        """Reference CEë¥¼ MTP í—¤ë“œì™€ ì •ë ¬.

        Reference ëª¨ë¸ì˜ ìœ„ì¹˜ë³„ CEë¥¼ MTP í—¤ë“œë³„ CEì™€ ì˜¬ë°”ë¥¸ time stepìœ¼ë¡œ ë§¤ì¹­í•©ë‹ˆë‹¤.

        ì •ë ¬ ì›ë¦¬:
        - MTP Head k: t ì‹œì ì—ì„œ t+k+1 í† í° ì˜ˆì¸¡
        - Reference: t ì‹œì ì—ì„œ t+1 í† í° ì˜ˆì¸¡
        - ë§¤ì¹­: head_k[t] â†” reference[t+k] (ë‘˜ ë‹¤ t+k+1 í† í° ì˜ˆì¸¡)

        Args:
            ref_ce_all: Reference CE [B, S-1] - ìœ„ì¹˜ tì—ì„œ t+1 í† í° ì˜ˆì¸¡ CE
            mtp_ce_heads: MTP í—¤ë“œë³„ CE [B, S, H]

        Returns:
            aligned_ref_ce: MTP í—¤ë“œì™€ ì •ë ¬ëœ Reference CE [B, S, H]
        """
        B, S, H = mtp_ce_heads.shape
        aligned_ref_ce = torch.zeros_like(mtp_ce_heads)

        # ê° í—¤ë“œë³„ë¡œ ì ì ˆí•œ reference CE ë§¤ì¹­
        for k in range(H):
            # Head këŠ” t+k+1 í† í°ì„ ì˜ˆì¸¡
            # Referenceì—ì„œ t+k ìœ„ì¹˜ì˜ CEëŠ” t+k+1 í† í°ì— ëŒ€í•œ ì˜ˆì¸¡
            if k < ref_ce_all.size(1):  # reference CE ë²”ìœ„ ë‚´ì¸ì§€ í™•ì¸
                valid_len = min(S - k - 1, ref_ce_all.size(1) - k)
                if valid_len > 0:
                    # í—¤ë“œ kì˜ ì²˜ìŒ valid_len ìœ„ì¹˜ì— reference[k:k+valid_len] ë§¤ì¹­
                    aligned_ref_ce[:, :valid_len, k] = ref_ce_all[:, k:k+valid_len]

        return aligned_ref_ce

    def compute_head_weights(self, logits: torch.Tensor, target_ids: torch.Tensor, ce_per_head: torch.Tensor, **kwargs) -> torch.Tensor | tuple[torch.Tensor, torch.Tensor]:
        """Rho-1 ë°©ì‹: |CE^ref - CE^base| ê¸°ë°˜ ê°€ì¤‘ì¹˜ ê³„ì‚°.

        Reference ëª¨ë¸ê³¼ Base ëª¨ë¸ì˜ CE ì°¨ì´ë¥¼ ê³„ì‚°í•˜ì—¬ í† í° ì¤‘ìš”ë„ë¥¼ ì¸¡ì •í•˜ê³ ,
        ì´ë¥¼ MTP í—¤ë“œë³„ ê°€ì¤‘ì¹˜ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

        Args:
            logits: MTP ëª¨ë¸ ì¶œë ¥ [B, S, H, V] (ì‚¬ìš©ë˜ì§€ ì•ŠìŒ, ce_per_head ì‚¬ìš©)
            target_ids: íƒ€ê²Ÿ í† í° ID [B, S]
            ce_per_head: MTP í—¤ë“œë³„ CE [B, S, H] - compute_weighted_mtp_lossì—ì„œ ê³„ì‚°ë¨
            **kwargs: input_ids ë“± ì¶”ê°€ ì •ë³´

        Returns:
            - Weighted mode: head_weightsë§Œ ë°˜í™˜ [B, S, H]
            - Token skip mode: (head_weights, selection_mask) íŠœí”Œ ë°˜í™˜

        Raises:
            ValueError: input_idsê°€ ì œê³µë˜ì§€ ì•Šì€ ê²½ìš°
        """
        # Reference CE ê³„ì‚°ì„ ìœ„í•œ input_ids í•„ìš”
        input_ids = kwargs.get("input_ids")
        if input_ids is None:
            raise ValueError(
                "Rho1WmtpTrainer requires 'input_ids' for reference CE calculation. "
                "Ensure input_ids are passed in kwargs."
            )

        # 1. Reference ëª¨ë¸ë¡œ CE ê³„ì‚°
        ref_ce_all = self.compute_reference_ce(input_ids, target_ids)

        # 2. MTP í—¤ë“œì™€ Reference CE ì •ë ¬
        aligned_ref_ce = self.align_ref_ce_to_mtp(ref_ce_all, ce_per_head)

        # 3. Excess loss ê³„ì‚°: |CE^ref - CE^base|
        # í° ì°¨ì´ = ë‘ ëª¨ë¸ ëª¨ë‘ ì–´ë ¤ì›Œí•¨ = ì¤‘ìš”í•œ í† í°
        excess_loss = torch.abs(ce_per_head - aligned_ref_ce)  # [B, S, H]
        
        # Phase 1.2: CE Difference Threshold ì ìš© (ë…¸ì´ì¦ˆ í•„í„°ë§)
        excess_loss = self._apply_ce_threshold(excess_loss)

        # 4. Selection modeì— ë”°ë¼ ë¶„ê¸°
        if self.selection_mode == "token_skip":
            return self._compute_token_skip_weights(excess_loss)
        else:
            return self._compute_weighted_weights(excess_loss)

    def _compute_token_skip_weights(
        self, 
        excess_loss: torch.Tensor
    ) -> tuple[torch.Tensor, torch.Tensor]:
        """
        Rho-1 Original: ìƒìœ„ í† í°ë§Œ ì„ íƒ, ë‚˜ë¨¸ì§€ ì œì™¸.
        
        Args:
            excess_loss: [B, S, H] - ê° í† í°-í—¤ë“œì˜ excess loss
            
        Returns:
            head_weights: [B, S, H] - ì„ íƒëœ í† í°ì€ 1.0, ì œì™¸ëŠ” 0.0
            selection_mask: [B, S, H] - ë™ì¼ (ë°”ì´ë„ˆë¦¬ ë§ˆìŠ¤í¬)
        """
        B, S, H = excess_loss.shape
        
        # ë°°ì¹˜ë³„ë¡œ threshold ê³„ì‚°
        flat_loss = excess_loss.view(B, -1)  # [B, S*H]
        
        # í•˜ìœ„ k% percentile ê°’ êµ¬í•˜ê¸°
        k_threshold = torch.quantile(
            flat_loss, 
            self.skip_threshold_pct,  # í•˜ìœ„ 30% ê¸°ë³¸ê°’
            dim=1, 
            keepdim=True
        ).view(B, 1, 1)  # [B, 1, 1]
        
        # ì„ê³„ê°’ ì´ìƒì¸ í† í°ë§Œ ì„ íƒ (binary mask)
        selection_mask = (excess_loss >= k_threshold).float()  # [B, S, H]
        
        # ì„ íƒëœ í† í°ì—ë§Œ ê· ë“± ê°€ì¤‘ì¹˜ ë¶€ì—¬
        head_weights = selection_mask.clone()
        
        # í†µê³„ ë¡œê¹…
        selected_ratio = selection_mask.mean()
        console.print(f"[cyan]Token Skip: {selected_ratio:.1%} tokens selected[/cyan]")
        
        return head_weights, selection_mask
    
    def _compute_weighted_weights(
        self, 
        excess_loss: torch.Tensor
    ) -> torch.Tensor:
        """
        WMTP: ëª¨ë“  í† í°ì— ì—°ì†ì  ê°€ì¤‘ì¹˜ ì ìš© (ê¸°ì¡´ ë°©ì‹).
        
        Args:
            excess_loss: [B, S, H] - ê° í† í°-í—¤ë“œì˜ excess loss
            
        Returns:
            weights: [B, S, H] - Softmax ê°€ì¤‘ì¹˜
        """
        # Softmaxë¡œ ì—°ì†ì  ê°€ì¤‘ì¹˜ ê³„ì‚°
        weights = F.softmax(excess_loss / self.temperature, dim=-1)  # [B, S, H]
        
        # í†µê³„ ë¡œê¹…
        weight_std = weights.std()
        console.print(f"[cyan]Weighted: std={weight_std:.3f}[/cyan]")
        
        return weights  # selection_mask ì—†ì´ weightsë§Œ ë°˜í™˜
    
    def _apply_ce_threshold(self, excess_loss: torch.Tensor) -> torch.Tensor:
        """
        Phase 1.2: CE Difference Threshold ì ìš© - ë…¸ì´ì¦ˆ í•„í„°ë§.
        
        ë„ˆë¬´ ì‘ì€ CE ì°¨ì´ëŠ” ë…¸ì´ì¦ˆë¡œ ê°„ì£¼í•˜ê³  0ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬
        ê°€ì¤‘ì¹˜ ê³„ì‚°ì—ì„œ ì œì™¸í•©ë‹ˆë‹¤.
        
        Args:
            excess_loss: [B, S, H] - ì›ë³¸ excess loss ê°’
            
        Returns:
            filtered_excess_loss: [B, S, H] - threshold ì ìš©ëœ excess loss
        """
        if self.min_ce_diff <= 0:
            return excess_loss  # threshold ë¹„í™œì„±í™” ì‹œ ì›ë³¸ ê°’ ë°˜í™˜
            
        # Threshold ì ìš©: min_ce_diff ë¯¸ë§Œì€ 0ìœ¼ë¡œ ì²˜ë¦¬
        filtered_loss = torch.where(
            excess_loss >= self.min_ce_diff,
            excess_loss,
            torch.zeros_like(excess_loss)
        )
        
        # Edge case ì²˜ë¦¬: ëª¨ë“  ê°’ì´ threshold ë¯¸ë§Œì¸ ê²½ìš°
        B, S, H = filtered_loss.shape
        
        # ë°°ì¹˜ë³„ë¡œ ì²˜ë¦¬
        for b in range(B):
            batch_loss = filtered_loss[b]  # [S, H]
            
            # ìœ íš¨í•œ ê°’ì´ í•˜ë‚˜ë¼ë„ ìˆëŠ”ì§€ í™•ì¸
            if torch.all(batch_loss == 0):
                # ëª¨ë“  ê°’ì´ 0ì´ë©´ uniform weight fallback
                console.print(
                    f"[yellow]âš ï¸ Batch {b}: All excess_loss < {self.min_ce_diff}, using uniform weights[/yellow]"
                )
                # ê· ë“± ê°€ì¤‘ì¹˜ë¡œ ëŒ€ì²´ (1/H ëŒ€ì‹  1.0 ì‚¬ìš© - softmaxì—ì„œ ì •ê·œí™”ë¨)
                filtered_loss[b] = torch.ones_like(batch_loss)
        
        return filtered_loss

    def train_step(self, batch: dict[str, Any]) -> dict[str, Any]:
        """Rho-1 WMTP í›ˆë ¨ ìŠ¤í… - Reference CE ë¹„êµ ê¸°ë°˜ ë™ì  ê°€ì¤‘ì¹˜ WMTP ì†ì‹¤ ê³„ì‚°.

        Args:
            batch: í›ˆë ¨ ë°°ì¹˜ ë°ì´í„° (input_ids, labels, attention_mask ë“±)

        Returns:
            ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬ (loss, lr, rho1 íŠ¹í™” ë©”íŠ¸ë¦­ í¬í•¨)
        """
        if self.model is None:
            raise RuntimeError("Trainer not initialized. Call setup() first.")

        self.model.train()

        # ë°°ì¹˜ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        batch = {k: v.to(self.device) if torch.is_tensor(v) else v
                 for k, v in batch.items()}

        target_ids: torch.Tensor = batch["labels"]  # [B, S]
        input_ids: torch.Tensor = batch.get("input_ids")

        if input_ids is None:
            raise ValueError("Rho1WmtpTrainer requires 'input_ids' in batch")

        # autocast ë””ë°”ì´ìŠ¤ íƒ€ì… ê²°ì •
        if torch.cuda.is_available():
            autocast_device = "cuda"
        elif torch.backends.mps.is_available() and str(self.device).startswith("mps"):
            autocast_device = "cpu"  # MPSëŠ” ì•„ì§ autocast ë¯¸ì§€ì›
        else:
            autocast_device = "cpu"

        with torch.autocast(
            device_type=autocast_device,
            dtype=self._amp_dtype,
        ):
            # ëª¨ë¸ forward pass
            outputs: dict[str, Any] | torch.Tensor = self.model(**batch)

            if isinstance(outputs, dict) and "logits" in outputs:
                logits = outputs["logits"]  # [B, S, H, V] ì˜ˆìƒ
            else:
                logits = outputs  # tensorë¼ê³  ê°€ì •

            # Shape ê²€ì¦
            if logits.ndim != 4:
                raise ValueError(
                    f"Expected logits shape [B,S,H,V], got {tuple(logits.shape)}"
                )

            # gradient í™œì„±í™”
            if not logits.requires_grad:
                logits = logits.detach().requires_grad_(True)

            # ğŸ¯ ë‹¨ê³„ 1: MTP í—¤ë“œë³„ CE ê³„ì‚° (ì„ì‹œ ê· ë“± ê°€ì¤‘ì¹˜ë¡œ)
            B, S, H, V = logits.shape
            temp_weights = torch.ones((B, S, H), device=logits.device, dtype=logits.dtype)

            _, valid_mask, ce_per_head = compute_weighted_mtp_loss(
                logits=logits,
                target_ids=target_ids,
                head_weights=temp_weights,
                horizon=self.horizon,
                ignore_index=-100,
            )

            # ğŸ¯ ë‹¨ê³„ 2: Rho-1 ê°€ì¤‘ì¹˜ ê³„ì‚° (Reference CE ë¹„êµ)
            result = self.compute_head_weights(
                logits, target_ids, ce_per_head, input_ids=input_ids
            )
            
            # ë°˜í™˜ê°’ íƒ€ì…ì— ë”°ë¼ ì²˜ë¦¬
            if isinstance(result, tuple):
                head_weights, selection_mask = result
            else:
                head_weights = result
                selection_mask = None  # Weighted mode

            # ğŸ¯ ë‹¨ê³„ 3: ìµœì¢… ê°€ì¤‘ WMTP ì†ì‹¤ ê³„ì‚°
            weighted_loss, valid_mask, ce_per_head = compute_weighted_mtp_loss(
                logits=logits,  # [B, S, H, V]
                target_ids=target_ids,  # [B, S]
                head_weights=head_weights,  # [B, S, H] - Rho-1 ê°€ì¤‘ì¹˜
                selection_mask=selection_mask,  # [B, S, H] - Token skip mask (ìƒˆë¡œ ì¶”ê°€)
                horizon=self.horizon,
                ignore_index=-100,
            )

            # Lambda scaling
            lambda_w = float(self.loss_cfg.get("lambda", 0.3))
            loss = lambda_w * weighted_loss  # ìµœì¢… ìŠ¤ì¹¼ë¼ ì†ì‹¤

        # ì—­ì „íŒŒ ë° ìµœì í™”
        loss.backward()

        # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
        grad_clip = float(getattr(self.optimizer, "grad_clip", 1.0))
        if math.isfinite(grad_clip) and grad_clip > 0:
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), grad_clip)

        self.optimizer.step()
        self.optimizer.zero_grad()

        self.global_step += 1

        # MLflow ë¡œê¹… (ì„ íƒì )
        if self.mlflow is not None:
            try:
                # í—¤ë“œë³„ CE í‰ê·  (ì§„ë‹¨ìš©)
                with torch.no_grad():
                    ce_head_means = []
                    for k in range(H):
                        shift = k + 1
                        valid_len = S - shift
                        if valid_len <= 0:
                            ce_head_means.append(torch.tensor(0.0, device=logits.device))
                            continue
                        logits_k = logits[:, :valid_len, k, :]
                        labels_k = target_ids[:, shift : shift + valid_len]
                        ce_k = F.cross_entropy(
                            logits_k.transpose(1, 2),
                            labels_k,
                            ignore_index=-100,
                            reduction="none",
                        )
                        ce_head_means.append(ce_k.mean())
                    ce_head_means = torch.stack(ce_head_means)

                    # ê¸°ë³¸ ë©”íŠ¸ë¦­
                    metrics = {
                        f"train/ce_head_{i}": float(x)
                        for i, x in enumerate(ce_head_means)
                    }
                    metrics.update({
                        "train/loss": float(loss.detach().item()),
                        "train/ce_mean": float(
                            (ce_per_head[valid_mask.unsqueeze(-1).expand(-1, -1, H)]).mean().item()
                        ) if valid_mask.any() else 0.0,
                    })

                    # ê°€ì¤‘ì¹˜ í†µê³„ (Rho-1 ê°€ì¤‘ì¹˜ ë¶„ì„ìš©)
                    w_eff = head_weights[valid_mask.unsqueeze(-1).expand(-1, -1, H)]
                    if w_eff.numel() > 0:
                        weight_stats = {
                            "train/weight_mean": float(w_eff.mean().item()),
                            "train/weight_min": float(w_eff.min().item()),
                            "train/weight_max": float(w_eff.max().item()),
                            "train/weight_std": float(w_eff.std().item()),
                        }

                        # ê°€ì¤‘ì¹˜ ë¶„í¬ ë°±ë¶„ìœ„ìˆ˜
                        try:
                            weight_stats.update({
                                "train/weight_p25": float(torch.quantile(w_eff, 0.25).item()),
                                "train/weight_p75": float(torch.quantile(w_eff, 0.75).item()),
                                "train/weight_p95": float(torch.quantile(w_eff, 0.95).item()),
                            })
                        except Exception:
                            sorted_w = torch.sort(w_eff)[0]
                            n = sorted_w.numel()
                            weight_stats.update({
                                "train/weight_p25": float(sorted_w[int(n * 0.25)].item()),
                                "train/weight_p75": float(sorted_w[int(n * 0.75)].item()),
                                "train/weight_p95": float(sorted_w[int(n * 0.95)].item()),
                            })

                        weight_stats.update({
                            "train/nan_weights": int((~torch.isfinite(head_weights)).sum().item()),
                            "train/extreme_weights": int((head_weights > 5.0).sum().item()),
                        })

                        metrics.update(weight_stats)

                    # Rho-1 íŠ¹í™” ë©”íŠ¸ë¦­ (excess loss ë¶„ì„)
                    try:
                        # Reference CE ì¬ê³„ì‚° (ë¡œê¹…ìš©)
                        ref_ce_all = self.compute_reference_ce(input_ids, target_ids)
                        aligned_ref_ce = self.align_ref_ce_to_mtp(ref_ce_all, ce_per_head)
                        excess_loss = torch.abs(ce_per_head - aligned_ref_ce)

                        excess_eff = excess_loss[valid_mask.unsqueeze(-1).expand(-1, -1, H)]
                        if excess_eff.numel() > 0:
                            # Excess loss í†µê³„
                            metrics.update({
                                "train/rho1_excess_mean": float(excess_eff.mean().item()),
                                "train/rho1_excess_std": float(excess_eff.std().item()),
                                "train/rho1_excess_max": float(excess_eff.max().item()),
                            })

                            # ë†’ì€ excess loss í† í° ë¹„ìœ¨ (ì¤‘ìš” í† í° ë¹„ìœ¨)
                            threshold = excess_eff.mean() + excess_eff.std()
                            important_tokens = float((excess_eff > threshold).sum().item())
                            total_tokens = float(excess_eff.numel())
                            metrics["train/rho1_important_ratio"] = (
                                important_tokens / total_tokens if total_tokens > 0 else 0.0
                            )

                            metrics["train/rho1_algorithm"] = 1  # Rho-1 í”Œë˜ê·¸
                            metrics["train/rho1_temperature"] = self.temperature
                    except Exception:
                        # Reference CE ê³„ì‚° ì‹¤íŒ¨ì‹œ ë¬´ì‹œ
                        pass

                    # ìœ íš¨ í† í° ë¹„ìœ¨
                    total_tokens = float(valid_mask.numel())
                    valid_tokens = float(valid_mask.sum().item())
                    metrics["train/valid_token_ratio"] = (
                        valid_tokens / total_tokens if total_tokens > 0 else 0.0
                    )

                self.mlflow.log_metrics(metrics, step=self.global_step)
            except Exception:
                # ë¡œê¹… ì˜¤ë¥˜ë¡œ í›ˆë ¨ ì¤‘ë‹¨ ë°©ì§€
                pass

        # ì‹¤íŒ¨ ê°ì§€ (NaN/Inf ì²´í¬)
        if (
            not torch.isfinite(loss)
            or not torch.isfinite(ce_per_head).all()
            or not torch.isfinite(head_weights).all()
        ):
            if self.mlflow is not None:
                try:
                    self.mlflow.log_metrics(
                        {"train/failure": 1.0}, step=self.global_step
                    )
                except Exception:
                    pass
            raise RuntimeError(
                "Detected NaN/Inf in loss or inputs; aborting training step."
            )

        return {
            "loss": float(loss.detach().item()),
            "lr": float(getattr(self.optimizer, "_last_lr", 0.0)),
        }