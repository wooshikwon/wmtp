"""
Critic WMTP Trainer - ê°•í™”í•™ìŠµ ê°€ì¹˜í•¨ìˆ˜ ê¸°ë°˜ WMTP ì•Œê³ ë¦¬ì¦˜ (Scorer í†µí•© ë²„ì „)

ê°€ì¹˜í•¨ìˆ˜ì˜ ë¸íƒ€(Î´_t = V_t - V_{t-1})ë¥¼ ì‚¬ìš©í•˜ì—¬ í† í° ì¤‘ìš”ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
"ë¯¸ë˜ì— ë” í° ë³´ìƒì„ ê°€ì ¸ë‹¤ì£¼ëŠ” í† í° = ë” ì¤‘ìš”í•œ í† í°"ì´ë¼ëŠ” ì§ê´€ì„ êµ¬í˜„í•©ë‹ˆë‹¤.

[ë¦¬íŒ©í† ë§ v2.1.0]
- CriticDeltaScorer ì™„ì „ í†µí•©: Value Head ë° Delta ê³„ì‚° ë¡œì§ì„ ì§ì ‘ êµ¬í˜„
- ì„±ëŠ¥ í–¥ìƒ: scorer.run() í˜¸ì¶œ ì˜¤ë²„í—¤ë“œ ì œê±°
- ì½”ë“œ ëª…í™•ì„±: Critic ë¡œì§ì´ í•œ íŒŒì¼ì— ì§‘ì¤‘

íŠ¹ì§•:
- Value Head ì§ì ‘ ê´€ë¦¬: nn.Sequential(Linear, ReLU, Linear) êµ¬ì¡°
- TD Error ê³„ì‚°: Î´_t = V_t - Î»V_{t-1} ì§ì ‘ êµ¬í˜„
- í—¤ë“œë³„ ê°€ì¤‘ì¹˜: softmax([Î´_{t+1}, Î´_{t+2}, Î´_{t+3}, Î´_{t+4}]) ì§ì ‘ ê³„ì‚°
- Stage 1 ì²´í¬í¬ì¸íŠ¸ ì§€ì›: ì‚¬ì „í•™ìŠµëœ Value Head ë¡œë“œ ê°€ëŠ¥

ìˆ˜í•™ì  ê³µì‹:
    Î´_t = V_t - Î»V_{t-1} (TD error ê´€ì ì˜ ê°€ì¹˜ ì¦ê°€ëŸ‰)
    w_{t+k} = softmax([Î´_{t+1}, Î´_{t+2}, Î´_{t+3}, Î´_{t+4}])_k
    L_WMTP = Î£(k=0 to H-1) w_{t+k} Ã— CE_k
"""

from __future__ import annotations

from typing import Any

import torch
import torch.nn as nn
import torch.nn.functional as F
from rich.console import Console

from src.components.registry import trainer_registry
from src.components.trainer.base_wmtp_trainer import (
    BaseWmtpTrainer,
    compute_weighted_mtp_loss,
)
from src.utils.reward_utils import compute_sequence_rewards

console = Console()


@trainer_registry.register("critic-wmtp", category="trainer", version="2.1.0")
class CriticWmtpTrainer(BaseWmtpTrainer):
    """Critic WMTP íŠ¸ë ˆì´ë„ˆ - ê°€ì¹˜í•¨ìˆ˜ ë¸íƒ€ ê¸°ë°˜ WMTP ì•Œê³ ë¦¬ì¦˜.

    ì—°êµ¬ ì² í•™ "Not All Tokens Are What You Need"ì˜ ê°•í™”í•™ìŠµ êµ¬í˜„:
        ê°•í™”í•™ìŠµì˜ ê°€ì¹˜í•¨ìˆ˜ë¥¼ í™œìš©í•˜ì—¬ ê° í† í°ì˜ ë¯¸ë˜ ë³´ìƒ ê¸°ì—¬ë„ë¥¼ ì¸¡ì •í•˜ê³ ,
        ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë™ì  ê°€ì¤‘ì¹˜ë¥¼ ê³„ì‚°í•˜ì—¬ WMTP í•™ìŠµì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

    ğŸ”¬ í•µì‹¬ ë™ì‘:
        1. CriticDeltaScorerë¥¼ í†µí•´ ê°€ì¹˜í•¨ìˆ˜ ë¸íƒ€ ê³„ì‚°: Î´_t = V_t - Î»V_{t-1}
        2. ë¸íƒ€ë¥¼ í—¤ë“œë³„ ê°€ì¤‘ì¹˜ë¡œ ë³€í™˜: softmax([Î´_{t+1}, Î´_{t+2}, Î´_{t+3}, Î´_{t+4}])
        3. ê°€ì¤‘ëœ MTP ì†ì‹¤ ê³„ì‚°: L = Î£ w_{t+k} Ã— CE_k

    2ë‹¨ê³„ í•™ìŠµ í”„ë¡œì„¸ìŠ¤:
        Stage 1: ê°€ì¹˜í—¤ë“œ ì‚¬ì „í•™ìŠµ (CriticStage1Pretrainer ì‚¬ìš©)
        - RM ë³´ìƒìœ¼ë¡œë¶€í„° í† í°ë³„ ê°€ì¹˜ ëª©í‘œê°’ ìƒì„±
        - Value Headê°€ í† í°ë³„ ëˆ„ì  ë³´ìƒì„ ì •í™•íˆ ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµ

        Stage 2: Critic-WMTP í•™ìŠµ (ì´ í´ë˜ìŠ¤)
        - ì‚¬ì „í•™ìŠµëœ ê°€ì¹˜í—¤ë“œë¡œ í† í°ë³„ ì¤‘ìš”ë„ ê³„ì‚°
        - ë™ì  ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•œ WMTP ì†ì‹¤ë¡œ ëª¨ë¸ í›ˆë ¨

    ì¥ì :
        - ë™ì  ì ì‘: ê° ì‹œí€€ìŠ¤ì™€ ìœ„ì¹˜ì— ë§ëŠ” ìµœì  ê°€ì¤‘ì¹˜ ê³„ì‚°
        - ë†’ì€ í‘œí˜„ë ¥: ë³µì¡í•œ í† í° íŒ¨í„´ì˜ ì¤‘ìš”ë„ í•™ìŠµ ê°€ëŠ¥
        - ì´ë¡ ì  ê·¼ê±°: ê°•í™”í•™ìŠµì˜ ê°€ì¹˜í•¨ìˆ˜ ì´ë¡ ì— ê¸°ë°˜

    ì‚¬ìš© ì‚¬ë¡€:
        - ë³µì¡í•œ ì¶”ë¡ ì´ í•„ìš”í•œ íƒœìŠ¤í¬ (ìˆ˜í•™, ì½”ë”© ë“±)
        - í† í° ê°„ ì¥ê¸° ì˜ì¡´ì„±ì´ ì¤‘ìš”í•œ ì‹œí€€ìŠ¤
        - ë†’ì€ ì„±ëŠ¥ì´ í•„ìš”í•œ í”„ë¡œë•ì…˜ í™˜ê²½
    """

    def __init__(self, config: dict[str, Any] | None = None):
        """Critic WMTP íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”.

        Value Headì™€ ê´€ë ¨ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì§ì ‘ ê´€ë¦¬í•©ë‹ˆë‹¤.
        """
        super().__init__(config)

        # Recipe ê¸°ë°˜ ì„¤ì • ë¡œë“œ (Factoryì—ì„œ ì „ë‹¬)
        self.critic_cfg = self.config.get("critic_config", {})

        # Phase 2.1: discount_lambda íŒŒë¼ë¯¸í„° (Recipeì—ì„œ)
        self.discount_lambda = float(
            self.critic_cfg.get("discount_lambda", 0.95)
        )  # TD error í• ì¸ìœ¨
        # self.temperatureëŠ” setup()ì—ì„œ ì„¤ì •

        # Value HeadëŠ” setup()ì—ì„œ ì´ˆê¸°í™”
        self.value_head: nn.Module | None = None
        self.rm_model: Any = None  # Reward Model ì €ì¥

    def setup(self, ctx: dict[str, Any]) -> None:
        """Value Headë¥¼ ì´ˆê¸°í™”í•˜ê³  í•„ìš”ì‹œ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë¡œë“œ.

        Stage 1ì—ì„œ ì‚¬ì „í•™ìŠµëœ Value Headë¥¼ Stage 2ì—ì„œë„ ê³„ì† í•™ìŠµí•©ë‹ˆë‹¤.
        """
        super().setup(ctx)

        # Weight temperature íŒŒë¼ë¯¸í„° ì„¤ì • (Phase 1 í†µí•©: recipe.loss.weight_temperatureì—ì„œ)
        # Backward compatibility: temperature â†’ weight_temperature
        self.temperature = float(
            self.loss_cfg.get("weight_temperature")
            or self.loss_cfg.get("temperature", 0.7)
        )

        # RM model ì €ì¥ (Stage 2ì—ì„œë„ ì‚¬ìš©)
        self.rm_model = ctx.get("rm_model")  # Value loss ê³„ì‚°ìš©

        # ëª¨ë¸ hidden size ê°€ì ¸ì˜¤ê¸° (ëª¨ë¸ì—ì„œ ì§ì ‘ ì¶”ì¶œ)
        hidden_size = None
        if hasattr(self.model, "config"):
            # HuggingFace ìŠ¤íƒ€ì¼ ëª¨ë¸
            hidden_size = getattr(
                self.model.config,
                "hidden_size",
                getattr(self.model.config, "n_embd", None),
            )

        if hidden_size is None:
            # ctxì—ì„œ ì‹œë„
            hidden_size = ctx.get("hidden_size")

        if hidden_size is None:
            raise ValueError(
                f"Failed to extract hidden_size from model. "
                f"Model config attributes: {dir(self.model.config) if hasattr(self.model, 'config') else 'No config'}"
            )

        # Value Head ì´ˆê¸°í™”
        if self.value_head is None:
            self.value_head = nn.Sequential(
                nn.Linear(hidden_size, hidden_size // 2),  # ì€ë‹‰ìƒíƒœ â†’ ì¤‘ê°„ì¸µ
                nn.ReLU(),  # ë¹„ì„ í˜• í™œì„±í™”
                nn.Linear(hidden_size // 2, 1),  # ì¤‘ê°„ì¸µ â†’ ìŠ¤ì¹¼ë¼ ê°€ì¹˜
            )

        # Stage 1ì—ì„œ í•™ìŠµí•œ Value Head ê°€ì¤‘ì¹˜ ë¡œë“œ
        # Pipelineì´ Stage 1 ê²°ê³¼ë¥¼ value_head_pathë¡œ ì „ë‹¬
        value_head_path = ctx.get("value_head_path")

        if value_head_path:
            try:
                # GPU í™˜ê²½ ì¼ê´€ì„±: í˜„ì¬ deviceì— ë§ê²Œ ë¡œë“œ
                map_location = self.device if self.device else "cpu"
                state = torch.load(value_head_path, map_location=map_location)
                self.value_head.load_state_dict(state)
                console.print(
                    f"[green]âœ“ Loaded Stage 1 Value Head from {value_head_path} to {map_location}[/green]"
                )
            except Exception as e:
                console.print(
                    f"[yellow]âš  Failed to load Stage 1 Value Head: {e}[/yellow]"
                )
                console.print("[yellow]  Using random initialization instead[/yellow]")
        else:
            console.print(
                "[yellow]â„¹ No Stage 1 Value Head provided, using random initialization[/yellow]"
            )

        # Value Headë¥¼ ëª¨ë¸ê³¼ ê°™ì€ deviceë¡œ ì´ë™
        if self.device:
            self.value_head = self.value_head.to(self.device)

        # Value Headë¥¼ optimizerì— í¬í•¨ (Stage 2 continuous learning)
        if self.optimizer is not None:
            # Critic ì„¤ì • ê°€ì ¸ì˜¤ê¸° (value_lr ë“±)
            critic_config = (
                ctx.get("recipe", {}).critic
                if hasattr(ctx.get("recipe", {}), "critic")
                else {}
            )
            value_lr = (
                float(critic_config.get("value_lr", 5e-5))
                if isinstance(critic_config, dict)
                else 5e-5
            )

            # Value Head parametersë¥¼ ë³„ë„ param groupìœ¼ë¡œ ì¶”ê°€
            self.optimizer.add_param_group(
                {
                    "params": self.value_head.parameters(),
                    "lr": value_lr,  # Higher LR for value head
                }
            )
            console.print(
                f"[green]âœ“ Value Head added to optimizer with lr={value_lr}[/green]"
            )

    def _compute_deltas(self, values: torch.Tensor) -> torch.Tensor:
        """TD error ê³„ì‚°: Î´_t = V_t - Î»V_{t-1}.

        Args:
            values: [B, S] í˜•íƒœì˜ value predictions

        Returns:
            deltas: [B, S] í˜•íƒœì˜ TD errors
        """
        B, S = values.shape

        # V_{-1} = 0ìœ¼ë¡œ ê°€ì •í•˜ì—¬ ì´ì „ ê°’ ì¤€ë¹„
        zeros = torch.zeros((B, 1), device=values.device, dtype=values.dtype)
        prev_values = torch.cat([zeros, values[:, :-1]], dim=1)  # [B, S]

        # TD error with discount
        deltas = values - self.discount_lambda * prev_values
        return deltas

    def _compute_head_weights_from_values(
        self, values: torch.Tensor, valid_mask: torch.Tensor
    ) -> torch.Tensor:
        """ê°€ì¹˜í•¨ìˆ˜ë¡œë¶€í„° í—¤ë“œë³„ ê°€ì¤‘ì¹˜ ê³„ì‚°.

        Position tì—ì„œ 4ê°œ í—¤ë“œì— ëŒ€ì‘í•˜ëŠ” ê°€ì¤‘ì¹˜:
        - Head 0 (ì˜ˆì¸¡ t+1): Î´_{t+1} = V_{t+1} - Î»V_t
        - Head 1 (ì˜ˆì¸¡ t+2): Î´_{t+2} = V_{t+2} - Î»V_{t+1}
        - Head 2 (ì˜ˆì¸¡ t+3): Î´_{t+3} = V_{t+3} - Î»V_{t+2}
        - Head 3 (ì˜ˆì¸¡ t+4): Î´_{t+4} = V_{t+4} - Î»V_{t+3}

        Args:
            values: [B, S] í˜•íƒœì˜ value predictions
            valid_mask: [B, S] í˜•íƒœì˜ ìœ íš¨ í† í° ë§ˆìŠ¤í¬

        Returns:
            head_weights: [B, S, H] í˜•íƒœì˜ í—¤ë“œë³„ ê°€ì¤‘ì¹˜
        """
        B, S = values.shape
        H = self.horizon

        # Delta ê³„ì‚°
        deltas = self._compute_deltas(values)  # [B, S]

        # í—¤ë“œë³„ ê°€ì¤‘ì¹˜ ê³„ì‚°
        head_weights = torch.zeros((B, S, H), device=values.device, dtype=values.dtype)

        for t in range(S):
            delta_list = []
            valid_heads = []

            for k in range(H):
                future_pos = t + k + 1  # kë²ˆì§¸ í—¤ë“œëŠ” t+(k+1) ì˜ˆì¸¡

                if future_pos < S:
                    # ìœ íš¨í•œ ìœ„ì¹˜ì˜ delta ì‚¬ìš©
                    delta_k = deltas[:, future_pos]  # [B]
                    delta_list.append(delta_k)
                    valid_heads.append(k)
                else:
                    # ì‹œí€€ìŠ¤ ëì„ ë„˜ì–´ê°€ëŠ” ê²½ìš° ë§¤ìš° ì‘ì€ ê°’
                    delta_k = torch.full(
                        (B,), -10.0, device=values.device, dtype=values.dtype
                    )
                    delta_list.append(delta_k)

            if delta_list:
                # Stack deltas for all heads: [B, H]
                delta_tensor = torch.stack(delta_list, dim=1)

                # ìœ íš¨í•˜ì§€ ì•Šì€ í—¤ë“œì— ëŒ€í•œ ë§ˆìŠ¤í‚¹ (Softmax ì „!)
                # ì‹œí€€ìŠ¤ ê²½ê³„ë¥¼ ë„˜ëŠ” ì˜ˆì¸¡ì€ ë§¤ìš° ì‘ì€ ê°’ìœ¼ë¡œ ì„¤ì •í•˜ì—¬
                # Softmax í›„ ìì—°ìŠ¤ëŸ½ê²Œ 0ì— ê°€ê¹Œì›Œì§€ë„ë¡ í•¨
                for k in range(H):
                    if t + k + 1 >= S:
                        delta_tensor[:, k] = -1e10

                # Softmax with temperature (ì´ì œ gradient-safe)
                weights_t = F.softmax(delta_tensor / self.temperature, dim=1)  # [B, H]

                # ìˆ˜ì¹˜ ì•ˆì •ì„±ì„ ìœ„í•œ í´ë¦¬í•‘ë§Œ ì ìš© (inplace ìˆ˜ì • ì—†ìŒ)
                weights_t = torch.clamp(weights_t, min=1e-8, max=1.0)

                head_weights[:, t, :] = weights_t

        # Valid mask ì ìš©
        head_weights = head_weights * valid_mask.unsqueeze(-1)

        return head_weights

    @torch.no_grad()
    def _compute_sequence_rewards(
        self,
        rm_model: Any,
        input_ids: torch.Tensor,
        attention_mask: torch.Tensor | None = None,
        amp_dtype: torch.dtype | None = None,
    ) -> torch.Tensor:
        """Stage 1ê³¼ ë™ì¼í•œ reward ê³„ì‚° (ê³µí†µ ìœ í‹¸ë¦¬í‹° ì‚¬ìš©)"""

        if rm_model is None:
            # Fallback: use negative CE as pseudo reward
            return self._compute_pseudo_rewards(input_ids, attention_mask)

        # ê³µí†µ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì‚¬ìš©
        return compute_sequence_rewards(rm_model, input_ids, attention_mask, amp_dtype)

    @torch.no_grad()
    def _compute_pseudo_rewards(
        self,
        input_ids: torch.Tensor,
        attention_mask: torch.Tensor | None = None,
    ) -> torch.Tensor:
        """Pseudo rewards using negative CE from base model"""

        # Use common utility with base model as fallback RM
        return compute_sequence_rewards(
            self.model, input_ids, attention_mask, amp_dtype=self._amp_dtype
        )

    def _compute_gae_returns(
        self,
        rewards: torch.Tensor,
        values: torch.Tensor,
        gamma: float = 0.99,
        gae_lambda: float = 0.95,
    ) -> torch.Tensor:
        """í† í°ë³„ value target ê³„ì‚° (Stage 1ê³¼ ë™ì¼)"""
        B, S = values.shape
        returns = torch.zeros_like(values)

        for b in range(B):
            gae = 0
            for t in reversed(range(S)):
                next_val = 0.0 if t == S - 1 else values[b, t + 1]

                # TD error
                delta = rewards[b, t] + gamma * next_val - values[b, t]

                # GAE
                gae = delta + gamma * gae_lambda * gae
                returns[b, t] = values[b, t] + gae

        return returns

    def compute_head_weights(
        self,
        logits: torch.Tensor,  # noqa: ARG002
        target_labels: torch.Tensor,
        **kwargs  # noqa: ARG002
    ) -> torch.Tensor:
        """Value Headë¥¼ ì‚¬ìš©í•œ ì§ì ‘ í—¤ë“œ ê°€ì¤‘ì¹˜ ê³„ì‚°.

        Hidden statesë¡œë¶€í„° ê°€ì¹˜í•¨ìˆ˜ë¥¼ ì˜ˆì¸¡í•˜ê³  TD errorë¥¼ ê³„ì‚°í•˜ì—¬
        MTP í—¤ë“œë³„ ê°€ì¤‘ì¹˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

        Args:
            logits: MTP ëª¨ë¸ ì¶œë ¥ [B, S, H, V]
            target_labels: 3D íƒ€ê²Ÿ ë¼ë²¨ [B, S, H] - MTPDataCollator ìƒì„±
            **kwargs: hidden_states ë“± ì¶”ê°€ ì •ë³´

        Returns:
            head_weights: Critic ê¸°ë°˜ ê°€ì¤‘ì¹˜ [B, S, H]

        Raises:
            RuntimeError: Value Headê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì€ ê²½ìš°
            ValueError: hidden_statesê°€ ì—†ëŠ” ê²½ìš°
        """
        if self.value_head is None:
            raise RuntimeError("Value Head not initialized. Call setup() first.")

        # Hidden states ì¶”ì¶œ
        hidden_states = kwargs.get("hidden_states")
        if hidden_states is None:
            raise ValueError(
                "CriticWmtpTrainer requires 'hidden_states' from model outputs. "
                "Ensure the model returns hidden states."
            )

        B, S, H = target_labels.shape

        # Hidden states shape ê²€ì¦
        if hidden_states.ndim != 3:
            raise ValueError(
                f"Expected hidden_states shape [B, S, D], got {hidden_states.shape}"
            )

        # Value Headë¥¼ ê°™ì€ deviceë¡œ ì´ë™
        if self.value_head.training != self.model.training:
            self.value_head.train(self.model.training)

        # Value prediction
        with torch.set_grad_enabled(self.model.training):
            # [B, S, D] -> [B*S, D] -> [B*S, 1] -> [B, S]
            B_hs, S_hs, D = hidden_states.shape
            values = (
                self.value_head(hidden_states.view(B_hs * S_hs, D))
                .view(B_hs, S_hs)
                .squeeze(-1)
            )

            # Shape alignment if needed
            if values.shape[0] != B or values.shape[1] != S:
                values = values[:B, :S]

        # Valid mask ê³„ì‚° (ignore_index=-100ì¸ í† í° ì œì™¸)
        # 3D ë¼ë²¨ì—ì„œ 2D ë§ˆìŠ¤í¬ ìƒì„± (ëª¨ë“  í—¤ë“œê°€ ë™ì¼í•œ ìœ íš¨ì„± ê°€ì •)
        valid_mask = (target_labels[:, :, 0] != -100).float()  # [B, S]

        # í—¤ë“œë³„ ê°€ì¤‘ì¹˜ ê³„ì‚°
        head_weights = self._compute_head_weights_from_values(values, valid_mask)

        # ë©”íŠ¸ë¦­ì„ ìœ„í•´ deltas ì €ì¥
        with torch.no_grad():
            deltas = self._compute_deltas(values)
            self._last_deltas = deltas
            self._last_values = values

        return head_weights

    def train_step(self, batch: dict[str, Any]) -> dict[str, Any]:
        """Critic WMTP í›ˆë ¨ ìŠ¤í… - ê°€ì¹˜í•¨ìˆ˜ ë¸íƒ€ ê¸°ë°˜ ë™ì  ê°€ì¤‘ì¹˜ WMTP ì†ì‹¤ ê³„ì‚°.

        Args:
            batch: í›ˆë ¨ ë°°ì¹˜ ë°ì´í„° (input_ids, labels, attention_mask ë“±)

        Returns:
            ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬ (loss, lr, critic íŠ¹í™” ë©”íŠ¸ë¦­ í¬í•¨)
        """
        if self.model is None:
            raise RuntimeError("Trainer not initialized. Call setup() first.")

        self.model.train()
        self.value_head.train()  # Value Headë„ í•™ìŠµ ëª¨ë“œë¡œ

        # ë°°ì¹˜ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        batch = {
            k: v.to(self.device) if torch.is_tensor(v) else v for k, v in batch.items()
        }

        input_ids = batch["input_ids"]
        target_labels: torch.Tensor = batch[
            "labels"
        ]  # [B, S, H] - MTPDataCollator ìƒì„±
        attention_mask = batch.get("attention_mask")

        # autocast ë””ë°”ì´ìŠ¤ íƒ€ì… ê²°ì •
        if torch.cuda.is_available():
            autocast_device = "cuda"
        elif torch.backends.mps.is_available() and str(self.device).startswith("mps"):
            autocast_device = "cpu"  # MPSëŠ” ì•„ì§ autocast ë¯¸ì§€ì›
        else:
            autocast_device = "cpu"

        with torch.autocast(
            device_type=autocast_device,
            dtype=self._amp_dtype,
        ):
            # ëª¨ë¸ forward pass (hidden_states í¬í•¨ ë°˜í™˜ í•„ìš”)
            outputs: dict[str, Any] | torch.Tensor = self.model(**batch)

            if isinstance(outputs, dict) and "logits" in outputs:
                logits = outputs["logits"]  # [B, S, H, V] ì˜ˆìƒ
            else:
                logits = outputs  # tensorë¼ê³  ê°€ì •

            # Shape ê²€ì¦
            if logits.ndim != 4:
                raise ValueError(
                    f"Expected logits shape [B,S,H,V], got {tuple(logits.shape)}"
                )

            # gradient í™œì„±í™”
            if not logits.requires_grad:
                logits = logits.detach().requires_grad_(True)

            # hidden_states ì¶”ì¶œ (CriticScorerì— í•„ìš”)
            from src.utils.model_utils import extract_hidden_states

            try:
                hidden_states = extract_hidden_states(outputs)
            except ValueError as e:
                raise RuntimeError(
                    f"CriticWmtpTrainer requires valid hidden_states [B,S,D] from model outputs. "
                    f"Error: {e}. Ensure your model is configured to return hidden states."
                ) from e

            # ğŸ¯ Critic WMTP: ê°€ì¹˜í•¨ìˆ˜ ë¸íƒ€ ê¸°ë°˜ ë™ì  ê°€ì¤‘ì¹˜ ê³„ì‚°
            head_weights = self.compute_head_weights(
                logits, target_labels, hidden_states=hidden_states
            )

            # Value prediction (gradient enabled for training)
            B, S, D = hidden_states.shape
            values = (
                self.value_head(hidden_states.view(B * S, D)).view(B, S).squeeze(-1)
            )  # [B, S]

            # WMTP ì†ì‹¤ ê³„ì‚° (ê°„ì†Œí™”ëœ 3D ë¼ë²¨ ê¸°ë°˜)
            weighted_loss, valid_mask, ce_per_head = compute_weighted_mtp_loss(
                logits=logits,  # [B, S, H, V]
                target_labels=target_labels,  # [B, S, H] - MTPDataCollator ìƒì„±
                head_weights=head_weights,  # [B, S, H] - ë™ì  ê°€ì¤‘ì¹˜
                ignore_index=-100,
                config=self.config,  # MPS ê²½ë¡œ íŒë‹¨ìš© ì„¤ì • ì „ë‹¬
            )

            # Value Loss ê³„ì‚° (auxiliary loss for continuous learning)
            value_loss = torch.tensor(0.0, device=self.device)
            critic_config = self.config.get("critic", {})
            if self.rm_model is not None or critic_config.get(
                "use_pseudo_rewards", True
            ):
                # Compute rewards
                rewards = self._compute_sequence_rewards(
                    self.rm_model, input_ids, attention_mask, amp_dtype=self._amp_dtype
                )

                # Spread rewards to tokens (uniform for simplicity)
                B, S = values.shape
                token_rewards = torch.zeros_like(values)
                for b in range(B):
                    seq_reward = rewards[b]
                    token_rewards[b, :] = seq_reward / S  # Uniform distribution

                # Compute returns using GAE
                with torch.no_grad():
                    returns = self._compute_gae_returns(
                        token_rewards,
                        values.detach(),
                        gamma=critic_config.get("gamma", 0.99),
                        gae_lambda=critic_config.get("gae_lambda", 0.95),
                    )

                # MSE loss for value prediction
                value_loss = F.mse_loss(
                    values[valid_mask.bool()], returns[valid_mask.bool()]
                )

            # Phase 2.2: Clean loss structure - main loss fixed at 1.0
            auxiliary_coef = float(critic_config.get("auxiliary_loss_coef", 0.1))

            # Main WMTP loss (1.0) + auxiliary value loss
            loss = weighted_loss + auxiliary_coef * value_loss

        # ì—­ì „íŒŒ ë° ìµœì í™”
        loss.backward()

        # ì—°êµ¬ ì·¨ì§€: Critic TDì˜ ìì—°ìŠ¤ëŸ¬ìš´ ì‹ í˜¸ë¥¼ ë³´ì¡´ (no gradient clipping)

        # ëŒ€ì‹  gradient ì•ˆì •ì„± ì²´í¬ (ì—°êµ¬ ì·¨ì§€ ìœ ì§€í•˜ë©´ì„œ ì•ˆì •ì„± í™•ë³´)
        total_norm = 0.0
        for p in list(self.model.parameters()) + list(self.value_head.parameters()):
            if p.grad is not None:
                param_norm = p.grad.data.norm(2)
                total_norm += param_norm.item() ** 2
        total_norm = total_norm ** (1.0 / 2)

        # ê·¹ë‹¨ì ì¸ ê²½ìš°ë§Œ ì²´í¬ (ìì—°ìŠ¤ëŸ¬ìš´ í•™ìŠµì€ í—ˆìš©)
        if total_norm > 100.0:  # Very high threshold for research integrity
            console.print(
                f"[yellow]âš  Large gradient norm detected: {total_norm:.2f} at step {self.global_step}[/yellow]"
            )

        self.optimizer.step()
        self.optimizer.zero_grad()

        self.global_step += 1

        # MLflow ë¡œê¹… (100 stepë§ˆë‹¤ + í•µì‹¬ ë©”íŠ¸ë¦­ë§Œ)
        if self.mlflow is not None and self.global_step % 100 == 0:
            try:
                # í•µì‹¬ ë©”íŠ¸ë¦­ë§Œ ë¡œê¹…
                metrics = {
                    "train/loss": float(loss.detach().item()),
                    "train/wmtp_loss": float(weighted_loss.item()),
                    "train/value_loss": float(value_loss.item()),
                }

                # Critic íŠ¹í™” ë©”íŠ¸ë¦­ (ì¤‘ìš”í•œ ê²ƒë§Œ)
                if hasattr(self, "_last_values") and self._last_values is not None:
                    metrics["train/value_mean"] = float(self._last_values.mean().item())

                # ê°€ì¤‘ì¹˜ í†µê³„ (í‰ê· ë§Œ)
                with torch.no_grad():
                    w_eff = head_weights[
                        valid_mask.unsqueeze(-1).expand(-1, -1, self.horizon)
                    ]
                    if w_eff.numel() > 0:
                        metrics["train/weight_mean"] = float(w_eff.mean().item())

                self.mlflow.log_metrics(metrics, step=self.global_step)
            except Exception:
                pass

        # ì‹¤íŒ¨ ê°ì§€ (NaN/Inf ì²´í¬)
        if not torch.isfinite(loss) or not torch.isfinite(head_weights).all():
            raise RuntimeError(
                f"NaN/Inf detected at step {self.global_step}; aborting training."
            )

        return {
            "loss": float(loss.detach().item()),
            "wmtp_loss": float(weighted_loss.item()),
            "value_loss": float(value_loss.item()),
            "lr": float(getattr(self.optimizer, "_last_lr", 0.0)),
        }
