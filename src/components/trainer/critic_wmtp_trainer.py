"""
Critic WMTP Trainer - ê°•í™”í•™ìŠµ ê°€ì¹˜í•¨ìˆ˜ ê¸°ë°˜ WMTP ì•Œê³ ë¦¬ì¦˜ (Scorer í†µí•© ë²„ì „)

ê°€ì¹˜í•¨ìˆ˜ì˜ ë¸íƒ€(Î´_t = V_t - V_{t-1})ë¥¼ ì‚¬ìš©í•˜ì—¬ í† í° ì¤‘ìš”ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
"ë¯¸ë˜ì— ë” í° ë³´ìƒì„ ê°€ì ¸ë‹¤ì£¼ëŠ” í† í° = ë” ì¤‘ìš”í•œ í† í°"ì´ë¼ëŠ” ì§ê´€ì„ êµ¬í˜„í•©ë‹ˆë‹¤.

[ë¦¬íŒ©í† ë§ v2.1.0]
- CriticDeltaScorer ì™„ì „ í†µí•©: Value Head ë° Delta ê³„ì‚° ë¡œì§ì„ ì§ì ‘ êµ¬í˜„
- ì„±ëŠ¥ í–¥ìƒ: scorer.run() í˜¸ì¶œ ì˜¤ë²„í—¤ë“œ ì œê±°
- ì½”ë“œ ëª…í™•ì„±: Critic ë¡œì§ì´ í•œ íŒŒì¼ì— ì§‘ì¤‘

íŠ¹ì§•:
- Value Head ì§ì ‘ ê´€ë¦¬: nn.Sequential(Linear, ReLU, Linear) êµ¬ì¡°
- TD Error ê³„ì‚°: Î´_t = V_t - Î»V_{t-1} ì§ì ‘ êµ¬í˜„
- í—¤ë“œë³„ ê°€ì¤‘ì¹˜: softmax([Î´_{t+1}, Î´_{t+2}, Î´_{t+3}, Î´_{t+4}]) ì§ì ‘ ê³„ì‚°
- Stage 1 ì²´í¬í¬ì¸íŠ¸ ì§€ì›: ì‚¬ì „í•™ìŠµëœ Value Head ë¡œë“œ ê°€ëŠ¥

ìˆ˜í•™ì  ê³µì‹:
    Î´_t = V_t - Î»V_{t-1} (TD error ê´€ì ì˜ ê°€ì¹˜ ì¦ê°€ëŸ‰)
    w_{t+k} = softmax([Î´_{t+1}, Î´_{t+2}, Î´_{t+3}, Î´_{t+4}])_k
    L_WMTP = Î£(k=0 to H-1) w_{t+k} Ã— CE_k
"""

from __future__ import annotations

import math
import os
from typing import Any

import torch
import torch.nn as nn
import torch.nn.functional as F
from rich.console import Console

from src.components.trainer.base_wmtp_trainer import BaseWmtpTrainer, compute_weighted_mtp_loss
from src.components.registry import trainer_registry

console = Console()


@trainer_registry.register("critic-wmtp", category="trainer", version="2.1.0")
class CriticWmtpTrainer(BaseWmtpTrainer):
    """Critic WMTP íŠ¸ë ˆì´ë„ˆ - ê°€ì¹˜í•¨ìˆ˜ ë¸íƒ€ ê¸°ë°˜ WMTP ì•Œê³ ë¦¬ì¦˜.

    ì—°êµ¬ ì² í•™ "Not All Tokens Are What You Need"ì˜ ê°•í™”í•™ìŠµ êµ¬í˜„:
        ê°•í™”í•™ìŠµì˜ ê°€ì¹˜í•¨ìˆ˜ë¥¼ í™œìš©í•˜ì—¬ ê° í† í°ì˜ ë¯¸ë˜ ë³´ìƒ ê¸°ì—¬ë„ë¥¼ ì¸¡ì •í•˜ê³ ,
        ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë™ì  ê°€ì¤‘ì¹˜ë¥¼ ê³„ì‚°í•˜ì—¬ WMTP í•™ìŠµì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

    ğŸ”¬ í•µì‹¬ ë™ì‘:
        1. CriticDeltaScorerë¥¼ í†µí•´ ê°€ì¹˜í•¨ìˆ˜ ë¸íƒ€ ê³„ì‚°: Î´_t = V_t - Î»V_{t-1}
        2. ë¸íƒ€ë¥¼ í—¤ë“œë³„ ê°€ì¤‘ì¹˜ë¡œ ë³€í™˜: softmax([Î´_{t+1}, Î´_{t+2}, Î´_{t+3}, Î´_{t+4}])
        3. ê°€ì¤‘ëœ MTP ì†ì‹¤ ê³„ì‚°: L = Î£ w_{t+k} Ã— CE_k

    2ë‹¨ê³„ í•™ìŠµ í”„ë¡œì„¸ìŠ¤:
        Stage 1: ê°€ì¹˜í—¤ë“œ ì‚¬ì „í•™ìŠµ (CriticStage1Pretrainer ì‚¬ìš©)
        - RM ë³´ìƒìœ¼ë¡œë¶€í„° í† í°ë³„ ê°€ì¹˜ ëª©í‘œê°’ ìƒì„±
        - Value Headê°€ í† í°ë³„ ëˆ„ì  ë³´ìƒì„ ì •í™•íˆ ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµ

        Stage 2: Critic-WMTP í•™ìŠµ (ì´ í´ë˜ìŠ¤)
        - ì‚¬ì „í•™ìŠµëœ ê°€ì¹˜í—¤ë“œë¡œ í† í°ë³„ ì¤‘ìš”ë„ ê³„ì‚°
        - ë™ì  ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•œ WMTP ì†ì‹¤ë¡œ ëª¨ë¸ í›ˆë ¨

    ì¥ì :
        - ë™ì  ì ì‘: ê° ì‹œí€€ìŠ¤ì™€ ìœ„ì¹˜ì— ë§ëŠ” ìµœì  ê°€ì¤‘ì¹˜ ê³„ì‚°
        - ë†’ì€ í‘œí˜„ë ¥: ë³µì¡í•œ í† í° íŒ¨í„´ì˜ ì¤‘ìš”ë„ í•™ìŠµ ê°€ëŠ¥
        - ì´ë¡ ì  ê·¼ê±°: ê°•í™”í•™ìŠµì˜ ê°€ì¹˜í•¨ìˆ˜ ì´ë¡ ì— ê¸°ë°˜

    ì‚¬ìš© ì‚¬ë¡€:
        - ë³µì¡í•œ ì¶”ë¡ ì´ í•„ìš”í•œ íƒœìŠ¤í¬ (ìˆ˜í•™, ì½”ë”© ë“±)
        - í† í° ê°„ ì¥ê¸° ì˜ì¡´ì„±ì´ ì¤‘ìš”í•œ ì‹œí€€ìŠ¤
        - ë†’ì€ ì„±ëŠ¥ì´ í•„ìš”í•œ í”„ë¡œë•ì…˜ í™˜ê²½
    """

    def __init__(self, config: dict[str, Any] | None = None):
        """Critic WMTP íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”.

        Value Headì™€ ê´€ë ¨ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì§ì ‘ ê´€ë¦¬í•©ë‹ˆë‹¤.
        """
        super().__init__(config)

        # Critic íŠ¹í™” í•˜ì´í¼íŒŒë¼ë¯¸í„°
        self.discount_lambda = self.config.get("discount_lambda", 0.95)  # TD error í• ì¸ìœ¨
        self.temperature = self.config.get("temperature", 0.7)  # Softmax ì˜¨ë„

        # Value HeadëŠ” setup()ì—ì„œ ì´ˆê¸°í™”
        self.value_head: nn.Module | None = None

    def setup(self, ctx: dict[str, Any]) -> None:
        """Value Headë¥¼ ì´ˆê¸°í™”í•˜ê³  í•„ìš”ì‹œ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë¡œë“œ.

        Stage 1ì—ì„œ ì‚¬ì „í•™ìŠµëœ Value Headë¥¼ Stage 2ì—ì„œ ì¬ì‚¬ìš©í•©ë‹ˆë‹¤.
        """
        super().setup(ctx)

        # ëª¨ë¸ hidden size ê°€ì ¸ì˜¤ê¸° (7B ëª¨ë¸ ê¸°ë³¸ê°’: 4096)
        hidden_size = ctx.get("hidden_size", 4096)

        # Value Head ì´ˆê¸°í™”
        if self.value_head is None:
            self.value_head = nn.Sequential(
                nn.Linear(hidden_size, hidden_size // 2),  # ì€ë‹‰ìƒíƒœ â†’ ì¤‘ê°„ì¸µ
                nn.ReLU(),  # ë¹„ì„ í˜• í™œì„±í™”
                nn.Linear(hidden_size // 2, 1),  # ì¤‘ê°„ì¸µ â†’ ìŠ¤ì¹¼ë¼ ê°€ì¹˜
            )

        # Stage 1ì—ì„œ í•™ìŠµí•œ Value Head ê°€ì¤‘ì¹˜ ë¡œë“œ (ì„ íƒì )
        value_head_path = ctx.get("value_head_path")
        if value_head_path:
            try:
                import os
                if os.path.exists(value_head_path):
                    state = torch.load(value_head_path, map_location="cpu")
                    self.value_head.load_state_dict(state)
                    console.print(f"[green]âœ“ Loaded Value Head from {value_head_path}[/green]")
            except Exception as e:
                console.print(f"[yellow]âš  Failed to load Value Head: {e}[/yellow]")
                # ì¹˜ëª…ì  ì˜¤ë¥˜ ì•„ë‹˜: ëœë¤ ì´ˆê¸°í™”ëœ í—¤ë“œë¡œ ê³„ì† ì§„í–‰

        # Value Headë¥¼ ëª¨ë¸ê³¼ ê°™ì€ deviceë¡œ ì´ë™
        if self.device:
            self.value_head = self.value_head.to(self.device)

    def _compute_deltas(self, values: torch.Tensor) -> torch.Tensor:
        """TD error ê³„ì‚°: Î´_t = V_t - Î»V_{t-1}.

        Args:
            values: [B, S] í˜•íƒœì˜ value predictions

        Returns:
            deltas: [B, S] í˜•íƒœì˜ TD errors
        """
        B, S = values.shape

        # V_{-1} = 0ìœ¼ë¡œ ê°€ì •í•˜ì—¬ ì´ì „ ê°’ ì¤€ë¹„
        zeros = torch.zeros((B, 1), device=values.device, dtype=values.dtype)
        prev_values = torch.cat([zeros, values[:, :-1]], dim=1)  # [B, S]

        # TD error with discount
        deltas = values - self.discount_lambda * prev_values
        return deltas

    def _compute_head_weights_from_values(
        self, values: torch.Tensor, valid_mask: torch.Tensor
    ) -> torch.Tensor:
        """ê°€ì¹˜í•¨ìˆ˜ë¡œë¶€í„° í—¤ë“œë³„ ê°€ì¤‘ì¹˜ ê³„ì‚°.

        Position tì—ì„œ 4ê°œ í—¤ë“œì— ëŒ€ì‘í•˜ëŠ” ê°€ì¤‘ì¹˜:
        - Head 0 (ì˜ˆì¸¡ t+1): Î´_{t+1} = V_{t+1} - Î»V_t
        - Head 1 (ì˜ˆì¸¡ t+2): Î´_{t+2} = V_{t+2} - Î»V_{t+1}
        - Head 2 (ì˜ˆì¸¡ t+3): Î´_{t+3} = V_{t+3} - Î»V_{t+2}
        - Head 3 (ì˜ˆì¸¡ t+4): Î´_{t+4} = V_{t+4} - Î»V_{t+3}

        Args:
            values: [B, S] í˜•íƒœì˜ value predictions
            valid_mask: [B, S] í˜•íƒœì˜ ìœ íš¨ í† í° ë§ˆìŠ¤í¬

        Returns:
            head_weights: [B, S, H] í˜•íƒœì˜ í—¤ë“œë³„ ê°€ì¤‘ì¹˜
        """
        B, S = values.shape
        H = self.horizon

        # Delta ê³„ì‚°
        deltas = self._compute_deltas(values)  # [B, S]

        # í—¤ë“œë³„ ê°€ì¤‘ì¹˜ ê³„ì‚°
        head_weights = torch.zeros((B, S, H), device=values.device, dtype=values.dtype)

        for t in range(S):
            delta_list = []
            valid_heads = []

            for k in range(H):
                future_pos = t + k + 1  # kë²ˆì§¸ í—¤ë“œëŠ” t+(k+1) ì˜ˆì¸¡

                if future_pos < S:
                    # ìœ íš¨í•œ ìœ„ì¹˜ì˜ delta ì‚¬ìš©
                    delta_k = deltas[:, future_pos]  # [B]
                    delta_list.append(delta_k)
                    valid_heads.append(k)
                else:
                    # ì‹œí€€ìŠ¤ ëì„ ë„˜ì–´ê°€ëŠ” ê²½ìš° ë§¤ìš° ì‘ì€ ê°’
                    delta_k = torch.full((B,), -10.0, device=values.device, dtype=values.dtype)
                    delta_list.append(delta_k)

            if delta_list:
                # Stack deltas for all heads: [B, H]
                delta_tensor = torch.stack(delta_list, dim=1)

                # Softmax with temperature
                weights_t = F.softmax(delta_tensor / self.temperature, dim=1)  # [B, H]

                # ìœ íš¨í•˜ì§€ ì•Šì€ í—¤ë“œëŠ” 0ìœ¼ë¡œ ë§ˆìŠ¤í‚¹
                for k in range(H):
                    if t + k + 1 >= S:
                        weights_t[:, k] = 0.0

                # ì¬ì •ê·œí™” (ìœ íš¨í•œ í—¤ë“œë§Œ)
                weights_sum = weights_t.sum(dim=1, keepdim=True).clamp(min=1e-8)
                weights_t = weights_t / weights_sum

                head_weights[:, t, :] = weights_t

        # Valid mask ì ìš©
        head_weights = head_weights * valid_mask.unsqueeze(-1)

        return head_weights

    def compute_head_weights(self, logits: torch.Tensor, target_ids: torch.Tensor, **kwargs) -> torch.Tensor:
        """Value Headë¥¼ ì‚¬ìš©í•œ ì§ì ‘ í—¤ë“œ ê°€ì¤‘ì¹˜ ê³„ì‚°.

        Hidden statesë¡œë¶€í„° ê°€ì¹˜í•¨ìˆ˜ë¥¼ ì˜ˆì¸¡í•˜ê³  TD errorë¥¼ ê³„ì‚°í•˜ì—¬
        MTP í—¤ë“œë³„ ê°€ì¤‘ì¹˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

        Args:
            logits: MTP ëª¨ë¸ ì¶œë ¥ [B, S, H, V]
            target_ids: íƒ€ê²Ÿ í† í° ID [B, S]
            **kwargs: hidden_states ë“± ì¶”ê°€ ì •ë³´

        Returns:
            head_weights: Critic ê¸°ë°˜ ê°€ì¤‘ì¹˜ [B, S, H]

        Raises:
            RuntimeError: Value Headê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì€ ê²½ìš°
            ValueError: hidden_statesê°€ ì—†ëŠ” ê²½ìš°
        """
        if self.value_head is None:
            raise RuntimeError(
                "Value Head not initialized. Call setup() first."
            )

        # Hidden states ì¶”ì¶œ
        hidden_states = kwargs.get("hidden_states")
        if hidden_states is None:
            raise ValueError(
                "CriticWmtpTrainer requires 'hidden_states' from model outputs. "
                "Ensure the model returns hidden states."
            )

        B, S = target_ids.shape

        # Hidden states shape ê²€ì¦
        if hidden_states.ndim != 3:
            raise ValueError(
                f"Expected hidden_states shape [B, S, D], got {hidden_states.shape}"
            )

        # Value Headë¥¼ ê°™ì€ deviceë¡œ ì´ë™
        if self.value_head.training != self.model.training:
            self.value_head.train(self.model.training)

        # Value prediction
        with torch.set_grad_enabled(self.model.training):
            # [B, S, D] -> [B*S, D] -> [B*S, 1] -> [B, S]
            B_hs, S_hs, D = hidden_states.shape
            values = self.value_head(
                hidden_states.view(B_hs * S_hs, D)
            ).view(B_hs, S_hs).squeeze(-1)

            # Shape alignment if needed
            if values.shape[0] != B or values.shape[1] != S:
                values = values[:B, :S]

        # Valid mask ê³„ì‚° (ignore_index=-100ì¸ í† í° ì œì™¸)
        valid_mask = (target_ids != -100).float()

        # í—¤ë“œë³„ ê°€ì¤‘ì¹˜ ê³„ì‚°
        head_weights = self._compute_head_weights_from_values(values, valid_mask)

        # ë©”íŠ¸ë¦­ì„ ìœ„í•´ deltas ì €ì¥
        with torch.no_grad():
            deltas = self._compute_deltas(values)
            self._last_deltas = deltas
            self._last_values = values

        return head_weights

    def train_step(self, batch: dict[str, Any]) -> dict[str, Any]:
        """Critic WMTP í›ˆë ¨ ìŠ¤í… - ê°€ì¹˜í•¨ìˆ˜ ë¸íƒ€ ê¸°ë°˜ ë™ì  ê°€ì¤‘ì¹˜ WMTP ì†ì‹¤ ê³„ì‚°.

        Args:
            batch: í›ˆë ¨ ë°°ì¹˜ ë°ì´í„° (input_ids, labels, attention_mask ë“±)

        Returns:
            ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬ (loss, lr, critic íŠ¹í™” ë©”íŠ¸ë¦­ í¬í•¨)
        """
        if self.model is None:
            raise RuntimeError("Trainer not initialized. Call setup() first.")

        self.model.train()

        # ë°°ì¹˜ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        batch = {k: v.to(self.device) if torch.is_tensor(v) else v
                 for k, v in batch.items()}

        target_ids: torch.Tensor = batch["labels"]  # [B, S]

        # autocast ë””ë°”ì´ìŠ¤ íƒ€ì… ê²°ì •
        if torch.cuda.is_available():
            autocast_device = "cuda"
        elif torch.backends.mps.is_available() and str(self.device).startswith("mps"):
            autocast_device = "cpu"  # MPSëŠ” ì•„ì§ autocast ë¯¸ì§€ì›
        else:
            autocast_device = "cpu"

        with torch.autocast(
            device_type=autocast_device,
            dtype=self._amp_dtype,
        ):
            # ëª¨ë¸ forward pass (hidden_states í¬í•¨ ë°˜í™˜ í•„ìš”)
            outputs: dict[str, Any] | torch.Tensor = self.model(**batch)

            if isinstance(outputs, dict) and "logits" in outputs:
                logits = outputs["logits"]  # [B, S, H, V] ì˜ˆìƒ
            else:
                logits = outputs  # tensorë¼ê³  ê°€ì •

            # Shape ê²€ì¦
            if logits.ndim != 4:
                raise ValueError(
                    f"Expected logits shape [B,S,H,V], got {tuple(logits.shape)}"
                )

            # gradient í™œì„±í™”
            if not logits.requires_grad:
                logits = logits.detach().requires_grad_(True)

            # hidden_states ì¶”ì¶œ (CriticScorerì— í•„ìš”)
            hidden_states = None
            try:
                if isinstance(outputs, dict) and "hidden_states" in outputs:
                    hs = outputs["hidden_states"]
                    hidden_states = hs[-1] if isinstance(hs, (list, tuple)) else hs
                elif hasattr(outputs, "hidden_states"):
                    hs = outputs.hidden_states
                    hidden_states = hs[-1] if isinstance(hs, (list, tuple)) else hs
            except Exception:
                pass

            if hidden_states is None or hidden_states.ndim != 3:
                raise RuntimeError(
                    "CriticWmtpTrainer requires valid hidden_states [B,S,D] from model outputs. "
                    "Ensure your model is configured to return hidden states."
                )

            # ğŸ¯ Critic WMTP: ê°€ì¹˜í•¨ìˆ˜ ë¸íƒ€ ê¸°ë°˜ ë™ì  ê°€ì¤‘ì¹˜ ê³„ì‚°
            head_weights = self.compute_head_weights(
                logits, target_ids, hidden_states=hidden_states
            )

            # WMTP ì†ì‹¤ ê³„ì‚° (BaseWmtpTrainerì˜ ê³µí†µ í•¨ìˆ˜ ì‚¬ìš©)
            weighted_loss, valid_mask, ce_per_head = compute_weighted_mtp_loss(
                logits=logits,  # [B, S, H, V]
                target_ids=target_ids,  # [B, S]
                head_weights=head_weights,  # [B, S, H] - ë™ì  ê°€ì¤‘ì¹˜
                horizon=self.horizon,
                ignore_index=-100,
            )

            # Lambda scaling
            lambda_w = float(self.loss_cfg.get("lambda", 0.3))
            loss = lambda_w * weighted_loss  # ìµœì¢… ìŠ¤ì¹¼ë¼ ì†ì‹¤

        # ì—­ì „íŒŒ ë° ìµœì í™”
        loss.backward()

        # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
        grad_clip = float(getattr(self.optimizer, "grad_clip", 1.0))
        if math.isfinite(grad_clip) and grad_clip > 0:
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), grad_clip)

        self.optimizer.step()
        self.optimizer.zero_grad()

        self.global_step += 1

        # MLflow ë¡œê¹… (ì„ íƒì )
        if self.mlflow is not None:
            try:
                # í—¤ë“œë³„ CE í‰ê·  (ì§„ë‹¨ìš©)
                with torch.no_grad():
                    B, S, H, V = logits.shape
                    ce_head_means = []
                    for k in range(H):
                        shift = k + 1
                        valid_len = S - shift
                        if valid_len <= 0:
                            ce_head_means.append(torch.tensor(0.0, device=logits.device))
                            continue
                        logits_k = logits[:, :valid_len, k, :]
                        labels_k = target_ids[:, shift : shift + valid_len]
                        ce_k = F.cross_entropy(
                            logits_k.transpose(1, 2),
                            labels_k,
                            ignore_index=-100,
                            reduction="none",
                        )
                        ce_head_means.append(ce_k.mean())
                    ce_head_means = torch.stack(ce_head_means)

                    # ê¸°ë³¸ ë©”íŠ¸ë¦­
                    metrics = {
                        f"train/ce_head_{i}": float(x)
                        for i, x in enumerate(ce_head_means)
                    }
                    metrics.update({
                        "train/loss": float(loss.detach().item()),
                        "train/ce_mean": float(
                            (ce_per_head[valid_mask.unsqueeze(-1).expand(-1, -1, H)]).mean().item()
                        ) if valid_mask.any() else 0.0,
                    })

                    # ê°€ì¤‘ì¹˜ í†µê³„ (ë™ì  ê°€ì¤‘ì¹˜ ë¶„ì„ìš©)
                    w_eff = head_weights[valid_mask.unsqueeze(-1).expand(-1, -1, H)]
                    if w_eff.numel() > 0:
                        weight_stats = {
                            "train/weight_mean": float(w_eff.mean().item()),
                            "train/weight_min": float(w_eff.min().item()),
                            "train/weight_max": float(w_eff.max().item()),
                            "train/weight_std": float(w_eff.std().item()),
                        }

                        # ê°€ì¤‘ì¹˜ ë¶„í¬ ë°±ë¶„ìœ„ìˆ˜ (ê³„íšì„œ ìš”êµ¬ì‚¬í•­)
                        try:
                            weight_stats.update({
                                "train/weight_p25": float(torch.quantile(w_eff, 0.25).item()),
                                "train/weight_p75": float(torch.quantile(w_eff, 0.75).item()),
                                "train/weight_p95": float(torch.quantile(w_eff, 0.95).item()),
                            })
                        except Exception:
                            # í´ë°± (ì´ì „ PyTorch ë²„ì „ìš©)
                            sorted_w = torch.sort(w_eff)[0]
                            n = sorted_w.numel()
                            weight_stats.update({
                                "train/weight_p25": float(sorted_w[int(n * 0.25)].item()),
                                "train/weight_p75": float(sorted_w[int(n * 0.75)].item()),
                                "train/weight_p95": float(sorted_w[int(n * 0.95)].item()),
                            })

                        # ì‹¤íŒ¨ ê°ì§€ (NaN/ê·¹ê°’ ì²´í¬)
                        weight_stats.update({
                            "train/nan_weights": int((~torch.isfinite(head_weights)).sum().item()),
                            "train/extreme_weights": int((head_weights > 5.0).sum().item()),
                        })

                        metrics.update(weight_stats)

                    # Critic íŠ¹í™” ë©”íŠ¸ë¦­ (ì§ì ‘ ê³„ì‚°ëœ ê°’ ê¸°ë°˜)
                    if hasattr(self, "_last_deltas") and self._last_deltas is not None:
                        metrics["train/critic_delta_mean"] = float(self._last_deltas.mean().item())
                        metrics["train/critic_delta_std"] = float(self._last_deltas.std().item())
                        metrics["train/critic_algorithm"] = 1  # Critic í”Œë˜ê·¸

                    if hasattr(self, "_last_values") and self._last_values is not None:
                        metrics["train/value_mean"] = float(self._last_values.mean().item())
                        metrics["train/value_std"] = float(self._last_values.std().item())

                    # ìœ íš¨ í† í° ë¹„ìœ¨
                    total_tokens = float(valid_mask.numel())
                    valid_tokens = float(valid_mask.sum().item())
                    metrics["train/valid_token_ratio"] = (
                        valid_tokens / total_tokens if total_tokens > 0 else 0.0
                    )

                self.mlflow.log_metrics(metrics, step=self.global_step)
            except Exception:
                # ë¡œê¹… ì˜¤ë¥˜ë¡œ í›ˆë ¨ ì¤‘ë‹¨ ë°©ì§€
                pass

        # ì‹¤íŒ¨ ê°ì§€ (NaN/Inf ì²´í¬)
        if (
            not torch.isfinite(loss)
            or not torch.isfinite(ce_per_head).all()
            or not torch.isfinite(head_weights).all()
        ):
            if self.mlflow is not None:
                try:
                    self.mlflow.log_metrics(
                        {"train/failure": 1.0}, step=self.global_step
                    )
                except Exception:
                    pass
            raise RuntimeError(
                "Detected NaN/Inf in loss or inputs; aborting training step."
            )

        return {
            "loss": float(loss.detach().item()),
            "lr": float(getattr(self.optimizer, "_last_lr", 0.0)),
        }