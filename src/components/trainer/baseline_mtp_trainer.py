"""
Baseline MTP Trainer - ê°€ì¥ ë‹¨ìˆœí•œ WMTP ì•Œê³ ë¦¬ì¦˜

ê· ë“± ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•˜ëŠ” baseline êµ¬í˜„ìœ¼ë¡œ WMTPì˜ ê¸°ë³¸ ë™ì‘ì„ ì œê³µí•©ë‹ˆë‹¤.
ëª¨ë“  í† í°ê³¼ ëª¨ë“  í—¤ë“œì— ë™ì¼í•œ ê°€ì¤‘ì¹˜ 1.0ì„ ì ìš©í•©ë‹ˆë‹¤.

íŠ¹ì§•:
- Scorer ì—†ìŒ: ì™¸ë¶€ í† í° ì¤‘ìš”ë„ ê³„ì‚° ë¶ˆí•„ìš”
- ê· ë“± ê°€ì¤‘ì¹˜: w_{t+k} = 1.0 (ëª¨ë“  kì— ëŒ€í•´)
- ìµœê³  ì„±ëŠ¥: ë³µì¡í•œ ê°€ì¤‘ì¹˜ ê³„ì‚° ì—†ì´ ìˆœìˆ˜ MTP ì„±ëŠ¥ ì¸¡ì • ê°€ëŠ¥
- ê¸°ì¤€ì„  ì—­í• : ë‹¤ë¥¸ WMTP ì•Œê³ ë¦¬ì¦˜ì˜ ì„±ëŠ¥ ë¹„êµ ê¸°ì¤€

ìˆ˜í•™ì  ê³µì‹:
    L_WMTP = Î£(k=0 to H-1) 1.0 Ã— CE_k
    = CE_0 + CE_1 + CE_2 + CE_3 (H=4ì¸ ê²½ìš°)
"""

from __future__ import annotations

import math
from typing import Any

import torch
import torch.nn.functional as F
from rich.console import Console

from src.components.registry import trainer_registry
from src.components.trainer.base_wmtp_trainer import (
    BaseWmtpTrainer,
    compute_weighted_mtp_loss,
)

console = Console()


@trainer_registry.register("baseline-mtp", category="trainer", version="2.0.0")
class BaselineMtpTrainer(BaseWmtpTrainer):
    """Baseline MTP íŠ¸ë ˆì´ë„ˆ - ê· ë“± ê°€ì¤‘ì¹˜ WMTP ì•Œê³ ë¦¬ì¦˜.

    ì—°êµ¬ ì² í•™ "Not All Tokens Are What You Need"ì˜ ê¸°ì¤€ì„  êµ¬í˜„:
        ëª¨ë“  í† í°ì— ë™ì¼í•œ ì¤‘ìš”ë„ë¥¼ ë¶€ì—¬í•˜ì—¬ í‘œì¤€ MTPì™€ ìœ ì‚¬í•˜ê²Œ ë™ì‘í•©ë‹ˆë‹¤.
        ë³µì¡í•œ ê°€ì¤‘ì¹˜ ê³„ì‚° ì—†ì´ WMTP í”„ë ˆì„ì›Œí¬ì˜ ê¸°ë³¸ ì„±ëŠ¥ì„ ì¸¡ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    ğŸ”¬ í•µì‹¬ ë™ì‘:
        1. ëª¨ë“  í—¤ë“œì— ê· ë“± ê°€ì¤‘ì¹˜ ì ìš©: w_{t+k} = 1.0
        2. MTP ì†ì‹¤ ê³„ì‚°: L = Î£ CE_k (ì¼ë°˜ì ì¸ MTPì™€ ë™ì¼)
        3. ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ ë° MLflow ë¡œê¹…

    ì¥ì :
        - ê°€ì¥ ë¹ ë¥¸ í•™ìŠµ ì†ë„ (ê°€ì¤‘ì¹˜ ê³„ì‚° ì˜¤ë²„í—¤ë“œ ì—†ìŒ)
        - ì•ˆì •ì  ì„±ëŠ¥ (ë³µì¡í•œ scorer ë¡œì§ ì—†ìŒ)
        - ë‹¤ë¥¸ ì•Œê³ ë¦¬ì¦˜ì˜ ì„±ëŠ¥ ë¹„êµ ê¸°ì¤€ì„ 

    ì‚¬ìš© ì‚¬ë¡€:
        - WMTP ì‹œìŠ¤í…œì˜ ê¸°ë³¸ ì„±ëŠ¥ ì¸¡ì •
        - ë³µì¡í•œ ì•Œê³ ë¦¬ì¦˜ì˜ íš¨ê³¼ ê²€ì¦ì„ ìœ„í•œ baseline
        - ë¹ ë¥¸ ì‹¤í—˜ ë° ë””ë²„ê¹…
    """

    def compute_head_weights(
        self,
        logits: torch.Tensor,
        target_labels: torch.Tensor,
        **kwargs,
    ) -> torch.Tensor:
        """ê· ë“± í—¤ë“œ ê°€ì¤‘ì¹˜ ê³„ì‚° - ëª¨ë“  í—¤ë“œì— 1.0 ê°€ì¤‘ì¹˜.

        ê°€ì¥ ë‹¨ìˆœí•œ ê°€ì¤‘ì¹˜ ê³„ì‚°ìœ¼ë¡œ ëª¨ë“  MTP í—¤ë“œì— ë™ì¼í•œ ì¤‘ìš”ë„ë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤.

        Args:
            logits: MTP ëª¨ë¸ ì¶œë ¥ [B, S, H, V]
            target_labels: 3D íƒ€ê²Ÿ ë¼ë²¨ [B, S, H] - MTPDataCollator ìƒì„±
            **kwargs: ì‚¬ìš©ë˜ì§€ ì•ŠìŒ (ê¸°ì¤€ì„  ì•Œê³ ë¦¬ì¦˜)

        Returns:
            head_weights: ê· ë“± ê°€ì¤‘ì¹˜ [B, S, H] - ëª¨ë“  ê°’ì´ 1.0
        """
        B, S, H, V = logits.shape
        return torch.ones((B, S, H), device=logits.device, dtype=logits.dtype)

    def train_step(self, batch: dict[str, Any]) -> dict[str, Any]:
        """MTP Baseline í›ˆë ¨ ìŠ¤í… - ê· ë“± ê°€ì¤‘ì¹˜ë¡œ ê°„ë‹¨í•œ WMTP ì†ì‹¤ ê³„ì‚°.

        Args:
            batch: í›ˆë ¨ ë°°ì¹˜ ë°ì´í„° (input_ids, labels, attention_mask ë“±)

        Returns:
            ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬ (loss, lr ë“±)
        """
        if self.model is None:
            raise RuntimeError("Trainer not initialized. Call setup() first.")

        self.model.train()

        # ë°°ì¹˜ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        batch = {
            k: v.to(self.device) if torch.is_tensor(v) else v for k, v in batch.items()
        }

        target_labels: torch.Tensor = batch[
            "labels"
        ]  # [B, S, H] - MTPDataCollator ìƒì„±

        # autocast ë””ë°”ì´ìŠ¤ íƒ€ì… ê²°ì •
        if torch.cuda.is_available():
            autocast_device = "cuda"
        elif torch.backends.mps.is_available() and str(self.device).startswith("mps"):
            autocast_device = "cpu"  # MPSëŠ” ì•„ì§ autocast ë¯¸ì§€ì›
        else:
            autocast_device = "cpu"

        with torch.autocast(
            device_type=autocast_device,
            dtype=self._amp_dtype,
        ):
            # ëª¨ë¸ forward pass
            outputs: dict[str, Any] | torch.Tensor = self.model(**batch)

            if isinstance(outputs, dict) and "logits" in outputs:
                logits = outputs["logits"]  # [B, S, H, V] ì˜ˆìƒ
            else:
                logits = outputs  # tensorë¼ê³  ê°€ì •

            # Shape ê²€ì¦
            if logits.ndim != 4:
                raise ValueError(
                    f"Expected logits shape [B,S,H,V], got {tuple(logits.shape)}"
                )

            # gradient í™œì„±í™” (ì¼ë¶€ ëª¨ë¸ì´ detached tensor ë°˜í™˜í•˜ëŠ” ê²½ìš°)
            if not logits.requires_grad:
                logits = logits.detach().requires_grad_(True)

            # ğŸ¯ MTP Baseline: ê· ë“± ê°€ì¤‘ì¹˜ë¡œ ë‹¨ìˆœí•œ ì†ì‹¤ ê³„ì‚°
            head_weights = self.compute_head_weights(logits, target_labels)

            # WMTP ì†ì‹¤ ê³„ì‚° (ê°„ì†Œí™”ëœ 3D ë¼ë²¨ ê¸°ë°˜)
            weighted_loss, valid_mask, ce_per_head = compute_weighted_mtp_loss(
                logits=logits,  # [B, S, H, V]
                target_labels=target_labels,  # [B, S, H] - MTPDataCollator ìƒì„±
                head_weights=head_weights,  # [B, S, H] - ëª¨ë‘ 1.0
                ignore_index=-100,
                config=self.config,  # MPS ê²½ë¡œ íŒë‹¨ìš© ì„¤ì • ì „ë‹¬
            )

            # Lambda scaling (ì„¤ì •ì—ì„œ ê°€ì ¸ì˜¤ê¸°)
            lambda_w = float(self.loss_cfg.get("lambda", 0.3))
            loss = lambda_w * weighted_loss  # ìµœì¢… ìŠ¤ì¹¼ë¼ ì†ì‹¤

        # ì—­ì „íŒŒ ë° ìµœì í™”
        loss.backward()

        # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
        grad_clip = float(getattr(self.optimizer, "grad_clip", 1.0))
        if math.isfinite(grad_clip) and grad_clip > 0:
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), grad_clip)

        self.optimizer.step()
        self.optimizer.zero_grad()

        self.global_step += 1

        # MLflow ë¡œê¹… (ì„ íƒì )
        if self.mlflow is not None:
            try:
                # í—¤ë“œë³„ CE í‰ê·  (ì§„ë‹¨ìš©)
                with torch.no_grad():
                    B, S, H, V = logits.shape
                    ce_head_means = []
                    for k in range(H):
                        shift = k + 1
                        valid_len = S - shift
                        if valid_len <= 0:
                            ce_head_means.append(
                                torch.tensor(0.0, device=logits.device)
                            )
                            continue
                        logits_k = logits[:, :valid_len, k, :]
                        labels_k = target_labels[:, shift : shift + valid_len]
                        ce_k = F.cross_entropy(
                            logits_k.transpose(1, 2),
                            labels_k,
                            ignore_index=-100,
                            reduction="none",
                        )
                        ce_head_means.append(ce_k.mean())
                    ce_head_means = torch.stack(ce_head_means)

                    # ê¸°ë³¸ ë©”íŠ¸ë¦­
                    metrics = {
                        f"train/ce_head_{i}": float(x)
                        for i, x in enumerate(ce_head_means)
                    }
                    metrics.update(
                        {
                            "train/loss": float(loss.detach().item()),
                            "train/ce_mean": float(
                                (
                                    ce_per_head[
                                        valid_mask.unsqueeze(-1).expand(-1, -1, H)
                                    ]
                                )
                                .mean()
                                .item()
                            )
                            if valid_mask.any()
                            else 0.0,
                        }
                    )

                    # Baseline íŠ¹í™” ë©”íŠ¸ë¦­
                    metrics.update(
                        {
                            "train/baseline_uniform_weight": 1.0,  # í•­ìƒ 1.0
                            "train/baseline_algorithm": 1,  # Baseline í”Œë˜ê·¸
                        }
                    )

                    # ìœ íš¨ í† í° ë¹„ìœ¨
                    total_tokens = float(valid_mask.numel())
                    valid_tokens = float(valid_mask.sum().item())
                    metrics["train/valid_token_ratio"] = (
                        valid_tokens / total_tokens if total_tokens > 0 else 0.0
                    )

                self.mlflow.log_metrics(metrics, step=self.global_step)
            except Exception:
                # ë¡œê¹… ì˜¤ë¥˜ë¡œ í›ˆë ¨ ì¤‘ë‹¨ ë°©ì§€
                pass

        # ì‹¤íŒ¨ ê°ì§€ (NaN/Inf ì²´í¬)
        if (
            not torch.isfinite(loss)
            or not torch.isfinite(ce_per_head).all()
            or not torch.isfinite(head_weights).all()
        ):
            if self.mlflow is not None:
                try:
                    self.mlflow.log_metrics(
                        {"train/failure": 1.0}, step=self.global_step
                    )
                except Exception:
                    pass
            raise RuntimeError(
                "Detected NaN/Inf in loss or inputs; aborting training step."
            )

        return {
            "loss": float(loss.detach().item()),
            "lr": float(getattr(self.optimizer, "_last_lr", 0.0)),
        }
