"""WMTP í•µì‹¬ êµ¬í˜„ì²´ - MTP Weighted Cross-Entropy Trainer.

ì—°êµ¬ ì² í•™ì˜ ì‹¤í˜„: "Not All Tokens Are What You Need"
=================================================

ì´ íŠ¸ë ˆì´ë„ˆëŠ” WMTP ì—°êµ¬ì˜ í•µì‹¬ ì•„ì´ë””ì–´ë¥¼ ì‹¤ì œë¡œ êµ¬í˜„í•©ë‹ˆë‹¤:
ê¸°ì¡´ MTPì˜ ê· ë“±í•œ í† í° ê°€ì¤‘ì¹˜ ëŒ€ì‹ , í† í°ë³„ ì¤‘ìš”ë„ë¥¼ ë™ì ìœ¼ë¡œ ê³„ì‚°í•˜ì—¬
ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•œ ìƒˆë¡œìš´ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

ğŸ”¬ WMTP ì†ì‹¤ ê³µì‹:
    L_WMTP = Î£(k=1 to H) w_{t+k} Ã— CE_k

    ì—¬ê¸°ì„œ:
    - w_{t+k}: kë²ˆì§¸ í—¤ë“œì˜ í† í°ë³„ ì¤‘ìš”ë„ ê°€ì¤‘ì¹˜
    - CE_k: kë²ˆì§¸ ì˜ˆì¸¡ í—¤ë“œì˜ Cross-Entropy ì†ì‹¤
    - H: ì˜ˆì¸¡ í—¤ë“œ ìˆ˜ (ì¼ë°˜ì ìœ¼ë¡œ 4ê°œ: t+1, t+2, t+3, t+4)

ì•Œê³ ë¦¬ì¦˜ë³„ ê°€ì¤‘ì¹˜ ê³„ì‚° ë°©ì‹:
    - mtp-baseline: w_{t+k} = 1.0 (ê· ë“± ê°€ì¤‘ì¹˜, Scorer=None)
    - critic-wmtp: w_{t+k} = f(Î´_t) where Î´_t = V_t - V_{t-1}
    - rho1-wmtp: w_{t+k} = |CE^ref_t - CE^base_t|

ê¸°ìˆ ì  íŠ¹ì§•:
    - Mixed Precision ì§€ì›: BF16/FP16/FP32 ìë™ ì„ íƒ
    - FSDP (Fully Sharded Data Parallel) ë¶„ì‚° í›ˆë ¨
    - ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ìœ¼ë¡œ ì•ˆì •ì„± ë³´ì¥
    - MLflow ìë™ ë¡œê¹…ìœ¼ë¡œ ì‹¤í—˜ ì¶”ì 
    - ë™ì  ë©”ëª¨ë¦¬ ìµœì í™” ë° ë°°ì¹˜ ì²˜ë¦¬
"""

from __future__ import annotations  # Python 3.10+ íƒ€ì… íŒíŠ¸ í˜¸í™˜ì„±

import math  # ìˆ˜í•™ ì—°ì‚° (ê°€ì¤‘ì¹˜ ì •ê·œí™” ë“±)
from typing import Any  # ë²”ìš© íƒ€ì… íŒíŠ¸

import numpy as np  # ìˆ˜ì¹˜ ì—°ì‚°
import torch  # PyTorch ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬
import torch.nn as nn  # ì‹ ê²½ë§ ëª¨ë“ˆ
import torch.nn.functional as F  # í•¨ìˆ˜í˜• API (cross_entropy ë“±)
from rich.console import Console  # ì»¬ëŸ¬í’€í•œ ì½˜ì†” ì¶œë ¥

from src.components.base import BaseComponent  # WMTP ì»´í¬ë„ŒíŠ¸ ë² ì´ìŠ¤ í´ë˜ìŠ¤
from src.components.registry import trainer_registry  # íŠ¸ë ˆì´ë„ˆ ë ˆì§€ìŠ¤íŠ¸ë¦¬
from src.utils import get_dist_manager  # ë¶„ì‚° í›ˆë ¨ ë§¤ë‹ˆì €

console = Console()  # ì „ì—­ ì½˜ì†” ê°ì²´


def _compute_weighted_mtp_loss(
    logits: torch.Tensor,  # [B, S, H, V]
    target_ids: torch.Tensor,  # [B, S]
    head_weights: torch.Tensor,  # [B, S, H] - ìƒˆë¡œìš´ í—¤ë“œë³„ ê°€ì¤‘ì¹˜!
    horizon: int,
    ignore_index: int = -100,
) -> tuple[torch.Tensor, torch.Tensor]:
    """
    ì—°êµ¬ì œì•ˆì„œ ì •í™• êµ¬í˜„: L_WMTP = Î£(k=0 to H-1) w_{t+k} Ã— CE_k

    ê° í—¤ë“œë³„ CEì— í•´ë‹¹ í—¤ë“œì˜ ê°€ì¤‘ì¹˜ë¥¼ ì§ì ‘ ì ìš©í•˜ì—¬
    í† í° ì¤‘ìš”ë„ê°€ ì†ì‹¤ì— ì •í™•íˆ ë°˜ì˜ë˜ë„ë¡ í•¨.

    Args:
        logits: [batch, seq_len, horizon, vocab] - MTP ëª¨ë¸ ì¶œë ¥
        target_ids: [batch, seq_len] - íƒ€ê²Ÿ ë¼ë²¨
        head_weights: [batch, seq_len, horizon] - í—¤ë“œë³„ ê°€ì¤‘ì¹˜ ë§¤íŠ¸ë¦­ìŠ¤
        horizon: ì˜ˆì¸¡ í—¤ë“œ ìˆ˜ (4)
        ignore_index: ë¬´ì‹œí•  ë¼ë²¨ ê°’

    Returns:
        weighted_loss: ê°€ì¤‘ í‰ê·  ì†ì‹¤ (scalar)
        valid_mask: ìœ íš¨í•œ ìœ„ì¹˜ ë§ˆìŠ¤í¬ [batch, seq_len]
    """
    # Input validation
    if not isinstance(logits, torch.Tensor) or not isinstance(target_ids, torch.Tensor):
        raise TypeError("logits and target_ids must be torch.Tensor")

    if not isinstance(head_weights, torch.Tensor):
        raise TypeError("head_weights must be torch.Tensor")

    if logits.ndim != 4:
        raise ValueError(f"logits must be 4D [B,S,H,V], got shape {logits.shape}")

    if target_ids.ndim != 2:
        raise ValueError(f"target_ids must be 2D [B,S], got shape {target_ids.shape}")

    if head_weights.ndim != 3:
        raise ValueError(
            f"head_weights must be 3D [B,S,H], got shape {head_weights.shape}"
        )

    bsz, seqlen, H, vocab = logits.shape
    if target_ids.shape != (bsz, seqlen):
        raise ValueError(
            f"Shape mismatch: logits {logits.shape} vs target_ids {target_ids.shape}"
        )

    if head_weights.shape != (bsz, seqlen, H):
        raise ValueError(
            f"Shape mismatch: head_weights {head_weights.shape} vs expected {(bsz, seqlen, H)}"
        )

    if horizon != H:
        raise ValueError(
            f"Mismatch between logits heads ({H}) and configured horizon ({horizon})"
        )

    device = logits.device
    dtype = logits.dtype

    # í—¤ë“œë³„ ê°€ì¤‘ CE ëˆ„ì ìš©
    weighted_ce_sum = torch.zeros((bsz, seqlen), device=device, dtype=dtype)
    total_weights = torch.zeros((bsz, seqlen), device=device, dtype=dtype)

    # ê° í—¤ë“œë³„ë¡œ CE ê³„ì‚° ë° ê°€ì¤‘ì¹˜ ì ìš©
    for k in range(H):
        shift = k + 1  # kë²ˆì§¸ í—¤ë“œëŠ” t+(k+1) ìœ„ì¹˜ ì˜ˆì¸¡
        valid_len = seqlen - shift
        if valid_len <= 0:
            continue

        # ìœ íš¨ ì˜ì—­ ìŠ¬ë¼ì´ì‹±
        logits_k = logits[:, :valid_len, k, :]  # [B, valid_len, V]
        labels_k = target_ids[:, shift : shift + valid_len]  # [B, valid_len]
        weights_k = head_weights[:, :valid_len, k]  # [B, valid_len]

        # í—¤ë“œë³„ CE ê³„ì‚°
        ce_k = F.cross_entropy(
            logits_k.transpose(1, 2),  # [B, V, valid_len]
            labels_k,
            ignore_index=ignore_index,
            reduction="none",
        )  # [B, valid_len]

        # ìœ íš¨ ìœ„ì¹˜ ë§ˆìŠ¤í‚¹ (ignore_index ì œì™¸)
        valid_k_mask = (labels_k != ignore_index).to(dtype)  # [B, valid_len]

        # ê°€ì¤‘ CE: w_{t+k} Ã— CE_k (ì—°êµ¬ì œì•ˆì„œ ê³µì‹!)
        weighted_ce_k = weights_k * ce_k * valid_k_mask  # [B, valid_len]
        effective_weights_k = weights_k * valid_k_mask  # [B, valid_len]

        # ì „ì²´ ì‹œí€€ìŠ¤ì— ëˆ„ì  ([B, S] í˜•íƒœë¡œ ë§ì¶¤)
        weighted_ce_sum[:, :valid_len] += weighted_ce_k
        total_weights[:, :valid_len] += effective_weights_k

    # ê°€ì¤‘ í‰ê·  ê³„ì‚° (ë¶„ëª¨ 0 ë°©ì§€)
    total_weights_clamped = torch.clamp(total_weights, min=1e-8)
    weighted_loss_per_token = weighted_ce_sum / total_weights_clamped

    # ìœ íš¨ ë§ˆìŠ¤í¬: ìµœì†Œ í•˜ë‚˜ì˜ í—¤ë“œì—ì„œ ìœ íš¨í•œ ìœ„ì¹˜
    valid_mask = total_weights > 1e-8

    # ìµœì¢… ìŠ¤ì¹¼ë¼ ì†ì‹¤: ìœ íš¨ í† í°ë“¤ì˜ í‰ê· 
    if valid_mask.any():
        final_loss = (
            weighted_loss_per_token * valid_mask.to(dtype)
        ).sum() / valid_mask.sum().to(dtype)
    else:
        final_loss = torch.tensor(0.0, device=device, dtype=dtype)

    return final_loss, valid_mask


@trainer_registry.register(
    "mtp-weighted-ce-trainer", category="trainer", version="1.0.0"
)
class MTPWeightedCETrainer(BaseComponent):
    """WMTP í†µí•© íŠ¸ë ˆì´ë„ˆ - ëª¨ë“  ì•Œê³ ë¦¬ì¦˜ì˜ í•µì‹¬ ì‹¤í–‰ê¸°.

    ì—°êµ¬ ì² í•™ "Not All Tokens Are What You Need"ì˜ ì‹¤ì œ êµ¬í˜„:
        ì´ í´ë˜ìŠ¤ëŠ” ì„¸ ê°€ì§€ WMTP ì•Œê³ ë¦¬ì¦˜(mtp-baseline, critic-wmtp, rho1-wmtp)ì„
        ëª¨ë‘ ì§€ì›í•˜ëŠ” í†µí•© íŠ¸ë ˆì´ë„ˆì…ë‹ˆë‹¤. ì•Œê³ ë¦¬ì¦˜ ê°„ ì°¨ì´ëŠ” ì˜¤ì§ Scorerì— ì˜í•œ
        í† í° ê°€ì¤‘ì¹˜ ê³„ì‚° ë°©ì‹ë¿ì´ë©°, ë‚˜ë¨¸ì§€ í›ˆë ¨ ë¡œì§ì€ ì™„ì „íˆ ê³µìœ ë©ë‹ˆë‹¤.

    ğŸ”¬ í•µì‹¬ ë™ì‘ ì›ë¦¬:
        1. Scorerì—ì„œ í† í°ë³„ ì¤‘ìš”ë„ ê°€ì¤‘ì¹˜ w_{t+k} ê³„ì‚°
        2. ê° MTP í—¤ë“œë³„ë¡œ Cross-Entropy ì†ì‹¤ CE_k ê³„ì‚°
        3. WMTP ê³µì‹ ì ìš©: L_WMTP = Î£ w_{t+k} Ã— CE_k
        4. í˜¼í•© ì •ë°€ë„ì™€ ë¶„ì‚° í›ˆë ¨ìœ¼ë¡œ ì•ˆì •ì  ìµœì í™”

    ì•Œê³ ë¦¬ì¦˜ë³„ ë™ì‘ ì°¨ì´:
        - mtp-baseline: scorer=None â†’ ëª¨ë“  w_{t+k} = 1.0
        - critic-wmtp: CriticScorer â†’ Î´_t = V_t - V_{t-1} ê¸°ë°˜ ê°€ì¤‘ì¹˜
        - rho1-wmtp: Rho1Scorer â†’ |CE^ref_t - CE^base_t| ê¸°ë°˜ ê°€ì¤‘ì¹˜

    í•„ìˆ˜ ì„¤ì • í‚¤:
        - n_heads: MTP í—¤ë“œ ìˆ˜ (ì¼ë°˜ì ìœ¼ë¡œ 4)
        - horizon: ì˜ˆì¸¡ ë²”ìœ„ (n_headsì™€ ë™ì¼)
        - loss_config: ì†ì‹¤ í•¨ìˆ˜ ì„¤ì • (ì •ê·œí™”, ì˜¨ë„ ë“±)
        - mixed_precision: í˜¼í•© ì •ë°€ë„ ("bf16"/"fp16"/"fp32")
        - fsdp_config: FSDP ë¶„ì‚° í›ˆë ¨ ì„¤ì • (dict ë˜ëŠ” None)
        - scorer: í† í° ê°€ì¤‘ì¹˜ ê³„ì‚°ê¸° (Noneì´ë©´ baseline)

    ì„ íƒì  ì„¤ì •:
        - full_finetune: ì „ì²´ íŒŒì¸íŠœë‹ ì—¬ë¶€
        - lora_config: LoRA ì„¤ì • (ë©”ëª¨ë¦¬ íš¨ìœ¨ì  íŒŒì¸íŠœë‹)
    """

    def __init__(self, config: dict[str, Any] | None = None):
        """WMTP íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”.

        Args:
            config: íŠ¸ë ˆì´ë„ˆ ì„¤ì • ë”•ì…”ë„ˆë¦¬ (horizon, ì†ì‹¤ ì„¤ì •, Scorer ë“±)
        """
        super().__init__(config)

        # ì£¼ìš” ì»´í¬ë„ŒíŠ¸ë“¤ (setupì—ì„œ ì´ˆê¸°í™”ë¨)
        self.model: nn.Module | None = None  # Facebook MTP ëª¨ë¸
        self.optimizer = None  # AdamW ë“± ìµœì í™”ê¸°

        # í›ˆë ¨ ìƒíƒœ ì¶”ì 
        self.global_step: int = 0  # ì „ì—­ í›ˆë ¨ ìŠ¤í… ì¹´ìš´í„°

        # MTP ì„¤ì •
        self.horizon: int = int(self.config.get("horizon", 4))  # ì˜ˆì¸¡ í—¤ë“œ ìˆ˜

        # Scorer ì¶œë ¥ ìºì‹± (ì„±ëŠ¥ ìµœì í™”ìš©)
        self._last_score_out: dict[str, Any] | None = None

    def setup(self, ctx: dict[str, Any]) -> None:
        """íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™” - ëª¨ë¸, ë¶„ì‚° í›ˆë ¨, Scorer ë“± ëª¨ë“  ì»´í¬ë„ŒíŠ¸ ì„¤ì •.

        ì´ ë©”ì„œë“œëŠ” íŒŒì´í”„ë¼ì¸ì—ì„œ ì œê³µë°›ì€ ì»´í¬ë„ŒíŠ¸ë“¤ì„ ì—°ê²°í•˜ê³ 
        WMTP í›ˆë ¨ì— í•„ìš”í•œ ëª¨ë“  ì„¤ì •ì„ ì™„ë£Œí•©ë‹ˆë‹¤.

        Args:
            ctx: ì»¨í…ìŠ¤íŠ¸ ë”•ì…”ë„ˆë¦¬ (model, optimizer, scorers, tokenizers ë“±)
        """
        super().setup(ctx)
        dm = get_dist_manager()  # ë¶„ì‚° í›ˆë ¨ ë§¤ë‹ˆì €

        # í•„ìˆ˜ ì»´í¬ë„ŒíŠ¸ ê²€ì¦ ë° ì„¤ì •
        model: nn.Module | None = ctx.get("model")  # Facebook MTP ëª¨ë¸
        optimizer = ctx.get("optimizer")  # AdamW ë“± ìµœì í™”ê¸°
        if model is None:
            raise ValueError("íŠ¸ë ˆì´ë„ˆì— 'model'ì´ í•„ìš”í•©ë‹ˆë‹¤ (ctxì—ì„œ ëˆ„ë½)")
        if optimizer is None:
            raise ValueError("íŠ¸ë ˆì´ë„ˆì— 'optimizer'ê°€ í•„ìš”í•©ë‹ˆë‹¤ (ctxì—ì„œ ëˆ„ë½)")

        # ë¶„ì‚° í›ˆë ¨: FSDP ë˜í•‘ (ì„ íƒì )
        fsdp_cfg = self.config.get("fsdp_config")
        if fsdp_cfg:
            # Fully Sharded Data Parallelë¡œ ëª¨ë¸ ë˜í•‘
            model = dm.setup_fsdp(model, fsdp_cfg)

        self.model = model
        self.optimizer = optimizer

        # í˜¼í•© ì •ë°€ë„ ì„¤ì •: ë©”ëª¨ë¦¬ì™€ ì†ë„ ìµœì í™”
        mp = str(self.config.get("mixed_precision", "bf16")).lower()
        if mp not in {"bf16", "fp16", "fp32"}:
            mp = "bf16"  # ê¸°ë³¸ê°’: BFloat16 (ê¶Œì¥)

        self._amp_dtype = (
            torch.bfloat16  # BF16: ì•ˆì •ì„±ê³¼ ì„±ëŠ¥ì˜ ê· í˜•
            if mp == "bf16"
            else (torch.float16 if mp == "fp16" else torch.float32)  # FP16 ë˜ëŠ” FP32
        )

        # ğŸ¯ í•µì‹¬: ì•Œê³ ë¦¬ì¦˜ë³„ í† í° ê°€ì¤‘ì¹˜ ê³„ì‚° Scorer ì—°ê²°
        self.scorer = self.config.get(
            "scorer"
        )  # None(baseline), CriticScorer, Rho1Scorer

        # WMTP ì†ì‹¤ í•¨ìˆ˜ ì„¤ì •
        self.loss_cfg = self.config.get("loss_config", {})

        # MLflow ì‹¤í—˜ ì¶”ì  (ì„ íƒì )
        self.mlflow = ctx.get("mlflow_manager")

        # ì•Œê³ ë¦¬ì¦˜ë³„ ë³´ì¡° ëª¨ë¸ë“¤ (ì„ íƒì  - í•´ë‹¹ ì•Œê³ ë¦¬ì¦˜ì—ì„œë§Œ ì‚¬ìš©)
        self.ref_model: nn.Module | None = ctx.get("ref_model")  # Rho-1ìš© ì°¸ì¡° ëª¨ë¸
        self.rm_model: nn.Module | None = ctx.get("rm_model")  # Criticìš© ë³´ìƒ ëª¨ë¸
        self.base_tokenizer = ctx.get("base_tokenizer")  # ê¸°ë³¸ í† í¬ë‚˜ì´ì €
        self.ref_tokenizer = ctx.get("ref_tokenizer")  # ì°¸ì¡° ëª¨ë¸ìš© í† í¬ë‚˜ì´ì €

        # ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ ì„¤ì •
        self.dist_manager = dm  # ë¶„ì‚° í›ˆë ¨ ë§¤ë‹ˆì € (ì²´í¬í¬ì¸íŠ¸ ì €ì¥/ë¡œë“œìš©)

        # Recipeì—ì„œ ì²´í¬í¬ì¸íŠ¸ ì„¤ì • íŒŒì‹±
        recipe = ctx.get("recipe")  # Recipe ê°ì²´ ê°€ì ¸ì˜¤ê¸° (ì—†ìœ¼ë©´ None)
        if (
            recipe
            and hasattr(recipe, "train")
            and hasattr(recipe.train, "checkpointing")
        ):
            checkpointing = recipe.train.checkpointing
            self.save_interval = getattr(checkpointing, "save_interval", 100)
            self.keep_last = getattr(checkpointing, "keep_last", 3)
            self.save_final = getattr(checkpointing, "save_final", True)
        else:
            # ê¸°ë³¸ê°’ ì„¤ì •
            self.save_interval = 100
            self.keep_last = 3
            self.save_final = True

        # ì•Œê³ ë¦¬ì¦˜ ì •ë³´ ì €ì¥ (recipeì—ì„œ ì¶”ì¶œ)
        self.algorithm = (
            getattr(recipe.train, "algo", "wmtp")
            if recipe and hasattr(recipe, "train")
            else "wmtp"
        )

        # ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ ì„¤ì •
        from pathlib import Path

        run_name = (
            getattr(recipe, "run", {}).get("name", "default") if recipe else "default"
        )
        self.checkpoint_dir = Path("./checkpoints") / run_name
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)

        # ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸ ëª©ë¡ ê´€ë¦¬
        self.saved_checkpoints = []

        # ì¬ê°œ ì²˜ë¦¬ ë¡œì§
        self.start_step = 0
        self.resume_metrics = {}

        resume_checkpoint = ctx.get("resume_checkpoint")
        if resume_checkpoint:
            checkpoint_data = self.dist_manager.load_checkpoint(
                model=self.model,
                optimizer=self.optimizer,
                checkpoint_path=str(resume_checkpoint),
            )

            self.start_step = checkpoint_data.get("step", 0)
            self.resume_metrics = checkpoint_data.get("metrics", {})

            console.print(
                f"[green]Model and optimizer states restored from step {self.start_step}[/green]"
            )

    def train_step(self, batch: dict[str, Any]) -> dict[str, Any]:
        if self.model is None:
            raise RuntimeError("Trainer not initialized. Call setup() first.")

        self.model.train()

        # Note: input_ids may be unused by this method
        target_ids: torch.Tensor = batch["labels"]  # [B, S]

        with torch.autocast(
            device_type="cuda" if torch.cuda.is_available() else "cpu",
            dtype=self._amp_dtype,
        ):
            # Model is expected to output logits for each horizon head
            outputs: dict[str, Any] | torch.Tensor = self.model(**batch)

            if isinstance(outputs, dict) and "logits" in outputs:
                logits = outputs["logits"]  # [B, S, H, V] expected
            else:
                logits = outputs  # assume tensor

            # Shape validation
            if logits.ndim != 4:
                raise ValueError(
                    f"Expected logits shape [B,S,H,V], got {tuple(logits.shape)}"
                )

            # Ensure logits require grad for tests/models that return detached tensors
            if not logits.requires_grad:
                logits = logits.detach().requires_grad_(True)

            # ìƒˆë¡œìš´ í—¤ë“œë³„ ê°€ì¤‘ì¹˜ ê¸°ë°˜ ì†ì‹¤ ê³„ì‚° ì‹œì‘

            # Build scorer context to get token weights
            if self.scorer is not None:
                scorer_ctx = {
                    "base_logits": logits[:, :, 0, :],  # provide one head if needed
                    "target_ids": target_ids,
                    "seq_lengths": [int(target_ids.shape[1])]
                    * int(target_ids.shape[0]),
                }

                # Provide hidden_states to critic scorer if available
                try:
                    hidden_states = None
                    if isinstance(outputs, dict) and "hidden_states" in outputs:
                        hs = outputs["hidden_states"]
                        hidden_states = hs[-1] if isinstance(hs, (list | tuple)) else hs
                    elif hasattr(outputs, "hidden_states"):
                        hs = outputs.hidden_states
                        hidden_states = hs[-1] if isinstance(hs, (list | tuple)) else hs
                    if hidden_states is not None and hidden_states.ndim == 3:
                        scorer_ctx["hidden_states"] = hidden_states
                except Exception:
                    # Hidden states are optional; ignore failures
                    pass
                # Optionally compute reference logits for Rho-1 scorer
                if self.ref_model is not None:
                    try:
                        with (
                            torch.no_grad(),
                            torch.autocast(
                                device_type="cuda"
                                if torch.cuda.is_available()
                                else "cpu",
                                dtype=self._amp_dtype,
                            ),
                        ):
                            ref_outputs = self.ref_model(
                                input_ids=batch.get("input_ids"),
                                attention_mask=batch.get("attention_mask"),
                            )
                            ref_logits = (
                                ref_outputs["logits"]
                                if isinstance(ref_outputs, dict)
                                and "logits" in ref_outputs
                                else ref_outputs
                            )
                        if ref_logits is not None and ref_logits.ndim == 3:
                            ref_vocab = ref_logits.shape[-1]
                            # ensure no negative labels included for max
                            valid_tids = target_ids[target_ids >= 0]
                            max_tid = (
                                int(valid_tids.max().item())
                                if valid_tids.numel() > 0
                                else 0
                            )
                            if max_tid < ref_vocab:
                                scorer_ctx["ref_logits"] = ref_logits
                    except Exception:
                        from rich.console import Console as _C

                        _C().print(
                            "[yellow]Reference forward failed; fallback to base-only scoring this step.[/yellow]"
                        )

                # ìƒˆë¡œìš´ í—¤ë“œë³„ ê°€ì¤‘ì¹˜ ê¸°ë°˜ ì ‘ê·¼ (ì—°êµ¬ì œì•ˆì„œ êµ¬í˜„)
                score_out = self.scorer.run(scorer_ctx)
                head_weights_out = score_out.get("weights")  # [B, S, H] í˜•íƒœ

                # Store score_out for extended metrics
                self._last_score_out = score_out

                # Convert head weights to tensor
                if isinstance(head_weights_out, torch.Tensor):
                    head_weights = head_weights_out.to(
                        device=logits.device, dtype=logits.dtype
                    )
                else:
                    head_weights_np = np.asarray(head_weights_out)
                    head_weights = torch.tensor(
                        head_weights_np, device=logits.device, dtype=logits.dtype
                    )

                # ìƒˆë¡œìš´ ê°€ì¤‘ MTP ì†ì‹¤ ê³„ì‚° (ì—°êµ¬ì œì•ˆì„œ ì •í™• êµ¬í˜„)
                weighted_loss, valid_mask = _compute_weighted_mtp_loss(
                    logits=logits,  # [B, S, H, V]
                    target_ids=target_ids,  # [B, S]
                    head_weights=head_weights,  # [B, S, H]
                    horizon=self.horizon,
                    ignore_index=-100,
                )

                # Lambda scaling
                lambda_w = float(self.loss_cfg.get("lambda", 0.3))
                loss = lambda_w * weighted_loss  # ìµœì¢… ìŠ¤ì¹¼ë¼ ì†ì‹¤

            else:
                # Scorerê°€ ì—†ëŠ” ê²½ìš°: uniform weights ì‚¬ìš©
                B, S, H, V = logits.shape
                uniform_weights = torch.ones(
                    (B, S, H), device=logits.device, dtype=logits.dtype
                )

                weighted_loss, valid_mask = _compute_weighted_mtp_loss(
                    logits=logits,
                    target_ids=target_ids,
                    head_weights=uniform_weights,
                    horizon=self.horizon,
                    ignore_index=-100,
                )

                lambda_w = float(self.loss_cfg.get("lambda", 0.3))
                loss = lambda_w * weighted_loss
                self._last_score_out = None

            # ìƒˆë¡œìš´ í—¤ë“œë³„ ê°€ì¤‘ì¹˜ ê¸°ë°˜ ì†ì‹¤ ê³„ì‚° ì™„ë£Œ
            # lossëŠ” ìœ„ì—ì„œ ì´ë¯¸ ê³„ì‚°ë¨

        # Backward and optimize
        loss.backward()

        # Grad clip (from optimizer component if available)
        grad_clip = float(getattr(self.optimizer, "grad_clip", 1.0))
        if math.isfinite(grad_clip) and grad_clip > 0:
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), grad_clip)

        self.optimizer.step()
        self.optimizer.zero_grad()

        self.global_step += 1

        # Optional MLflow logging
        if self.mlflow is not None:
            try:
                # Per-head CE means (diagnostics)
                with torch.no_grad():
                    bsz, seqlen, H, vocab = logits.shape
                    # Approximate head CE means using valid regions only
                    ce_head_means = []
                    for k in range(H):
                        shift = k + 1
                        valid_len = seqlen - shift
                        if valid_len <= 0:
                            ce_head_means.append(
                                torch.tensor(0.0, device=logits.device)
                            )
                            continue
                        logits_k = logits[:, :valid_len, k, :]
                        labels_k = target_ids[:, shift : shift + valid_len]
                        ce_k = F.cross_entropy(
                            logits_k.transpose(1, 2),
                            labels_k,
                            ignore_index=-100,
                            reduction="none",
                        )
                        ce_head_means.append(ce_k.mean())
                    ce_head_means = torch.stack(ce_head_means)
                    metrics = {
                        f"train/ce_head_{i}": float(x)
                        for i, x in enumerate(ce_head_means)
                    }
                    metrics.update(
                        {
                            "train/loss": float(loss.detach().item()),
                            "train/ce_mean": float(
                                (ce_per_token[valid_mask]).mean().item()
                            )
                            if valid_mask.any()
                            else 0.0,
                        }
                    )
                    # Extended weight statistics on aligned region
                    w_eff = weights[valid_mask]
                    if w_eff.numel() > 0:
                        # Basic weight statistics
                        weight_stats = {
                            "train/weight_mean": float(w_eff.mean().item()),
                            "train/weight_min": float(w_eff.min().item()),
                            "train/weight_max": float(w_eff.max().item()),
                            "train/weight_std": float(w_eff.std().item()),
                        }

                        # Weight distribution percentiles (ê³„íšì„œ ìš”êµ¬ì‚¬í•­)
                        try:
                            weight_stats.update(
                                {
                                    "train/weight_p25": float(
                                        torch.quantile(w_eff, 0.25).item()
                                    ),
                                    "train/weight_p75": float(
                                        torch.quantile(w_eff, 0.75).item()
                                    ),
                                    "train/weight_p95": float(
                                        torch.quantile(w_eff, 0.95).item()
                                    ),
                                }
                            )
                        except Exception:
                            # Fallback if quantile fails (e.g., older PyTorch versions)
                            sorted_w = torch.sort(w_eff)[0]
                            n = sorted_w.numel()
                            weight_stats.update(
                                {
                                    "train/weight_p25": float(
                                        sorted_w[int(n * 0.25)].item()
                                    ),
                                    "train/weight_p75": float(
                                        sorted_w[int(n * 0.75)].item()
                                    ),
                                    "train/weight_p95": float(
                                        sorted_w[int(n * 0.95)].item()
                                    ),
                                }
                            )

                        # Failure gates strengthening (ê³„íšì„œ ìš”êµ¬ì‚¬í•­)
                        weight_stats.update(
                            {
                                "train/nan_weights": int(
                                    (~torch.isfinite(weights)).sum().item()
                                ),
                                "train/extreme_weights": int(
                                    (weights > 5.0).sum().item()
                                ),
                            }
                        )

                        metrics.update(weight_stats)

                    # Scorer-specific metrics (ê³„íšì„œ ìš”êµ¬ì‚¬í•­: ë°©ì‹ë³„ íŠ¹í™” ì§€í‘œ)
                    if hasattr(self, "_last_score_out") and self._last_score_out:
                        # Detect scorer type
                        scorer_type = (
                            self.scorer.__class__.__name__.lower()
                            if self.scorer
                            else "unknown"
                        )

                        if "rho1" in scorer_type:
                            # Rho-1 specific metrics
                            scores = self._last_score_out.get("scores")
                            if scores:
                                scores_tensor = (
                                    torch.tensor(scores)
                                    if not isinstance(scores, torch.Tensor)
                                    else scores
                                )
                                total_tokens = float(scores_tensor.numel())
                                # ì„ê³„ê°’ ì´ìƒì˜ í† í°ë“¤ì˜ ë¹„ìœ¨ (usage ratio)
                                threshold = 0.5  # ì„ê³„ê°’ ì„¤ì •
                                high_score_tokens = float(
                                    (scores_tensor > threshold).sum().item()
                                )
                                metrics["train/rho1_usage_ratio"] = (
                                    high_score_tokens / total_tokens
                                    if total_tokens > 0
                                    else 0.0
                                )

                        elif "critic" in scorer_type:
                            # Critic specific metrics
                            deltas = self._last_score_out.get("deltas")
                            if deltas:
                                deltas_tensor = (
                                    torch.tensor(deltas)
                                    if not isinstance(deltas, torch.Tensor)
                                    else deltas
                                )
                                metrics["train/critic_delta_mean"] = float(
                                    deltas_tensor.mean().item()
                                )
                                metrics["train/critic_delta_std"] = float(
                                    deltas_tensor.std().item()
                                )
                    # Valid token ratio
                    total_tokens = float(valid_mask.numel())
                    valid_tokens = float(valid_mask.sum().item())
                    metrics["train/valid_token_ratio"] = (
                        valid_tokens / total_tokens if total_tokens > 0 else 0.0
                    )
                self.mlflow.log_metrics(metrics, step=self.global_step)
            except Exception:
                # Never fail training on logging errors
                pass

        # Failure gates
        if (
            not torch.isfinite(loss)
            or not torch.isfinite(ce_per_token).all()
            or not torch.isfinite(weights).all()
        ):
            if self.mlflow is not None:
                try:
                    self.mlflow.log_metrics(
                        {"train/failure": 1.0}, step=self.global_step
                    )
                except Exception:
                    pass
            raise RuntimeError(
                "Detected NaN/Inf in loss or inputs; aborting training step."
            )

        return {
            "loss": float(loss.detach().item()),
            "lr": float(getattr(self.optimizer, "_last_lr", 0.0)),
        }

    def run(self, ctx: dict[str, Any]) -> dict[str, Any]:
        """
        ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê¸°ëŠ¥ì´ í¬í•¨ëœ í™•ì¥ëœ í›ˆë ¨ ë£¨í”„.
        ì£¼ê¸°ì  ì²´í¬í¬ì¸íŠ¸ ì €ì¥ê³¼ ìµœì¢… ëª¨ë¸ ì €ì¥ì„ ì§€ì›í•©ë‹ˆë‹¤.

        Args:
            ctx: 'train_dataloader'ì™€ 'max_steps' í¬í•¨

        Returns:
            í›ˆë ¨ ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬
        """
        if self.model is None:
            raise RuntimeError("Trainer not initialized. Call setup() first.")

        dataloader = ctx.get("train_dataloader")
        if dataloader is None:
            raise ValueError("Trainer.run expects 'train_dataloader' in ctx")
        max_steps: int | None = ctx.get("max_steps")

        epoch = 0  # ë‹¨ìˆœí™”ë¥¼ ìœ„í•´ epoch=0ìœ¼ë¡œ ì„¤ì •
        metrics = {}

        console.print(
            f"[green]ì²´í¬í¬ì¸íŠ¸ ì €ì¥ í™œì„±í™”: ë§¤ {self.save_interval}ìŠ¤í…ë§ˆë‹¤ ì €ì¥[/green]"
        )
        console.print(f"[green]ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬: {self.checkpoint_dir}[/green]")

        for step, batch in enumerate(dataloader):
            current_step = step + 1

            # ì¬ê°œì‹œ ì´ë¯¸ ì™„ë£Œëœ ìŠ¤í… ê±´ë„ˆë›°ê¸°
            if current_step <= self.start_step:
                continue

            # ê¸°ì¡´ í›ˆë ¨ ìŠ¤í… ì‹¤í–‰ (ë³€ê²½ ì—†ìŒ)
            out = self.train_step(batch)
            metrics = out

            # ì£¼ê¸°ì  ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            if current_step % self.save_interval == 0:
                try:
                    checkpoint_path = self._save_checkpoint(
                        epoch, current_step, metrics
                    )
                    self.saved_checkpoints = self._manage_checkpoints(
                        self.saved_checkpoints, checkpoint_path
                    )
                except Exception as e:
                    console.print(
                        f"[yellow]ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì‹¤íŒ¨ (ìŠ¤í… {current_step}): {e}[/yellow]"
                    )

            # ìµœëŒ€ ìŠ¤í… ë„ë‹¬ ì‹œ ì¢…ë£Œ
            if max_steps is not None and current_step >= max_steps:
                break

        # ìµœì¢… ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        if self.save_final:
            try:
                final_step = step + 1 if "step" in locals() else 1
                final_path = self._save_final_checkpoint(epoch, final_step, metrics)
                console.print(f"[green]ìµœì¢… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {final_path}[/green]")
            except Exception as e:
                console.print(f"[yellow]ìµœì¢… ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}[/yellow]")

        return metrics

    def _save_checkpoint(self, epoch: int, step: int, metrics: dict) -> Path:
        """
        ë‹¨ì¼ ì²´í¬í¬ì¸íŠ¸ ì €ì¥.

        Args:
            epoch: í˜„ì¬ ì—í­
            step: í˜„ì¬ ìŠ¤í…
            metrics: í›ˆë ¨ ë©”íŠ¸ë¦­

        Returns:
            ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
        """

        checkpoint_path = self.checkpoint_dir / f"checkpoint_step_{step}.pt"

        # FSDP í˜¸í™˜ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        self.dist_manager.save_checkpoint(
            model=self.model,
            optimizer=self.optimizer,
            checkpoint_path=str(checkpoint_path),
            epoch=epoch,
            step=step,
            metrics=metrics,
            algorithm=getattr(self, "algorithm", "wmtp"),
            mlflow_run_id=self.mlflow.get_run_id() if self.mlflow else None,
        )

        # MLflowì— ì•„í‹°íŒ©íŠ¸ ì—…ë¡œë“œ (ìˆëŠ” ê²½ìš°)
        if self.mlflow is not None:
            try:
                self.mlflow.log_artifact(
                    local_path=checkpoint_path, artifact_path="checkpoints"
                )
                console.print(
                    f"[green]Checkpoint uploaded to MLflow: {checkpoint_path.name}[/green]"
                )
            except Exception as e:
                console.print(f"[yellow]MLflow upload warning: {e}[/yellow]")

        console.print(f"[green]ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì™„ë£Œ: {checkpoint_path}[/green]")
        return checkpoint_path

    def _manage_checkpoints(
        self, saved_checkpoints: list, new_checkpoint: Path
    ) -> list:
        """
        ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê°œìˆ˜ ê´€ë¦¬ (keep_last ê°œë§Œ ìœ ì§€).

        Args:
            saved_checkpoints: ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸ ëª©ë¡
            new_checkpoint: ìƒˆë¡œ ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸

        Returns:
            ì—…ë°ì´íŠ¸ëœ ì²´í¬í¬ì¸íŠ¸ ëª©ë¡
        """
        saved_checkpoints.append(new_checkpoint)

        # keep_last ê°œìˆ˜ ì´ˆê³¼ ì‹œ ì˜¤ë˜ëœ íŒŒì¼ ì‚­ì œ
        while len(saved_checkpoints) > self.keep_last:
            old_checkpoint = saved_checkpoints.pop(0)
            try:
                if old_checkpoint.exists():
                    old_checkpoint.unlink()
                    console.print(
                        f"[blue]ì´ì „ ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ: {old_checkpoint.name}[/blue]"
                    )
            except Exception as e:
                console.print(f"[yellow]ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ ì‹¤íŒ¨: {e}[/yellow]")

        return saved_checkpoints

    def _save_final_checkpoint(self, epoch: int, step: int, metrics: dict) -> Path:
        """
        ìµœì¢… ëª¨ë¸ ì €ì¥.

        Args:
            epoch: ìµœì¢… ì—í­
            step: ìµœì¢… ìŠ¤í…
            metrics: ìµœì¢… ë©”íŠ¸ë¦­

        Returns:
            ì €ì¥ëœ ìµœì¢… ëª¨ë¸ ê²½ë¡œ
        """

        final_path = self.checkpoint_dir / "final_model.pt"

        # ìµœì¢… ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        self.dist_manager.save_checkpoint(
            model=self.model,
            optimizer=self.optimizer,
            checkpoint_path=str(final_path),
            epoch=epoch,
            step=step,
            metrics=metrics,
            algorithm=getattr(self, "algorithm", "wmtp"),
            final_model=True,
            mlflow_run_id=self.mlflow.get_run_id() if self.mlflow else None,
        )

        # MLflow ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ë“±ë¡ ë° ì•„í‹°íŒ©íŠ¸ ì—…ë¡œë“œ
        if self.mlflow is not None:
            try:
                # ëª¨ë¸ ì´ë¦„ ìƒì„± (recipeì—ì„œ ì•Œê³ ë¦¬ì¦˜ ì •ë³´ ì‚¬ìš©)
                model_name = f"wmtp-{self.algorithm}"

                # ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ë“±ë¡
                self.mlflow.log_model(
                    model=self.model,
                    artifact_path="final_model",
                    registered_model_name=model_name,
                )

                # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì—…ë¡œë“œ
                self.mlflow.log_artifact(
                    local_path=final_path, artifact_path="final_checkpoint"
                )

                console.print(f"[green]MLflow ëª¨ë¸ ë“±ë¡ ì™„ë£Œ: {model_name}[/green]")
            except Exception as e:
                console.print(
                    f"[yellow]MLflow model registration warning: {e}[/yellow]"
                )

        return final_path
