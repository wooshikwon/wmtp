"""
WMTP ì•Œê³ ë¦¬ì¦˜ ê³µí†µ ê¸°ë°˜ í´ë˜ìŠ¤

BaseWmtpTrainerëŠ” ëª¨ë“  WMTP ì•Œê³ ë¦¬ì¦˜(mtp-baseline, critic-wmtp, rho1-wmtp)ì˜
ê³µí†µ ê¸°ëŠ¥ì„ ì œê³µí•˜ëŠ” ì¶”ìƒ í´ë˜ìŠ¤ì…ë‹ˆë‹¤.

ê³µí†µ ê¸°ëŠ¥:
- ëª¨ë¸/ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™” (setup)
- ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ í›ˆë ¨ ë£¨í”„ (run)
- ì²´í¬í¬ì¸íŠ¸ ì €ì¥/ê´€ë¦¬ (_save_checkpoint, _manage_checkpoints, _save_final_checkpoint)
- MLflow í†µí•© ë° ë¶„ì‚° í›ˆë ¨ ì§€ì›

ê° ì•Œê³ ë¦¬ì¦˜ë³„ êµ¬í˜„ì´ í•„ìš”í•œ ì¶”ìƒ ë©”ì„œë“œ:
- compute_head_weights: ì•Œê³ ë¦¬ì¦˜ë³„ í—¤ë“œ ê°€ì¤‘ì¹˜ ê³„ì‚°
- train_step: ì•Œê³ ë¦¬ì¦˜ë³„ í›ˆë ¨ ìŠ¤í… êµ¬í˜„
"""

from __future__ import annotations  # Python 3.10+ íƒ€ì… íŒíŠ¸ í˜¸í™˜ì„±

from abc import abstractmethod  # ì¶”ìƒ ë©”ì„œë“œ
from pathlib import Path  # ê²½ë¡œ ì²˜ë¦¬
from typing import Any  # ë²”ìš© íƒ€ì… íŒíŠ¸

import torch  # PyTorch ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬
import torch.nn as nn  # ì‹ ê²½ë§ ëª¨ë“ˆ
import torch.nn.functional as F  # í•¨ìˆ˜í˜• API (cross_entropy ë“±)
from rich.console import Console  # ì»¬ëŸ¬í’€í•œ ì½˜ì†” ì¶œë ¥

from src.components.base import BaseComponent  # WMTP ì»´í¬ë„ŒíŠ¸ ë² ì´ìŠ¤ í´ë˜ìŠ¤
from src.utils import get_dist_manager  # ë¶„ì‚° í›ˆë ¨ ë§¤ë‹ˆì €

console = Console()  # ì „ì—­ ì½˜ì†” ê°ì²´


def compute_weighted_mtp_loss(
    logits: torch.Tensor,  # [B, S, H, V]
    target_labels: torch.Tensor,  # [B, S, H] - 3D ë¼ë²¨ (MTPDataCollatorì—ì„œ ìƒì„±)
    head_weights: torch.Tensor,  # [B, S, H]
    ignore_index: int = -100,
    selection_mask: torch.Tensor | None = None,  # [B, S, H] - í† í° ì„ íƒ ë§ˆìŠ¤í¬
) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    MTPDataCollator ê¸°ë°˜ ê°„ë‹¨í™”ëœ WMTP ì†ì‹¤ ê³„ì‚°: L_WMTP = Î£(k=0 to H-1) w_{t+k} Ã— CE_k

    3D ë¼ë²¨ì„ ì§ì ‘ ë°›ì•„ì„œ í—¤ë“œë³„ CEë¥¼ ê³„ì‚°í•˜ê³  ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•©ë‹ˆë‹¤.
    ë³µì¡í•œ shift ì—°ì‚°ì´ ì œê±°ë˜ì–´ ì„±ëŠ¥ì´ ëŒ€í­ í–¥ìƒë©ë‹ˆë‹¤.

    Args:
        logits: [B, S, H, V] - MTP ëª¨ë¸ ì¶œë ¥
        target_labels: [B, S, H] - MTPDataCollatorì—ì„œ ìƒì„±ëœ 3D ë¼ë²¨
        head_weights: [B, S, H] - í—¤ë“œë³„ ê°€ì¤‘ì¹˜ ë§¤íŠ¸ë¦­ìŠ¤
        ignore_index: ë¬´ì‹œí•  ë¼ë²¨ ê°’ (-100)
        selection_mask: [B, S, H] - í† í° ì„ íƒ ë§ˆìŠ¤í¬ (Noneì´ë©´ ëª¨ë‘ 1)

    Returns:
        weighted_loss: ê°€ì¤‘ í‰ê·  ì†ì‹¤ (scalar)
        valid_mask: ìœ íš¨í•œ ìœ„ì¹˜ ë§ˆìŠ¤í¬ [B, S]
        ce_per_head: í—¤ë“œë³„ CE ì†ì‹¤ [B, S, H]
    """
    B, S, H, V = logits.shape

    # Input validation (ê°„ì†Œí™”)
    if target_labels.shape != (B, S, H):
        raise ValueError(
            f"Expected target_labels shape [B,S,H], got {target_labels.shape}"
        )
    if head_weights.shape != (B, S, H):
        raise ValueError(
            f"Expected head_weights shape [B,S,H], got {head_weights.shape}"
        )

    # Selection mask ê¸°ë³¸ê°’
    if selection_mask is None:
        selection_mask = torch.ones_like(target_labels, dtype=torch.float)

    # ìœ íš¨ ë¼ë²¨ ë§ˆìŠ¤í¬ ìƒì„±
    valid_mask = (target_labels != ignore_index).float()  # [B, S, H]

    # í—¤ë“œë³„ CE ê³„ì‚° (ë²¡í„°í™”)
    # logits: [B, S, H, V] -> [B*S*H, V]
    # target_labels: [B, S, H] -> [B*S*H]
    logits_flat = logits.view(B * S * H, V)
    target_flat = target_labels.view(B * S * H)

    ce_flat = F.cross_entropy(
        logits_flat, target_flat, ignore_index=ignore_index, reduction="none"
    )  # [B*S*H]

    ce_per_head = ce_flat.view(B, S, H)  # [B, S, H]

    # ë§ˆìŠ¤í‚¹ ì ìš©
    effective_mask = valid_mask * selection_mask  # [B, S, H]

    # ê°€ì¤‘ CE ê³„ì‚°
    weighted_ce = head_weights * ce_per_head * effective_mask  # [B, S, H]
    effective_weights = head_weights * effective_mask  # [B, S, H]

    # í† í°ë³„ ê°€ì¤‘ í‰ê·  ([B, S] ì°¨ì›ìœ¼ë¡œ ì¶•ì•½)
    token_weighted_ce = weighted_ce.sum(dim=2)  # [B, S]
    token_weights = effective_weights.sum(dim=2).clamp(min=1e-8)  # [B, S]

    # í† í°ë³„ ìœ íš¨ì„± (ìµœì†Œ í•˜ë‚˜ í—¤ë“œê°€ ìœ íš¨í•œ ê²½ìš°)
    token_valid_mask = token_weights > 1e-8  # [B, S]

    # ìµœì¢… ìŠ¤ì¹¼ë¼ ì†ì‹¤
    if token_valid_mask.any():
        weighted_loss_per_token = token_weighted_ce / token_weights
        final_loss = (
            weighted_loss_per_token * token_valid_mask.float()
        ).sum() / token_valid_mask.sum()
    else:
        final_loss = torch.tensor(0.0, device=logits.device, dtype=logits.dtype)

    return final_loss, token_valid_mask, ce_per_head


class BaseWmtpTrainer(BaseComponent):
    """WMTP ì•Œê³ ë¦¬ì¦˜ ê³µí†µ ê¸°ëŠ¥ì„ ì œê³µí•˜ëŠ” ì¶”ìƒ ê¸°ë°˜ í´ë˜ìŠ¤.

    ì—°êµ¬ ì² í•™ "Not All Tokens Are What You Need"ì˜ êµ¬í˜„ì„ ìœ„í•´
    ëª¨ë“  WMTP ì•Œê³ ë¦¬ì¦˜(mtp-baseline, critic-wmtp, rho1-wmtp)ì´
    ê³µìœ í•˜ëŠ” ê¸°ë³¸ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

    ğŸ”¬ í•µì‹¬ ë™ì‘ ì›ë¦¬:
        1. ì•Œê³ ë¦¬ì¦˜ë³„ í† í° ê°€ì¤‘ì¹˜ ê³„ì‚° (compute_head_weights - ì¶”ìƒ)
        2. ê° MTP í—¤ë“œë³„ë¡œ Cross-Entropy ì†ì‹¤ ê³„ì‚°
        3. WMTP ê³µì‹ ì ìš©: L_WMTP = Î£ w_{t+k} Ã— CE_k
        4. ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ì™€ MLflow í†µí•©ëœ í›ˆë ¨ ë£¨í”„

    ê³µí†µ ì œê³µ ê¸°ëŠ¥:
        - ëª¨ë¸/ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™” ë° ë¶„ì‚° í›ˆë ¨ ì„¤ì • (setup)
        - ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ í›ˆë ¨ ë£¨í”„ (run)
        - ì£¼ê¸°ì /ìµœì¢… ì²´í¬í¬ì¸íŠ¸ ì €ì¥/ê´€ë¦¬
        - MLflow ì‹¤í—˜ ì¶”ì  í†µí•©
        - í˜¼í•© ì •ë°€ë„ ë° ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘

    í•„ìˆ˜ êµ¬í˜„ ë©”ì„œë“œ (í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ):
        - compute_head_weights: ì•Œê³ ë¦¬ì¦˜ë³„ í—¤ë“œ ê°€ì¤‘ì¹˜ ê³„ì‚°
        - train_step: ì•Œê³ ë¦¬ì¦˜ë³„ í›ˆë ¨ ìŠ¤í… êµ¬í˜„

    í•„ìˆ˜ ì„¤ì • í‚¤:
        - horizon: ì˜ˆì¸¡ í—¤ë“œ ìˆ˜ (ì¼ë°˜ì ìœ¼ë¡œ 4)
        - mixed_precision: í˜¼í•© ì •ë°€ë„ ("bf16"/"fp16"/"fp32")
        - loss_config: ì†ì‹¤ í•¨ìˆ˜ ì„¤ì • (lambda ë“±)
        - scorer: í† í° ê°€ì¤‘ì¹˜ ê³„ì‚°ê¸° (Noneì´ë©´ baseline)
    """

    def __init__(self, config: dict[str, Any] | None = None):
        """WMTP ë² ì´ìŠ¤ íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”.

        Args:
            config: íŠ¸ë ˆì´ë„ˆ ì„¤ì • ë”•ì…”ë„ˆë¦¬ (horizon, ì†ì‹¤ ì„¤ì •, Scorer ë“±)
        """
        super().__init__(config)

        # ì£¼ìš” ì»´í¬ë„ŒíŠ¸ë“¤ (setupì—ì„œ ì´ˆê¸°í™”ë¨)
        self.model: nn.Module | None = None  # Facebook MTP ëª¨ë¸
        self.optimizer = None  # AdamW ë“± ìµœì í™”ê¸°

        # í›ˆë ¨ ìƒíƒœ ì¶”ì 
        self.global_step: int = 0  # ì „ì—­ í›ˆë ¨ ìŠ¤í… ì¹´ìš´í„°

        # MTP ì„¤ì •
        self.horizon: int = int(self.config.get("horizon", 4))  # ì˜ˆì¸¡ í—¤ë“œ ìˆ˜

        # Scorer ì¶œë ¥ ìºì‹± (ì„±ëŠ¥ ìµœì í™”ìš©)
        self._last_score_out: dict[str, Any] | None = None

    def setup(self, ctx: dict[str, Any]) -> None:
        """íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™” - ëª¨ë¸, ë¶„ì‚° í›ˆë ¨, Scorer ë“± ëª¨ë“  ì»´í¬ë„ŒíŠ¸ ì„¤ì •.

        ì´ ë©”ì„œë“œëŠ” íŒŒì´í”„ë¼ì¸ì—ì„œ ì œê³µë°›ì€ ì»´í¬ë„ŒíŠ¸ë“¤ì„ ì—°ê²°í•˜ê³ 
        WMTP í›ˆë ¨ì— í•„ìš”í•œ ëª¨ë“  ì„¤ì •ì„ ì™„ë£Œí•©ë‹ˆë‹¤.

        Args:
            ctx: ì»¨í…ìŠ¤íŠ¸ ë”•ì…”ë„ˆë¦¬ (model, optimizer, scorers, tokenizers ë“±)
        """
        super().setup(ctx)
        dm = get_dist_manager()  # ë¶„ì‚° í›ˆë ¨ ë§¤ë‹ˆì €

        # í•„ìˆ˜ ì»´í¬ë„ŒíŠ¸ ê²€ì¦ ë° ì„¤ì •
        model: nn.Module | None = ctx.get("model")  # Facebook MTP ëª¨ë¸
        optimizer = ctx.get("optimizer")  # AdamW ë“± ìµœì í™”ê¸°
        if model is None:
            raise ValueError("íŠ¸ë ˆì´ë„ˆì— 'model'ì´ í•„ìš”í•©ë‹ˆë‹¤ (ctxì—ì„œ ëˆ„ë½)")
        if optimizer is None:
            raise ValueError("íŠ¸ë ˆì´ë„ˆì— 'optimizer'ê°€ í•„ìš”í•©ë‹ˆë‹¤ (ctxì—ì„œ ëˆ„ë½)")

        # ë¶„ì‚° í›ˆë ¨: FSDP ë˜í•‘ (ì„ íƒì )
        fsdp_cfg = self.config.get("fsdp_config")
        if fsdp_cfg:
            # Fully Sharded Data Parallelë¡œ ëª¨ë¸ ë˜í•‘
            model = dm.setup_fsdp(model, fsdp_cfg)

        self.model = model
        self.optimizer = optimizer

        # ë””ë°”ì´ìŠ¤ ì„¤ì • - ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë¡œë¶€í„° ì¶”ë¡ 
        if hasattr(model, "parameters") and list(model.parameters()):
            self.device = next(model.parameters()).device
        else:
            # í´ë°±: ì‚¬ìš© ê°€ëŠ¥í•œ ìµœì  ë””ë°”ì´ìŠ¤ ìë™ ì„ íƒ
            if torch.cuda.is_available():
                self.device = torch.device("cuda")
            elif torch.backends.mps.is_available():
                self.device = torch.device("mps")
            else:
                self.device = torch.device("cpu")

        # í˜¼í•© ì •ë°€ë„ ì„¤ì •: ë©”ëª¨ë¦¬ì™€ ì†ë„ ìµœì í™”
        mp = str(self.config.get("mixed_precision", "bf16")).lower()
        if mp not in {"bf16", "fp16", "fp32"}:
            mp = "bf16"  # ê¸°ë³¸ê°’: BFloat16 (ê¶Œì¥)

        self._amp_dtype = (
            torch.bfloat16  # BF16: ì•ˆì •ì„±ê³¼ ì„±ëŠ¥ì˜ ê· í˜•
            if mp == "bf16"
            else (torch.float16 if mp == "fp16" else torch.float32)  # FP16 ë˜ëŠ” FP32
        )

        # ğŸ¯ í•µì‹¬: ì•Œê³ ë¦¬ì¦˜ë³„ í† í° ê°€ì¤‘ì¹˜ ê³„ì‚° Scorer ì—°ê²°
        self.scorer = self.config.get(
            "scorer"
        )  # None(baseline), CriticScorer, Rho1Scorer

        # WMTP ì†ì‹¤ í•¨ìˆ˜ ì„¤ì •
        self.loss_cfg = self.config.get("loss_config", {})

        # MLflow ì‹¤í—˜ ì¶”ì  (ì„ íƒì )
        self.mlflow = ctx.get("mlflow_manager")

        # ì•Œê³ ë¦¬ì¦˜ë³„ ë³´ì¡° ëª¨ë¸ë“¤ (ì„ íƒì  - í•´ë‹¹ ì•Œê³ ë¦¬ì¦˜ì—ì„œë§Œ ì‚¬ìš©)
        self.ref_model: nn.Module | None = ctx.get("ref_model")  # Rho-1ìš© ì°¸ì¡° ëª¨ë¸
        self.rm_model: nn.Module | None = ctx.get("rm_model")  # Criticìš© ë³´ìƒ ëª¨ë¸
        self.base_tokenizer = ctx.get("base_tokenizer")  # ê¸°ë³¸ í† í¬ë‚˜ì´ì €
        self.ref_tokenizer = ctx.get("ref_tokenizer")  # ì°¸ì¡° ëª¨ë¸ìš© í† í¬ë‚˜ì´ì €

        # ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ ì„¤ì •
        self.dist_manager = dm  # ë¶„ì‚° í›ˆë ¨ ë§¤ë‹ˆì € (ì²´í¬í¬ì¸íŠ¸ ì €ì¥/ë¡œë“œìš©)

        # Configì—ì„œ ì²´í¬í¬ì¸íŠ¸ ì„¤ì • íŒŒì‹± (Phase 2: Recipeì—ì„œ Configë¡œ ì´ë™)
        config = ctx.get("config")  # Config ê°ì²´ ê°€ì ¸ì˜¤ê¸°
        recipe = ctx.get("recipe")  # Recipe ê°ì²´ ê°€ì ¸ì˜¤ê¸° (ì•Œê³ ë¦¬ì¦˜ ì •ë³´ìš©)

        if config and hasattr(config, "paths") and hasattr(config.paths, "checkpoints"):
            checkpoint_config = config.paths.checkpoints
            self.save_interval = checkpoint_config.save_interval
            self.keep_last = checkpoint_config.keep_last
            self.save_final = checkpoint_config.save_final
        else:
            # ê¸°ë³¸ê°’ ì„¤ì • (í•˜ìœ„ í˜¸í™˜ì„±)
            self.save_interval = 500
            self.keep_last = 3
            self.save_final = True

        # ì•Œê³ ë¦¬ì¦˜ ì •ë³´ ì €ì¥ (recipeì—ì„œ ì¶”ì¶œ)
        self.algorithm = (
            getattr(recipe.train, "algo", "wmtp")
            if recipe and hasattr(recipe, "train")
            else "wmtp"
        )

        # ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ ì„¤ì • (Phase 3: Config + MLflow run_id ê¸°ë°˜)
        checkpoint_path, self.is_s3_checkpoint = self._resolve_checkpoint_path(
            config, recipe, ctx
        )

        if self.is_s3_checkpoint:
            # S3 ê²½ë¡œ: ë¬¸ìì—´ë¡œ ì €ì¥
            self.checkpoint_dir = checkpoint_path
        else:
            # ë¡œì»¬ ê²½ë¡œ: Path ê°ì²´ë¡œ ìƒì„± ë° ë””ë ‰í† ë¦¬ ìƒì„±
            self.checkpoint_dir = Path(checkpoint_path)
            self.checkpoint_dir.mkdir(parents=True, exist_ok=True)

        # ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸ ëª©ë¡ ê´€ë¦¬
        self.saved_checkpoints = []

        # ì¬ê°œ ì²˜ë¦¬ ë¡œì§
        self.start_step = 0
        self.resume_metrics = {}

        resume_checkpoint = ctx.get("resume_checkpoint")
        if resume_checkpoint:
            checkpoint_data = self.dist_manager.load_checkpoint(
                model=self.model,
                optimizer=self.optimizer,
                checkpoint_path=str(resume_checkpoint),
            )

            self.start_step = checkpoint_data.get("step", 0)
            self.resume_metrics = checkpoint_data.get("metrics", {})

            console.print(
                f"[green]Model and optimizer states restored from step {self.start_step}[/green]"
            )

    def _resolve_checkpoint_path(self, config, recipe, ctx) -> tuple[str, bool]:
        """ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ í•´ì„ (Phase 3: Config + MLflow run_id ê¸°ë°˜)

        Args:
            config: Config ê°ì²´
            recipe: Recipe ê°ì²´
            ctx: ì»¨í…ìŠ¤íŠ¸ ë”•ì…”ë„ˆë¦¬

        Returns:
            (checkpoint_dir, is_s3): ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ì™€ S3 ì—¬ë¶€
        """
        from src.utils.path_resolver import resolve_checkpoint_path

        # MLflow run_id ê°€ì ¸ì˜¤ê¸° (ìµœìš°ì„  ì‹ë³„ì)
        run_id = self.mlflow.get_run_id() if self.mlflow else None

        # run_idê°€ ì—†ìœ¼ë©´ recipe.run.name ì‚¬ìš© (fallback)
        if not run_id:
            run_id = (
                recipe.run.name
                if recipe and hasattr(recipe, "run") and hasattr(recipe.run, "name")
                else "no_mlflow_run"
            )

        # Configì—ì„œ ì²´í¬í¬ì¸íŠ¸ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        if config and hasattr(config, "paths") and hasattr(config.paths, "checkpoints"):
            base_path = config.paths.checkpoints.base_path
            checkpoint_dir, is_s3 = resolve_checkpoint_path(base_path, run_id)
            return checkpoint_dir, is_s3
        else:
            # ê¸°ë³¸ê°’ (í•˜ìœ„ í˜¸í™˜ì„±)
            checkpoint_dir = f"./checkpoints/{run_id}"
            return checkpoint_dir, False

    @abstractmethod
    def compute_head_weights(
        self, logits: torch.Tensor, target_ids: torch.Tensor, **kwargs
    ) -> torch.Tensor:
        """ê° ì•Œê³ ë¦¬ì¦˜ë³„ í—¤ë“œ ê°€ì¤‘ì¹˜ ê³„ì‚° (í•„ìˆ˜ êµ¬í˜„).

        Args:
            logits: MTP ëª¨ë¸ ì¶œë ¥ [B, S, H, V]
            target_ids: íƒ€ê²Ÿ í† í° ID [B, S]
            **kwargs: ì•Œê³ ë¦¬ì¦˜ë³„ ì¶”ê°€ ì¸ì (hidden_states, ce_per_head ë“±)

        Returns:
            head_weights: í—¤ë“œë³„ ê°€ì¤‘ì¹˜ [B, S, H]
        """
        pass

    @abstractmethod
    def train_step(self, batch: dict[str, Any]) -> dict[str, Any]:
        """ì•Œê³ ë¦¬ì¦˜ë³„ í›ˆë ¨ ìŠ¤í… êµ¬í˜„ (í•„ìˆ˜ êµ¬í˜„).

        Args:
            batch: í›ˆë ¨ ë°°ì¹˜ ë°ì´í„°

        Returns:
            ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬ (loss, lr ë“±)
        """
        pass

    def run(self, ctx: dict[str, Any]) -> dict[str, Any]:
        """
        ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê¸°ëŠ¥ì´ í¬í•¨ëœ í™•ì¥ëœ í›ˆë ¨ ë£¨í”„.
        ì£¼ê¸°ì  ì²´í¬í¬ì¸íŠ¸ ì €ì¥ê³¼ ìµœì¢… ëª¨ë¸ ì €ì¥ì„ ì§€ì›í•©ë‹ˆë‹¤.

        Args:
            ctx: 'train_dataloader'ì™€ 'max_steps' í¬í•¨

        Returns:
            í›ˆë ¨ ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬
        """
        if self.model is None:
            raise RuntimeError("Trainer not initialized. Call setup() first.")

        dataloader = ctx.get("train_dataloader")
        if dataloader is None:
            raise ValueError("Trainer.run expects 'train_dataloader' in ctx")
        max_steps: int | None = ctx.get("max_steps")

        epoch = 0  # ë‹¨ìˆœí™”ë¥¼ ìœ„í•´ epoch=0ìœ¼ë¡œ ì„¤ì •
        metrics = {}

        console.print(
            f"[green]ì²´í¬í¬ì¸íŠ¸ ì €ì¥ í™œì„±í™”: ë§¤ {self.save_interval}ìŠ¤í…ë§ˆë‹¤ ì €ì¥[/green]"
        )
        console.print(f"[green]ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬: {self.checkpoint_dir}[/green]")

        for step, batch in enumerate(dataloader):
            current_step = step + 1

            # ì¬ê°œì‹œ ì´ë¯¸ ì™„ë£Œëœ ìŠ¤í… ê±´ë„ˆë›°ê¸°
            if current_step <= self.start_step:
                continue

            # ê° ì•Œê³ ë¦¬ì¦˜ë³„ í›ˆë ¨ ìŠ¤í… ì‹¤í–‰ (ì¶”ìƒ ë©”ì„œë“œ)
            out = self.train_step(batch)
            metrics = out

            # ì£¼ê¸°ì  ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            if current_step % self.save_interval == 0:
                try:
                    checkpoint_path = self._save_checkpoint(
                        epoch, current_step, metrics
                    )
                    self.saved_checkpoints = self._manage_checkpoints(
                        self.saved_checkpoints, checkpoint_path
                    )
                except Exception as e:
                    # ì‹¤ì œ íŒŒì¼ ì €ì¥ ì—¬ë¶€ í™•ì¸
                    checkpoint_path = (
                        self.checkpoint_dir / f"checkpoint_step_{current_step}.pt"
                    )
                    if checkpoint_path.exists():
                        console.print(
                            f"[yellow]ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì™„ë£Œ, ë¶€ê°€ ê¸°ëŠ¥ ì˜¤ë¥˜ (ìŠ¤í… {current_step}): {repr(e)}[/yellow]"
                        )
                    else:
                        console.print(
                            f"[red]ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì‹¤íŒ¨ (ìŠ¤í… {current_step}): {repr(e)}[/red]"
                        )

            # ìµœëŒ€ ìŠ¤í… ë„ë‹¬ ì‹œ ì¢…ë£Œ
            if max_steps is not None and current_step >= max_steps:
                break

        # ìµœì¢… ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        if self.save_final:
            try:
                final_step = step + 1 if "step" in locals() else 1
                final_path = self._save_final_checkpoint(epoch, final_step, metrics)
                console.print(f"[green]ìµœì¢… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {final_path}[/green]")
            except Exception as e:
                # ì‹¤ì œ íŒŒì¼ ì €ì¥ ì—¬ë¶€ í™•ì¸
                final_path = self.checkpoint_dir / "final_model.pt"
                if final_path.exists():
                    console.print(
                        f"[yellow]ìµœì¢… ëª¨ë¸ ì €ì¥ ì™„ë£Œ, ë¶€ê°€ ê¸°ëŠ¥ ì˜¤ë¥˜: {repr(e)}[/yellow]"
                    )
                else:
                    console.print(f"[red]ìµœì¢… ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {repr(e)}[/red]")

        return metrics

    def _save_checkpoint(self, epoch: int, step: int, metrics: dict) -> str:
        """
        ë‹¨ì¼ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (Phase 3: S3/ë¡œì»¬ ìë™ íŒë‹¨)

        Args:
            epoch: í˜„ì¬ ì—í­
            step: í˜„ì¬ ìŠ¤í…
            metrics: í›ˆë ¨ ë©”íŠ¸ë¦­

        Returns:
            ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ (ë¬¸ìì—´)
        """
        # S3/ë¡œì»¬ ìë™ íŒë‹¨í•˜ì—¬ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ìƒì„±
        if self.is_s3_checkpoint:
            # S3 ê²½ë¡œ: ë¬¸ìì—´ ê²°í•©
            checkpoint_path = f"{self.checkpoint_dir}/checkpoint_step_{step}.pt"
        else:
            # ë¡œì»¬ ê²½ë¡œ: Path ê°ì²´ ì‚¬ìš©
            checkpoint_path = str(self.checkpoint_dir / f"checkpoint_step_{step}.pt")

        # FSDP í˜¸í™˜ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (MLflow í†µí•©)
        self.dist_manager.save_checkpoint(
            model=self.model,
            optimizer=self.optimizer,
            checkpoint_path=checkpoint_path,
            epoch=epoch,
            step=step,
            mlflow_manager=self.mlflow,  # MLflow ë§¤ë‹ˆì € ì „ë‹¬
            metrics=metrics,
            algorithm=getattr(self, "algorithm", "wmtp"),
            mlflow_run_id=self.mlflow.get_run_id() if self.mlflow else None,
        )

        # MLflow ì—…ë¡œë“œëŠ” ë¶„ì‚° ë§¤ë‹ˆì €ì—ì„œ ìˆ˜í–‰í•¨ (ì¤‘ë³µ ì œê±°)

        storage_type = "S3" if self.is_s3_checkpoint else "ë¡œì»¬"
        console.print(
            f"[green]{storage_type} ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì™„ë£Œ: {checkpoint_path}[/green]"
        )
        return checkpoint_path

    def _manage_checkpoints(self, saved_checkpoints: list, new_checkpoint: str) -> list:
        """
        ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê°œìˆ˜ ê´€ë¦¬ (keep_last ê°œë§Œ ìœ ì§€).
        Phase 3: S3/ë¡œì»¬ ìë™ íŒë‹¨

        Args:
            saved_checkpoints: ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸ ëª©ë¡
            new_checkpoint: ìƒˆë¡œ ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ (ë¬¸ìì—´)

        Returns:
            ì—…ë°ì´íŠ¸ëœ ì²´í¬í¬ì¸íŠ¸ ëª©ë¡
        """
        saved_checkpoints.append(new_checkpoint)

        # keep_last ê°œìˆ˜ ì´ˆê³¼ ì‹œ ì˜¤ë˜ëœ íŒŒì¼ ì‚­ì œ
        while len(saved_checkpoints) > self.keep_last:
            old_checkpoint_path = saved_checkpoints.pop(0)
            try:
                if self.is_s3_checkpoint:
                    # S3 ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ
                    from src.utils.s3 import S3Manager

                    s3_manager = S3Manager()
                    # S3 ê²½ë¡œì—ì„œ ë²„í‚·ê³¼ í‚¤ ë¶„ë¦¬
                    bucket, key = old_checkpoint_path.replace("s3://", "").split("/", 1)
                    s3_manager.delete_object(bucket, key)
                    checkpoint_name = key.split("/")[-1]
                    console.print(
                        f"[blue]ì´ì „ S3 ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ: {checkpoint_name}[/blue]"
                    )
                else:
                    # ë¡œì»¬ ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ
                    old_checkpoint = Path(old_checkpoint_path)
                    if old_checkpoint.exists():
                        old_checkpoint.unlink()
                        console.print(
                            f"[blue]ì´ì „ ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ: {old_checkpoint.name}[/blue]"
                        )
            except Exception as e:
                storage_type = "S3" if self.is_s3_checkpoint else "ë¡œì»¬"
                console.print(
                    f"[yellow]{storage_type} ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ ì‹¤íŒ¨: {e}[/yellow]"
                )

        return saved_checkpoints

    def _save_final_checkpoint(self, epoch: int, step: int, metrics: dict) -> str:
        """
        ìµœì¢… ëª¨ë¸ ì €ì¥ (Phase 3: S3/ë¡œì»¬ ìë™ íŒë‹¨)

        Args:
            epoch: ìµœì¢… ì—í­
            step: ìµœì¢… ìŠ¤í…
            metrics: ìµœì¢… ë©”íŠ¸ë¦­

        Returns:
            ì €ì¥ëœ ìµœì¢… ëª¨ë¸ ê²½ë¡œ (ë¬¸ìì—´)
        """
        # S3/ë¡œì»¬ ìë™ íŒë‹¨í•˜ì—¬ ìµœì¢… ëª¨ë¸ ê²½ë¡œ ìƒì„±
        if self.is_s3_checkpoint:
            # S3 ê²½ë¡œ: ë¬¸ìì—´ ê²°í•©
            final_path = f"{self.checkpoint_dir}/final_model.pt"
        else:
            # ë¡œì»¬ ê²½ë¡œ: Path ê°ì²´ ì‚¬ìš©
            final_path = str(self.checkpoint_dir / "final_model.pt")

        # ìµœì¢… ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (MLflow í†µí•©)
        self.dist_manager.save_checkpoint(
            model=self.model,
            optimizer=self.optimizer,
            checkpoint_path=final_path,
            epoch=epoch,
            step=step,
            mlflow_manager=self.mlflow,  # MLflow ë§¤ë‹ˆì € ì „ë‹¬
            metrics=metrics,
            algorithm=getattr(self, "algorithm", "wmtp"),
            final_model=True,
            mlflow_run_id=self.mlflow.get_run_id() if self.mlflow else None,
        )

        # MLflow ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ë“±ë¡ ë° ì•„í‹°íŒ©íŠ¸ ì—…ë¡œë“œ
        if self.mlflow is not None:
            try:
                # ëª¨ë¸ ì´ë¦„ ìƒì„± (recipeì—ì„œ ì•Œê³ ë¦¬ì¦˜ ì •ë³´ ì‚¬ìš©)
                model_name = f"wmtp-{self.algorithm}"

                # ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ë“±ë¡
                self.mlflow.log_model(
                    model=self.model,
                    artifact_path="final_model",
                    registered_model_name=model_name,
                )

                # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì—…ë¡œë“œ (ë¡œì»¬ ê²½ë¡œë§Œ ì§€ì›)
                if not self.is_s3_checkpoint:
                    self.mlflow.log_artifact(
                        local_path=final_path, artifact_path="final_checkpoint"
                    )
                else:
                    console.print(
                        "[blue]S3 ì²´í¬í¬ì¸íŠ¸ëŠ” MLflow artifact ì—…ë¡œë“œ ìƒëµ[/blue]"
                    )

                console.print(f"[green]MLflow ëª¨ë¸ ë“±ë¡ ì™„ë£Œ: {model_name}[/green]")
            except Exception as e:
                console.print(
                    f"[yellow]MLflow model registration warning: {e}[/yellow]"
                )

        storage_type = "S3" if self.is_s3_checkpoint else "ë¡œì»¬"
        console.print(
            f"[green]{storage_type} ìµœì¢… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {final_path}[/green]"
        )
        return final_path
