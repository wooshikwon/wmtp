"""
ModelLoader V2: ìˆœì°¨ì ì´ê³  ì§ê´€ì ì¸ ëª¨ë¸ ë¡œë”

4ë‹¨ê³„ ìˆœì°¨ í”„ë¡œì„¸ìŠ¤:
1. ë©”íƒ€ë°ì´í„° ë¡œë“œ
2. S3 ë‹¤ìš´ë¡œë“œ (í•„ìš”ì‹œ)
3. ë¡œë”© ì „ëµ ê²°ì •
4. ëª¨ë¸ ë¡œë“œ (ì–‘ìí™” ë° state dict ë§¤í•‘ í¬í•¨)
"""

from __future__ import annotations

import importlib.util
import json
import tempfile
from pathlib import Path
from typing import Any

import torch
from safetensors.torch import load_file
from transformers import AutoConfig, AutoModel, AutoModelForCausalLM, BitsAndBytesConfig

from src.components.loader.base_loader import ModelLoader as BaseModelLoader
from src.components.registry import loader_registry
from src.utils.path_resolver import PathResolver
from src.utils.s3 import create_s3_manager


@loader_registry.register(
    "standardized-model-loader",
    version="4.0.0",
    description="Sequential and intuitive model loader",
)
class ModelLoader(BaseModelLoader):
    """4ë‹¨ê³„ í”„ë¡œì„¸ìŠ¤ë¡œ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    ê° ë‹¨ê³„ëŠ” ë…ë¦½ì ì´ë©° ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.
    """

    def __init__(self, config: dict[str, Any]):
        """ì´ˆê¸°í™”: í•„ìˆ˜ ì„¤ì •ë§Œ ì €ì¥"""
        super().__init__(config)

        # ê²½ë¡œì™€ S3 ì„¤ì •
        self.model_path = config.get("model_path")
        self.path_resolver = PathResolver()
        self.s3_manager = create_s3_manager(config)

        # ì–‘ìí™” ì„¤ì •
        self.use_4bit = config.get("use_4bit", False)
        self.use_8bit = config.get("use_8bit", False)
        devices_config = config.get("devices", {})
        self.mixed_precision = devices_config.get("mixed_precision", "fp32")

    def run(self, inputs: dict[str, Any]) -> dict[str, Any]:
        """ë©”ì¸ ì‹¤í–‰ ë©”ì„œë“œ"""
        model_path = inputs.get("model_path", self.model_path)
        if not model_path:
            raise ValueError("model_path is required")

        # ìˆœì°¨ì  4ë‹¨ê³„ ì‹¤í–‰
        model = self.load_model(model_path)

        return {
            "model": model,
            "path": model_path,
            "loader": self.__class__.__name__,
        }

    def load_model(self, model_path: str, **kwargs) -> Any:  # noqa: ARG002
        """
        ëª¨ë¸ ë¡œë”©ì˜ ì „ì²´ íë¦„ì„ ê´€ë¦¬í•˜ëŠ” ë©”ì¸ ë©”ì„œë“œ
        4ë‹¨ê³„ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰
        """
        print(f"\nğŸš€ ëª¨ë¸ ë¡œë”© ì‹œì‘: {model_path}")

        # Step 1: ë©”íƒ€ë°ì´í„° ë¡œë“œ
        metadata, local_path = self.step1_load_metadata(model_path)

        # Step 2: S3 ë‹¤ìš´ë¡œë“œ (í•„ìš”ì‹œ)
        local_path = self.step2_download_if_needed(model_path, local_path, metadata)

        # Step 3: ë¡œë”© ì „ëµ ê²°ì •
        strategy = self.step3_determine_strategy(metadata)

        # Step 4: ëª¨ë¸ ë¡œë“œ (ì „ëµì— ë”°ë¼)
        if strategy["loader_type"] == "custom_mtp":
            model = self.step4_load_custom_model(local_path, strategy)
        else:
            model = self.step4_load_huggingface_model(local_path, strategy)

        # Step 5: hidden_states ì¶œë ¥ ì„¤ì • (Critic-WMTP ì§€ì›ì„ ìœ„í•´)
        from src.utils.model_utils import ensure_output_hidden_states

        ensure_output_hidden_states(model)

        print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ\n")
        return model

    # ============= STEP 1: ë©”íƒ€ë°ì´í„° ë¡œë“œ =============
    def step1_load_metadata(self, model_path: str) -> tuple[dict, Path | None]:
        """Step 1: ë©”íƒ€ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ê²½ë¡œ íƒ€ì… í™•ì¸"""
        print("  [1/4] ë©”íƒ€ë°ì´í„° ë¡œë“œ ì¤‘...")

        path_type, resolved = self.path_resolver.resolve(model_path)
        metadata = {}
        local_path = None

        if path_type == "s3":
            # S3ì—ì„œ ë©”íƒ€ë°ì´í„°ë§Œ ë¨¼ì € ë¡œë“œ
            metadata = self._load_metadata_from_s3(resolved)
        else:
            # ë¡œì»¬ì—ì„œ ë©”íƒ€ë°ì´í„° ë¡œë“œ
            local_path = Path(resolved)
            metadata_file = local_path / "metadata.json"
            if metadata_file.exists():
                with open(metadata_file) as f:
                    metadata = json.load(f)

        # ë©”íƒ€ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì¶”ë¡ 
        if not metadata.get("loading_strategy"):
            metadata["loading_strategy"] = self._infer_strategy(metadata)

        return metadata, local_path

    # ============= STEP 2: S3 ë‹¤ìš´ë¡œë“œ =============
    def step2_download_if_needed(
        self, model_path: str, local_path: Path | None, metadata: dict
    ) -> Path:
        """Step 2: S3 ê²½ë¡œì¸ ê²½ìš° í•„ìš”í•œ íŒŒì¼ë“¤ì„ ë‹¤ìš´ë¡œë“œ"""
        path_type, resolved = self.path_resolver.resolve(model_path)

        if path_type != "s3":
            print("  [2/4] ë¡œì»¬ ëª¨ë¸ ì‚¬ìš© (ë‹¤ìš´ë¡œë“œ ìŠ¤í‚µ)")
            return local_path

        print("  [2/4] S3ì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘...")

        # ì„ì‹œ ë””ë ‰í† ë¦¬ì— ë‹¤ìš´ë¡œë“œ
        temp_dir = tempfile.mkdtemp()
        local_path = Path(temp_dir) / "model"
        local_path.mkdir(parents=True)

        # í•„ìš”í•œ íŒŒì¼ ëª©ë¡ ê²°ì •
        strategy = metadata.get("loading_strategy", {})
        required_files = strategy.get(
            "required_files",
            ["config.json", "model.safetensors", "modeling.py", "metadata.json"],
        )

        # S3ì—ì„œ ë‹¤ìš´ë¡œë“œ
        bucket, key_prefix = self.path_resolver.extract_bucket_and_key(resolved)
        for filename in required_files:
            self._download_file_from_s3(key_prefix, filename, local_path)

        return local_path

    # ============= STEP 3: ì „ëµ ê²°ì • =============
    def step3_determine_strategy(self, metadata: dict) -> dict:
        """Step 3: ë©”íƒ€ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¡œë”© ì „ëµ ê²°ì •"""
        print("  [3/4] ë¡œë”© ì „ëµ ê²°ì • ì¤‘...")

        strategy = metadata.get("loading_strategy", {})

        # ê¸°ë³¸ê°’ ì„¤ì •
        strategy.setdefault("loader_type", "huggingface")
        strategy.setdefault("state_dict_mapping", {})

        print(f"      â†’ {strategy['loader_type']} ì „ëµ ì‚¬ìš©")
        return strategy

    # ============= STEP 4: ëª¨ë¸ ë¡œë“œ =============
    def step4_load_custom_model(self, local_path: Path, strategy: dict) -> Any:
        """Step 4-A: ì»¤ìŠ¤í…€ MTP ëª¨ë¸ ë¡œë“œ"""
        print("  [4/4] ì»¤ìŠ¤í…€ MTP ëª¨ë¸ ë¡œë“œ ì¤‘...")

        # 4.1: modeling.py ë™ì  ì„í¬íŠ¸
        module_file = strategy.get("custom_module_file", "modeling.py")
        modeling_path = local_path / module_file

        spec = importlib.util.spec_from_file_location("custom_modeling", modeling_path)
        custom_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(custom_module)

        # 4.2: ëª¨ë¸ í´ë˜ìŠ¤ ì°¾ê¸°
        model_class_name = strategy.get("model_class_name")
        model_class = getattr(custom_module, model_class_name, None)

        if not model_class:
            # MTP í´ë˜ìŠ¤ ìë™ íƒìƒ‰
            for name in dir(custom_module):
                if "MTP" in name and isinstance(getattr(custom_module, name), type):
                    model_class = getattr(custom_module, name)
                    break

        # 4.3: ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        config = AutoConfig.from_pretrained(str(local_path), trust_remote_code=True)
        model = model_class(config)

        # 4.4: State dict ë¡œë“œ ë° ë§¤í•‘
        safetensors_path = local_path / "model.safetensors"
        if safetensors_path.exists():
            state_dict = load_file(str(safetensors_path))

            # State dict ë§¤í•‘ ì ìš©
            mapping = strategy.get("state_dict_mapping", {})
            if mapping:
                state_dict = self.apply_state_dict_mapping(state_dict, mapping)

            model.load_state_dict(state_dict, strict=False)

        # 4.5: Meta MTP ëª¨ë¸ì— HuggingFace í˜¸í™˜ì„± íŒ¨ì¹˜ ì ìš©
        if "llama" in str(local_path).lower() and hasattr(model, "forward"):
            self._patch_meta_mtp_forward(model)
            print("      â†’ Meta MTP ëª¨ë¸ì— HF í˜¸í™˜ì„± íŒ¨ì¹˜ ì ìš© ì™„ë£Œ")

        return model

    def step4_load_huggingface_model(self, local_path: Path, strategy: dict) -> Any:
        """Step 4-B: HuggingFace ëª¨ë¸ ë¡œë“œ"""
        print("  [4/4] HuggingFace ëª¨ë¸ ë¡œë“œ ì¤‘...")

        # 4.1: ëª¨ë¸ í´ë˜ìŠ¤ ê²°ì •
        transformers_class = strategy.get("transformers_class", "AutoModelForCausalLM")
        model_class = (
            AutoModel if transformers_class == "AutoModel" else AutoModelForCausalLM
        )

        # 4.2: ì–‘ìí™” ì„¤ì •
        quantization_config = None
        if self.use_4bit or self.use_8bit:
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=self.use_4bit,
                load_in_8bit=self.use_8bit,
                bnb_4bit_compute_dtype=self.get_compute_dtype(),
            )
            print(f"      â†’ ì–‘ìí™” ì ìš©: 4bit={self.use_4bit}, 8bit={self.use_8bit}")

        # 4.3: ëª¨ë¸ ë¡œë“œ
        model = model_class.from_pretrained(
            str(local_path),
            quantization_config=quantization_config,
            device_map=None,  # Trainerê°€ ë‹´ë‹¹
            torch_dtype=self.get_torch_dtype(),
            trust_remote_code=True,
        )

        return model

    # ============= ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œ =============
    def apply_state_dict_mapping(self, state_dict: dict, mapping: dict) -> dict:
        """State dict í‚¤ ë§¤í•‘ ì ìš©"""
        if not mapping:
            return state_dict

        result = {}
        remove_prefix = mapping.get("remove_prefix", "")
        add_prefix = mapping.get("add_prefix", "")

        for key, value in state_dict.items():
            new_key = key

            if remove_prefix and new_key.startswith(remove_prefix):
                new_key = new_key[len(remove_prefix) :]

            if add_prefix:
                new_key = f"{add_prefix}{new_key}"

            result[new_key] = value

        return result

    def get_torch_dtype(self):
        """Torch dtype ê²°ì •"""
        if self.mixed_precision == "bf16":
            return torch.bfloat16
        elif self.mixed_precision == "fp16":
            return torch.float16
        return torch.float32

    def get_compute_dtype(self):
        """ì–‘ìí™” compute dtype ê²°ì •"""
        return torch.bfloat16 if self.mixed_precision == "bf16" else torch.float16

    def _patch_meta_mtp_forward(self, model):
        """Meta MTP ëª¨ë¸ì— HuggingFace í˜¸í™˜ forward ë©”ì„œë“œ íŒ¨ì¹˜

        Metaì˜ Llama MTP ëª¨ë¸ì€ ë‹¤ë¥¸ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì‚¬ìš©:
        - Meta: forward(tokens, start_pos, return_all_heads)
        - HF: forward(input_ids, attention_mask, output_hidden_states, ...)

        ì´ íŒ¨ì¹˜ëŠ” ëŸ°íƒ€ì„ì— HF í˜¸í™˜ì„±ì„ ì¶”ê°€í•˜ì—¬ WMTP ì‹œìŠ¤í…œê³¼ í†µí•©í•©ë‹ˆë‹¤.
        """
        original_forward = model.forward
        model._original_forward = original_forward  # ì›ë³¸ ë³´ì¡´

        def hf_compatible_forward(
            input_ids=None,
            attention_mask=None,  # noqa: ARG001
            tokens=None,
            start_pos=None,
            output_hidden_states=False,
            output_attentions=False,  # noqa: ARG001
            return_dict=True,
            **kwargs,
        ):
            """HuggingFace í˜¸í™˜ forward ì¸í„°í˜ì´ìŠ¤"""

            # HF ìŠ¤íƒ€ì¼ ì…ë ¥ ì²˜ë¦¬
            if input_ids is not None:
                tokens = input_ids
                start_pos = 0 if start_pos is None else start_pos

                # Meta MTP forward í˜¸ì¶œ
                logits = original_forward(
                    tokens=tokens,
                    start_pos=start_pos,
                    return_all_heads=True,  # ëª¨ë“  MTP í—¤ë“œ ì‚¬ìš©
                )

                # HF ìŠ¤íƒ€ì¼ ì¶œë ¥ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                if return_dict:
                    result = {
                        "logits": logits,
                    }

                    # hidden_states ì¶”ì¶œ ì‹œë„ (ëª¨ë¸ì´ ì €ì¥í–ˆë‹¤ë©´)
                    if output_hidden_states and hasattr(model, "_last_hidden_states"):
                        result["hidden_states"] = model._last_hidden_states
                        result["last_hidden_state"] = model._last_hidden_states[-1]

                    return result  # Return dict directly for trainer compatibility
                else:
                    return logits

            # Meta ìŠ¤íƒ€ì¼ ì§ì ‘ í˜¸ì¶œ
            elif tokens is not None:
                return original_forward(
                    tokens=tokens,
                    start_pos=start_pos if start_pos is not None else 0,
                    **kwargs,
                )
            else:
                raise ValueError("Either input_ids or tokens must be provided")

        # forward ë©”ì„œë“œ êµì²´
        model.forward = hf_compatible_forward

        # HF í˜¸í™˜ ì†ì„± ì¶”ê°€
        if hasattr(model, "params"):
            model.config = type(
                "Config",
                (),
                {
                    "vocab_size": model.params.vocab_size,
                    "hidden_size": model.params.dim,
                    "n_layers": model.params.n_layers,
                    "n_heads": model.params.n_heads,
                    "output_hidden_states": False,
                    "output_attentions": False,
                },
            )()

        return model

    def _infer_strategy(self, metadata: dict) -> dict:
        """ë©”íƒ€ë°ì´í„°ê°€ ì—†ì„ ë•Œ ì „ëµ ì¶”ë¡ """
        wmtp_type = metadata.get("wmtp_type", "base_model")

        if wmtp_type == "base_model":
            return {
                "loader_type": "custom_mtp",
                "model_class_name": "GPTMTPForCausalLM",
                "custom_module_file": "modeling.py",
            }
        else:
            return {
                "loader_type": "huggingface",
                "transformers_class": "AutoModelForCausalLM",
            }

    def _load_metadata_from_s3(self, s3_path: str) -> dict:
        """S3ì—ì„œ metadata.jsonë§Œ ë¡œë“œ"""
        if not self.s3_manager:
            return {}

        try:
            bucket, key_prefix = self.path_resolver.extract_bucket_and_key(s3_path)
            metadata_key = f"{key_prefix.rstrip('/')}/metadata.json"
            stream = self.s3_manager.stream_dataset(metadata_key)
            return json.load(stream)
        except Exception:
            return {}

    def _download_file_from_s3(
        self, key_prefix: str, filename: str, local_path: Path
    ):
        """S3ì—ì„œ ë‹¨ì¼ íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
        if not self.s3_manager:
            return

        try:
            file_key = f"{key_prefix.rstrip('/')}/{filename}"

            # íŒŒì¼ íƒ€ì…ì— ë”°ë¼ ì ì ˆí•œ ìŠ¤íŠ¸ë¦¼ ë©”ì„œë“œ ì‚¬ìš©
            if filename.endswith((".json", ".py")):
                stream = self.s3_manager.stream_dataset(file_key)
            else:
                stream = self.s3_manager.stream_model(file_key)

            with open(local_path / filename, "wb") as f:
                f.write(stream.read())
        except Exception as e:
            # í•„ìˆ˜ íŒŒì¼ì´ ì•„ë‹ˆë©´ ê²½ê³ ë§Œ
            if filename in ["config.json", "model.safetensors"]:
                raise RuntimeError(f"Required file {filename} not found: {e}") from e
            print(f"      â†’ {filename} ìŠ¤í‚µ (ì˜µì…˜)")
