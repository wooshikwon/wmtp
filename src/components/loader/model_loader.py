"""
ModelLoader V2: ìˆœì°¨ì ì´ê³  ì§ê´€ì ì¸ ëª¨ë¸ ë¡œë”

4ë‹¨ê³„ ìˆœì°¨ í”„ë¡œì„¸ìŠ¤:
1. ë©”íƒ€ë°ì´í„° ë¡œë“œ
2. S3 ë‹¤ìš´ë¡œë“œ (í•„ìš”ì‹œ)
3. ë¡œë”© ì „ëµ ê²°ì •
4. ëª¨ë¸ ë¡œë“œ (ì–‘ìí™” ë° state dict ë§¤í•‘ í¬í•¨)
"""

from __future__ import annotations

import json
import tempfile
import importlib.util
from pathlib import Path
from typing import Any, Dict, Optional, Tuple

import torch
from safetensors.torch import load_file
from transformers import AutoConfig, AutoModelForCausalLM, AutoModel, BitsAndBytesConfig

from src.components.loader.base_loader import ModelLoader as BaseModelLoader
from src.components.registry import loader_registry
from src.utils.optimized_s3_transfer import OptimizedS3Transfer
from src.utils.path_resolver import PathResolver
from src.utils.s3 import create_s3_manager


@loader_registry.register(
    "standardized-model-loader",
    version="4.0.0",
    description="Sequential and intuitive model loader",
)
class ModelLoader(BaseModelLoader):
    """4ë‹¨ê³„ í”„ë¡œì„¸ìŠ¤ë¡œ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    ê° ë‹¨ê³„ëŠ” ë…ë¦½ì ì´ë©° ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.
    """

    def __init__(self, config: dict[str, Any]):
        """ì´ˆê¸°í™”: í•„ìˆ˜ ì„¤ì •ë§Œ ì €ì¥"""
        super().__init__(config)

        # ê²½ë¡œì™€ S3 ì„¤ì •
        self.model_path = config.get("model_path")
        self.path_resolver = PathResolver()
        self.s3_manager = create_s3_manager(config)

        # ì–‘ìí™” ì„¤ì •
        self.use_4bit = config.get("use_4bit", False)
        self.use_8bit = config.get("use_8bit", False)
        devices_config = config.get("devices", {})
        self.mixed_precision = devices_config.get("mixed_precision", "fp32")

    def run(self, inputs: dict[str, Any]) -> dict[str, Any]:
        """ë©”ì¸ ì‹¤í–‰ ë©”ì„œë“œ"""
        model_path = inputs.get("model_path", self.model_path)
        if not model_path:
            raise ValueError("model_path is required")

        # ìˆœì°¨ì  4ë‹¨ê³„ ì‹¤í–‰
        model = self.load_model(model_path)

        return {
            "model": model,
            "path": model_path,
            "loader": self.__class__.__name__,
        }

    def load_model(self, model_path: str, **kwargs) -> Any:
        """
        ëª¨ë¸ ë¡œë”©ì˜ ì „ì²´ íë¦„ì„ ê´€ë¦¬í•˜ëŠ” ë©”ì¸ ë©”ì„œë“œ
        4ë‹¨ê³„ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰
        """
        print(f"\nğŸš€ ëª¨ë¸ ë¡œë”© ì‹œì‘: {model_path}")

        # Step 1: ë©”íƒ€ë°ì´í„° ë¡œë“œ
        metadata, local_path = self.step1_load_metadata(model_path)

        # Step 2: S3 ë‹¤ìš´ë¡œë“œ (í•„ìš”ì‹œ)
        local_path = self.step2_download_if_needed(model_path, local_path, metadata)

        # Step 3: ë¡œë”© ì „ëµ ê²°ì •
        strategy = self.step3_determine_strategy(metadata)

        # Step 4: ëª¨ë¸ ë¡œë“œ (ì „ëµì— ë”°ë¼)
        if strategy["loader_type"] == "custom_mtp":
            model = self.step4_load_custom_model(local_path, strategy)
        else:
            model = self.step4_load_huggingface_model(local_path, strategy)

        print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ\n")
        return model

    # ============= STEP 1: ë©”íƒ€ë°ì´í„° ë¡œë“œ =============
    def step1_load_metadata(self, model_path: str) -> Tuple[Dict, Optional[Path]]:
        """Step 1: ë©”íƒ€ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ê²½ë¡œ íƒ€ì… í™•ì¸"""
        print(f"  [1/4] ë©”íƒ€ë°ì´í„° ë¡œë“œ ì¤‘...")

        path_type, resolved = self.path_resolver.resolve(model_path)
        metadata = {}
        local_path = None

        if path_type == "s3":
            # S3ì—ì„œ ë©”íƒ€ë°ì´í„°ë§Œ ë¨¼ì € ë¡œë“œ
            metadata = self._load_metadata_from_s3(resolved)
        else:
            # ë¡œì»¬ì—ì„œ ë©”íƒ€ë°ì´í„° ë¡œë“œ
            local_path = Path(resolved)
            metadata_file = local_path / "metadata.json"
            if metadata_file.exists():
                with open(metadata_file) as f:
                    metadata = json.load(f)

        # ë©”íƒ€ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì¶”ë¡ 
        if not metadata.get("loading_strategy"):
            metadata["loading_strategy"] = self._infer_strategy(metadata)

        return metadata, local_path

    # ============= STEP 2: S3 ë‹¤ìš´ë¡œë“œ =============
    def step2_download_if_needed(
        self, model_path: str, local_path: Optional[Path], metadata: Dict
    ) -> Path:
        """Step 2: S3 ê²½ë¡œì¸ ê²½ìš° í•„ìš”í•œ íŒŒì¼ë“¤ì„ ë‹¤ìš´ë¡œë“œ"""
        path_type, resolved = self.path_resolver.resolve(model_path)

        if path_type != "s3":
            print(f"  [2/4] ë¡œì»¬ ëª¨ë¸ ì‚¬ìš© (ë‹¤ìš´ë¡œë“œ ìŠ¤í‚µ)")
            return local_path

        print(f"  [2/4] S3ì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘...")

        # ì„ì‹œ ë””ë ‰í† ë¦¬ì— ë‹¤ìš´ë¡œë“œ
        temp_dir = tempfile.mkdtemp()
        local_path = Path(temp_dir) / "model"
        local_path.mkdir(parents=True)

        # í•„ìš”í•œ íŒŒì¼ ëª©ë¡ ê²°ì •
        strategy = metadata.get("loading_strategy", {})
        required_files = strategy.get("required_files", [
            "config.json",
            "model.safetensors",
            "modeling.py",
            "metadata.json"
        ])

        # S3ì—ì„œ ë‹¤ìš´ë¡œë“œ
        bucket, key_prefix = self.path_resolver.extract_bucket_and_key(resolved)
        for filename in required_files:
            self._download_file_from_s3(bucket, key_prefix, filename, local_path)

        return local_path

    # ============= STEP 3: ì „ëµ ê²°ì • =============
    def step3_determine_strategy(self, metadata: Dict) -> Dict:
        """Step 3: ë©”íƒ€ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¡œë”© ì „ëµ ê²°ì •"""
        print(f"  [3/4] ë¡œë”© ì „ëµ ê²°ì • ì¤‘...")

        strategy = metadata.get("loading_strategy", {})

        # ê¸°ë³¸ê°’ ì„¤ì •
        strategy.setdefault("loader_type", "huggingface")
        strategy.setdefault("state_dict_mapping", {})

        print(f"      â†’ {strategy['loader_type']} ì „ëµ ì‚¬ìš©")
        return strategy

    # ============= STEP 4: ëª¨ë¸ ë¡œë“œ =============
    def step4_load_custom_model(self, local_path: Path, strategy: Dict) -> Any:
        """Step 4-A: ì»¤ìŠ¤í…€ MTP ëª¨ë¸ ë¡œë“œ"""
        print(f"  [4/4] ì»¤ìŠ¤í…€ MTP ëª¨ë¸ ë¡œë“œ ì¤‘...")

        # 4.1: modeling.py ë™ì  ì„í¬íŠ¸
        module_file = strategy.get("custom_module_file", "modeling.py")
        modeling_path = local_path / module_file

        spec = importlib.util.spec_from_file_location("custom_modeling", modeling_path)
        custom_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(custom_module)

        # 4.2: ëª¨ë¸ í´ë˜ìŠ¤ ì°¾ê¸°
        model_class_name = strategy.get("model_class_name")
        model_class = getattr(custom_module, model_class_name, None)

        if not model_class:
            # MTP í´ë˜ìŠ¤ ìë™ íƒìƒ‰
            for name in dir(custom_module):
                if "MTP" in name and isinstance(getattr(custom_module, name), type):
                    model_class = getattr(custom_module, name)
                    break

        # 4.3: ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        config = AutoConfig.from_pretrained(str(local_path), trust_remote_code=True)
        model = model_class(config)

        # 4.4: State dict ë¡œë“œ ë° ë§¤í•‘
        safetensors_path = local_path / "model.safetensors"
        if safetensors_path.exists():
            state_dict = load_file(str(safetensors_path))

            # State dict ë§¤í•‘ ì ìš©
            mapping = strategy.get("state_dict_mapping", {})
            if mapping:
                state_dict = self.apply_state_dict_mapping(state_dict, mapping)

            model.load_state_dict(state_dict, strict=False)

        return model

    def step4_load_huggingface_model(self, local_path: Path, strategy: Dict) -> Any:
        """Step 4-B: HuggingFace ëª¨ë¸ ë¡œë“œ"""
        print(f"  [4/4] HuggingFace ëª¨ë¸ ë¡œë“œ ì¤‘...")

        # 4.1: ëª¨ë¸ í´ë˜ìŠ¤ ê²°ì •
        transformers_class = strategy.get("transformers_class", "AutoModelForCausalLM")
        model_class = AutoModel if transformers_class == "AutoModel" else AutoModelForCausalLM

        # 4.2: ì–‘ìí™” ì„¤ì •
        quantization_config = None
        if self.use_4bit or self.use_8bit:
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=self.use_4bit,
                load_in_8bit=self.use_8bit,
                bnb_4bit_compute_dtype=self.get_compute_dtype(),
            )
            print(f"      â†’ ì–‘ìí™” ì ìš©: 4bit={self.use_4bit}, 8bit={self.use_8bit}")

        # 4.3: ëª¨ë¸ ë¡œë“œ
        model = model_class.from_pretrained(
            str(local_path),
            quantization_config=quantization_config,
            device_map=None,  # Trainerê°€ ë‹´ë‹¹
            torch_dtype=self.get_torch_dtype(),
            trust_remote_code=True,
        )

        return model

    # ============= ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œ =============
    def apply_state_dict_mapping(self, state_dict: Dict, mapping: Dict) -> Dict:
        """State dict í‚¤ ë§¤í•‘ ì ìš©"""
        if not mapping:
            return state_dict

        result = {}
        remove_prefix = mapping.get("remove_prefix", "")
        add_prefix = mapping.get("add_prefix", "")

        for key, value in state_dict.items():
            new_key = key

            if remove_prefix and new_key.startswith(remove_prefix):
                new_key = new_key[len(remove_prefix):]

            if add_prefix:
                new_key = f"{add_prefix}{new_key}"

            result[new_key] = value

        return result

    def get_torch_dtype(self):
        """Torch dtype ê²°ì •"""
        if self.mixed_precision == "bf16":
            return torch.bfloat16
        elif self.mixed_precision == "fp16":
            return torch.float16
        return torch.float32

    def get_compute_dtype(self):
        """ì–‘ìí™” compute dtype ê²°ì •"""
        return torch.bfloat16 if self.mixed_precision == "bf16" else torch.float16

    def _infer_strategy(self, metadata: Dict) -> Dict:
        """ë©”íƒ€ë°ì´í„°ê°€ ì—†ì„ ë•Œ ì „ëµ ì¶”ë¡ """
        wmtp_type = metadata.get("wmtp_type", "base_model")

        if wmtp_type == "base_model":
            return {
                "loader_type": "custom_mtp",
                "model_class_name": "GPTMTPForCausalLM",
                "custom_module_file": "modeling.py",
            }
        else:
            return {
                "loader_type": "huggingface",
                "transformers_class": "AutoModelForCausalLM",
            }

    def _load_metadata_from_s3(self, s3_path: str) -> Dict:
        """S3ì—ì„œ metadata.jsonë§Œ ë¡œë“œ"""
        if not self.s3_manager:
            return {}

        try:
            bucket, key_prefix = self.path_resolver.extract_bucket_and_key(s3_path)
            metadata_key = f"{key_prefix.rstrip('/')}/metadata.json"
            stream = self.s3_manager.stream_dataset(metadata_key)
            return json.load(stream)
        except Exception:
            return {}

    def _download_file_from_s3(
        self, bucket: str, key_prefix: str, filename: str, local_path: Path
    ):
        """S3ì—ì„œ ë‹¨ì¼ íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
        if not self.s3_manager:
            return

        try:
            file_key = f"{key_prefix.rstrip('/')}/{filename}"

            # íŒŒì¼ íƒ€ì…ì— ë”°ë¼ ì ì ˆí•œ ìŠ¤íŠ¸ë¦¼ ë©”ì„œë“œ ì‚¬ìš©
            if filename.endswith((".json", ".py")):
                stream = self.s3_manager.stream_dataset(file_key)
            else:
                stream = self.s3_manager.stream_model(file_key)

            with open(local_path / filename, "wb") as f:
                f.write(stream.read())
        except Exception as e:
            # í•„ìˆ˜ íŒŒì¼ì´ ì•„ë‹ˆë©´ ê²½ê³ ë§Œ
            if filename in ["config.json", "model.safetensors"]:
                raise RuntimeError(f"Required file {filename} not found: {e}")
            print(f"      â†’ {filename} ìŠ¤í‚µ (ì˜µì…˜)")