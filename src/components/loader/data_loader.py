"""
DataLoader V2: ìˆœì°¨ì ì´ê³  ì§ê´€ì ì¸ ë°ì´í„° ë¡œë”

4ë‹¨ê³„ ìˆœì°¨ í”„ë¡œì„¸ìŠ¤:
1. ë°ì´í„°ì…‹ ì†ŒìŠ¤ í™•ì¸
2. ë©”íƒ€ë°ì´í„° ë¡œë“œ ë° ê°€ìš©ì„± í™•ì¸
3. ë°ì´í„° ë‹¤ìš´ë¡œë“œ/ë¡œë“œ
4. í¬ë§· ì •ê·œí™” ë° ì „ì²˜ë¦¬
"""

from __future__ import annotations

import json
from pathlib import Path
from typing import Any

from datasets import Dataset, load_dataset, load_from_disk
from src.components.loader.base_loader import DatasetLoader
from src.components.registry import loader_registry
from src.utils.path_resolver import PathResolver
from src.utils.s3 import create_s3_manager


@loader_registry.register(
    "unified-data-loader",
    version="3.0.0",
    description="Sequential and intuitive data loader",
)
class DataLoader(DatasetLoader):
    """4ë‹¨ê³„ í”„ë¡œì„¸ìŠ¤ë¡œ ë°ì´í„°ì…‹ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    ê° ë‹¨ê³„ëŠ” ë…ë¦½ì ì´ë©° ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.
    """

    def __init__(self, config: dict[str, Any]):
        """ì´ˆê¸°í™”: í•„ìˆ˜ ì„¤ì •ë§Œ ì €ì¥"""
        super().__init__(config)

        # ê²½ë¡œì™€ íƒ€ì… ì„¤ì •
        self.dataset_path = config.get("dataset_path")
        self.dataset_type = config.get("dataset_type")
        self.split = config.get("split", "train")

        # ìœ í‹¸ë¦¬í‹°
        self.path_resolver = PathResolver()
        self.s3_manager = create_s3_manager(config)

        # ì˜µì…˜
        self.max_samples = config.get("max_samples")
        self.seed = config.get("seed", 42)

    def run(self, inputs: dict[str, Any]) -> dict[str, Any]:
        """ë©”ì¸ ì‹¤í–‰ ë©”ì„œë“œ"""
        # Factoryì—ì„œ ì´ë¯¸ ì„¤ì •ëœ ê²½ë¡œ ì‚¬ìš© (ì˜¤ë²„ë¼ì´ë“œ ê°€ëŠ¥)
        dataset_path = inputs.get("dataset_path", self.dataset_path)
        if not dataset_path:
            raise ValueError("dataset_path is required")

        # ìˆœì°¨ì  4ë‹¨ê³„ ì‹¤í–‰
        dataset = self.load_dataset_sequential(dataset_path)

        return {
            "dataset": dataset,
            "dataset_type": self.dataset_type,
            "split": self.split,
            "size": len(dataset),
            "path": dataset_path,
            "loader": self.__class__.__name__,
        }

    def load(self, path: str, **kwargs) -> Any:
        """BaseLoader abstract method êµ¬í˜„"""
        return self.load_dataset_sequential(path)

    def preprocess(self, data: Any, **kwargs) -> Any:
        """BaseLoader abstract method êµ¬í˜„"""
        # ì´ë¯¸ step4ì—ì„œ ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•˜ë¯€ë¡œ ê·¸ëŒ€ë¡œ ë°˜í™˜
        return data

    def load_dataset_sequential(self, dataset_path: str) -> Dataset:
        """
        ë°ì´í„°ì…‹ ë¡œë”©ì˜ ì „ì²´ íë¦„ì„ ê´€ë¦¬í•˜ëŠ” ë©”ì¸ ë©”ì„œë“œ
        4ë‹¨ê³„ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰
        """
        print(f"\nğŸš€ ë°ì´í„°ì…‹ ë¡œë”© ì‹œì‘: {dataset_path}")

        # Step 1: ë°ì´í„°ì…‹ ì†ŒìŠ¤ í™•ì¸
        dataset_type, path_type, resolved_path = self.step1_identify_source(
            dataset_path
        )

        # Step 2: ë©”íƒ€ë°ì´í„° í™•ì¸
        metadata = self.step2_check_metadata(resolved_path, dataset_type, path_type)

        # Step 3: ë°ì´í„° ë¡œë“œ
        raw_dataset = self.step3_load_data(resolved_path, path_type)

        # Step 4: í¬ë§· ì •ê·œí™” ë° ì „ì²˜ë¦¬
        dataset = self.step4_normalize_and_preprocess(
            raw_dataset, dataset_type, metadata
        )

        print(f"âœ… ë°ì´í„°ì…‹ ë¡œë”© ì™„ë£Œ: {len(dataset)} samples\n")
        return dataset

    # ============= STEP 1: ì†ŒìŠ¤ í™•ì¸ =============
    def step1_identify_source(self, dataset_path: str) -> tuple[str, str, str]:
        """Step 1: ë°ì´í„°ì…‹ ì†ŒìŠ¤ì™€ ê²½ë¡œ íƒ€ì… í™•ì¸"""
        print("  [1/4] ë°ì´í„°ì…‹ ì†ŒìŠ¤ í™•ì¸ ì¤‘...")

        # ê²½ë¡œ íƒ€ì… í•´ì„ (s3 ë˜ëŠ” local)
        path_type, resolved_path = self.path_resolver.resolve(dataset_path)

        # ë°ì´í„°ì…‹ íƒ€ì… ê²°ì • (Factoryì—ì„œ ì „ë‹¬ ë˜ëŠ” ìë™ ê°ì§€)
        dataset_type = self.dataset_type or self._detect_dataset_type(dataset_path)

        print(f"      â†’ {dataset_type} ë°ì´í„°ì…‹, {path_type} ê²½ë¡œ")
        return dataset_type, path_type, resolved_path

    # ============= STEP 2: ë©”íƒ€ë°ì´í„° í™•ì¸ =============
    def step2_check_metadata(
        self, resolved_path: str, dataset_type: str, path_type: str
    ) -> dict:
        """Step 2: ë°ì´í„°ì…‹ ë©”íƒ€ë°ì´í„° ë° ê°€ìš©ì„± í™•ì¸"""
        print("  [2/4] ë©”íƒ€ë°ì´í„° í™•ì¸ ì¤‘...")

        metadata = {}

        # MBPPë‚˜ CodeContests ê°™ì€ í‘œì¤€ ë°ì´í„°ì…‹ì€ ë©”íƒ€ë°ì´í„° ì •ì˜
        if dataset_type == "mbpp":
            metadata = {
                "expected_fields": ["task_id", "text", "code", "test_list"],
                "format": "jsonl",
                "split_available": ["train", "validation", "test"],
            }
        elif dataset_type == "codecontests":
            metadata = {
                "expected_fields": ["name", "description", "solutions"],
                "format": "parquet",
                "split_available": ["train", "valid", "test"],
            }
        elif dataset_type == "humaneval":
            metadata = {
                "expected_fields": ["task_id", "prompt", "canonical_solution", "test"],
                "format": "jsonl",
                "split_available": ["test"],
            }
        else:
            # Custom ë°ì´í„°ì…‹ì€ ë©”íƒ€ë°ì´í„° íŒŒì¼ í™•ì¸
            metadata = self._load_custom_metadata(resolved_path, path_type)

        print(f"      â†’ í¬ë§·: {metadata.get('format', 'unknown')}")
        return metadata

    # ============= STEP 3: ë°ì´í„° ë¡œë“œ =============
    def step3_load_data(self, resolved_path: str, path_type: str) -> Dataset:
        """Step 3: ì‹¤ì œ ë°ì´í„° ë¡œë“œ"""
        print("  [3/4] ë°ì´í„° ë¡œë“œ ì¤‘...")

        if path_type == "s3":
            dataset = self._load_from_s3(resolved_path)
        else:
            dataset = self._load_from_local(resolved_path)

        # ìƒ˜í”Œ ìˆ˜ ì œí•œ
        if self.max_samples and len(dataset) > self.max_samples:
            dataset = dataset.select(range(self.max_samples))
            print(f"      â†’ {self.max_samples}ê°œ ìƒ˜í”Œë¡œ ì œí•œ")

        return dataset

    # ============= STEP 4: ì •ê·œí™” ë° ì „ì²˜ë¦¬ =============
    def step4_normalize_and_preprocess(
        self,
        raw_dataset: Dataset,
        dataset_type: str,
        metadata: dict,
    ) -> Dataset:
        """Step 4: ë°ì´í„° í¬ë§· ì •ê·œí™” ë° ì „ì²˜ë¦¬"""
        print("  [4/4] ë°ì´í„° ì •ê·œí™” ë° ì „ì²˜ë¦¬ ì¤‘...")

        # ë°ì´í„°ì…‹ë³„ ì •ê·œí™”
        if dataset_type == "mbpp":
            dataset = self._normalize_mbpp(raw_dataset)
        elif dataset_type == "codecontests":
            dataset = self._normalize_codecontests(raw_dataset)
        elif dataset_type == "humaneval":
            dataset = self._normalize_humaneval(raw_dataset)
        else:
            # Customì€ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            dataset = raw_dataset

        # ê³µí†µ í•„ë“œ í™•ì¸ ë° ì¶”ê°€
        dataset = self._ensure_common_fields(dataset)

        print(f"      â†’ ì •ê·œí™” ì™„ë£Œ: {len(dataset.column_names)} í•„ë“œ")
        return dataset

    # ============= ë°ì´í„°ì…‹ë³„ ì •ê·œí™” ë©”ì„œë“œ =============
    def _normalize_mbpp(self, dataset: Dataset) -> Dataset:
        """MBPP ë°ì´í„°ì…‹ ì •ê·œí™”"""

        def normalize_example(example):
            return {
                "instruction": example.get("text", ""),
                "input": "",  # MBPPëŠ” ë³„ë„ ì…ë ¥ ì—†ìŒ
                "output": example.get("code", ""),
                "task_id": example.get("task_id", ""),
                "test_cases": example.get("test_list", []),
            }

        return dataset.map(normalize_example)

    def _normalize_codecontests(self, dataset: Dataset) -> Dataset:
        """CodeContests ë°ì´í„°ì…‹ ì •ê·œí™”"""

        def normalize_example(example):
            # solutionsê°€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ì²« ë²ˆì§¸ ì„ íƒ
            solutions = example.get("solutions", {})
            if isinstance(solutions, dict):
                python_sols = solutions.get("python", [])
                code = python_sols[0] if python_sols else ""
            else:
                code = ""

            return {
                "instruction": example.get("description", ""),
                "input": example.get("public_tests", {}).get("input", [""])[0],
                "output": code,
                "task_id": example.get("name", ""),
                "test_cases": example.get("public_tests", {}),
            }

        return dataset.map(normalize_example)

    def _normalize_humaneval(self, dataset: Dataset) -> Dataset:
        """HumanEval ë°ì´í„°ì…‹ ì •ê·œí™”"""

        def normalize_example(example):
            return {
                "instruction": example.get("prompt", ""),
                "input": "",  # HumanEvalì€ ë³„ë„ ì…ë ¥ ì—†ìŒ
                "output": example.get("canonical_solution", ""),
                "task_id": example.get("task_id", ""),
                "test_cases": example.get("test", ""),
            }

        return dataset.map(normalize_example)

    def _ensure_common_fields(self, dataset: Dataset) -> Dataset:
        """ëª¨ë“  ë°ì´í„°ì…‹ì— ê³µí†µ í•„ë“œ ë³´ì¥"""
        required_fields = ["instruction", "input", "output"]

        for field in required_fields:
            if field not in dataset.column_names:
                dataset = dataset.add_column(field, [""] * len(dataset))

        return dataset

    # ============= ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œ =============
    def _detect_dataset_type(self, path: str) -> str:
        """ê²½ë¡œì—ì„œ ë°ì´í„°ì…‹ íƒ€ì… ìë™ ê°ì§€"""
        path_lower = path.lower()

        if "mbpp" in path_lower:
            return "mbpp"
        elif "codecontests" in path_lower or "contest" in path_lower:
            return "codecontests"
        elif "humaneval" in path_lower:
            return "humaneval"
        else:
            return "custom"

    def _load_from_local(self, path: str) -> Dataset:
        """ë¡œì»¬ì—ì„œ ë°ì´í„°ì…‹ ë¡œë“œ"""
        path_obj = Path(path)

        # HuggingFace datasets í˜•ì‹
        if path_obj.is_dir() and (path_obj / "dataset_info.json").exists():
            return load_from_disk(str(path_obj))[self.split]

        # Parquet íŒŒì¼
        if path_obj.suffix == ".parquet":
            return load_dataset("parquet", data_files=str(path_obj))[self.split]

        # JSONL/JSON íŒŒì¼
        if path_obj.suffix in [".jsonl", ".json"]:
            return load_dataset("json", data_files=str(path_obj))["train"]

        # ë””ë ‰í† ë¦¬ì¸ ê²½ìš° splitë³„ íŒŒì¼ ì°¾ê¸°
        if path_obj.is_dir():
            split_file = path_obj / f"{self.split}.jsonl"
            if split_file.exists():
                return load_dataset("json", data_files=str(split_file))["train"]

        raise ValueError(f"Cannot load dataset from {path}")

    def _load_from_s3(self, s3_path: str) -> Dataset:
        """S3ì—ì„œ ë°ì´í„°ì…‹ ë¡œë“œ"""
        if not self.s3_manager:
            raise RuntimeError("S3 manager not available")

        # S3ì—ì„œ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë¡œë“œ
        bucket, key = self.path_resolver.extract_bucket_and_key(s3_path)

        # íŒŒì¼ í˜•ì‹ì— ë”°ë¼ ë¡œë“œ
        if key.endswith(".parquet"):
            # ParquetëŠ” ì§ì ‘ ìŠ¤íŠ¸ë¦¬ë° ê°€ëŠ¥
            return load_dataset(
                "parquet",
                data_files=f"s3://{bucket}/{key}",
                split=self.split,
            )
        else:
            # JSONL/JSONì€ ë‹¤ìš´ë¡œë“œ í›„ ë¡œë“œ
            stream = self.s3_manager.stream_dataset(key)
            data = json.loads(stream.read())
            return Dataset.from_list(data)

    def _load_custom_metadata(self, path: str, path_type: str) -> dict:
        """Custom ë°ì´í„°ì…‹ì˜ ë©”íƒ€ë°ì´í„° ë¡œë“œ"""
        metadata = {
            "format": "custom",
            "expected_fields": [],
            "split_available": ["train"],
        }

        # metadata.json íŒŒì¼ì´ ìˆìœ¼ë©´ ë¡œë“œ
        if path_type == "local":
            metadata_file = Path(path) / "metadata.json"
            if metadata_file.exists():
                with open(metadata_file) as f:
                    metadata.update(json.load(f))

        return metadata
