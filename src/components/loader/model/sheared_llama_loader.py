"""
Princeton Sheared-LLaMA-2.7B ì°¸ì¡° ëª¨ë¸ ë¡œë”

êµ¬ì¡°ì  ê°€ì§€ì¹˜ê¸°(Structured Pruning)ë¡œ ê²½ëŸ‰í™”ëœ LLaMA ëª¨ë¸ ë¡œë”.
Sheared-LLaMAëŠ” ì›ë³¸ LLaMA-2-7Bì˜ 2.7B íŒŒë¼ë¯¸í„° ë²„ì „ìœ¼ë¡œ,
ë™ì¼í•œ SentencePiece tokenizer.model ì‚¬ìš©.

ì£¼ìš” íŠ¹ì§•:
- 2.7B íŒŒë¼ë¯¸í„°ë¡œ ë¹ ë¥¸ ì‹¤í—˜ ê°€ëŠ¥
- ì›ë³¸ ëŒ€ë¹„ 60% í¬ê¸°ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì 
- WMTP Rho1 ì•Œê³ ë¦¬ì¦˜ì˜ ì°¸ì¡° ëª¨ë¸ë¡œ í™œìš©
- Facebook MTPì™€ ë™ì¼í•œ í† í¬ë‚˜ì´ì €ë¡œ ì™„ë²½ í˜¸í™˜
"""

from __future__ import annotations

from pathlib import Path
from typing import Any

import torch

from src.components.loader.base_loader import ModelLoader
from src.components.registry import loader_registry


@loader_registry.register(
    "sheared-llama",
    category="loader",
    version="1.0.0",
    description="Princeton Sheared-LLaMA-2.7B reference model loader",
)
class ShearedLLaMALoader(ModelLoader):
    """
    Sheared-LLaMA ì°¸ì¡° ëª¨ë¸ ë¡œë”

    WMTP Rho1-WMTP ì•Œê³ ë¦¬ì¦˜ì˜ ì°¸ì¡° ëª¨ë¸:
    - Reference modelê³¼ base modelì˜ CE ì°¨ì´ ê³„ì‚°
    - |CE^ref_t - CE^base_t|ë¡œ ì–´ë ¤ìš´ í† í° ì‹ë³„
    - Facebook MTPì™€ ë™ì¼í•œ SentencePiece í† í¬ë‚˜ì´ì € ì‚¬ìš©

    ëª¨ë¸ êµ¬ì¡°:
    - Base: LLaMA-2-7Bë¥¼ 2.7Bë¡œ ê°€ì§€ì¹˜ê¸°
    - Method: Structured pruning (êµ¬ì¡°ì  ê°€ì§€ì¹˜ê¸°)
    - Context: 4096 í† í° (ì›ë³¸ê³¼ ë™ì¼)
    - Tokenizer: SentencePiece (SHA256: 9e556afd...)

    ë°±ì—… ì˜µì…˜:
    ë” ê°•í•œ ì°¸ì¡° ëª¨ë¸ì´ í•„ìš”í•œ ê²½ìš° ì‚¬ìš©
    """

    def __init__(self, config: dict[str, Any] | None = None):
        """
        ì´ˆê¸°í™”

        Args:
            config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
                - device: ì‹¤í–‰ ë””ë°”ì´ìŠ¤ (cuda/cpu/mps)
                - use_4bit: 4ë¹„íŠ¸ ì–‘ìí™” ì—¬ë¶€
                - use_8bit: 8ë¹„íŠ¸ ì–‘ìí™” ì—¬ë¶€
                - cache_dir: ëª¨ë¸ ìºì‹œ ë””ë ‰í† ë¦¬
        """
        super().__init__(config or {})

        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        self.device = config.get("device", "auto") if config else "auto"

        # ì–‘ìí™” ì˜µì…˜ (ì‘ì€ ëª¨ë¸ì´ë¯€ë¡œ ê¸°ë³¸ì ìœ¼ë¡œ ë¹„í™œì„±í™”)
        self.use_4bit = config.get("use_4bit", False) if config else False
        self.use_8bit = config.get("use_8bit", False) if config else False

        # ëª¨ë¸ ID
        self.model_id = "princeton-nlp/Sheared-LLaMA-2.7B"

    def load_model(self, path: str, **kwargs) -> Any:
        """
        Sheared-LLaMA ëª¨ë¸ ë¡œë“œ

        2.7B ê²½ëŸ‰ ëª¨ë¸ë¡œ ë¹ ë¥¸ ì¶”ë¡ ê³¼ ì‹¤í—˜ ì§€ì›:
        - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: ~11GB (FP32) / ~5.5GB (BF16)
        - ì¶”ë¡  ì†ë„: 7B ëª¨ë¸ ëŒ€ë¹„ 2.5x ë¹ ë¦„
        - ì„±ëŠ¥: ë§ì€ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì›ë³¸ 7Bì˜ 95% ì„±ëŠ¥ ìœ ì§€

        Args:
            path: ëª¨ë¸ ê²½ë¡œ ë˜ëŠ” ìºì‹œ ë””ë ‰í† ë¦¬
            **kwargs: ì¶”ê°€ ì˜µì…˜
                - force_download: ê°•ì œ ë‹¤ìš´ë¡œë“œ ì—¬ë¶€
                - local_files_only: ë¡œì»¬ íŒŒì¼ë§Œ ì‚¬ìš©
                - revision: ëª¨ë¸ ë²„ì „/ë¸Œëœì¹˜

        Returns:
            ë¡œë“œëœ Sheared-LLaMA ëª¨ë¸

        Raises:
            ImportError: transformers íŒ¨í‚¤ì§€ ë¯¸ì„¤ì¹˜
            RuntimeError: ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨
        """
        try:
            from transformers import AutoModelForCausalLM
        except ImportError:
            raise ImportError(
                "transformers íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤. " "ì„¤ì¹˜: uv pip install transformers"
            )

        # ë¡œì»¬ ê²½ë¡œ í™•ì¸
        local_path = Path(path)
        cache_dir = str(local_path) if local_path.exists() else None

        # ì–‘ìí™” ì„¤ì • (ì‘ì€ ëª¨ë¸ì´ë¯€ë¡œ ì„ íƒì )
        model_kwargs = {}
        if self.use_4bit or self.use_8bit:
            from transformers import BitsAndBytesConfig

            if self.use_4bit:
                model_kwargs["quantization_config"] = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.bfloat16,
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4",
                )
                print("ğŸ“¦ 4ë¹„íŠ¸ ì–‘ìí™” í™œì„±í™” (ë©”ëª¨ë¦¬ ~1.4GB)")
            elif self.use_8bit:
                model_kwargs["quantization_config"] = BitsAndBytesConfig(
                    load_in_8bit=True
                )
                print("ğŸ“¦ 8ë¹„íŠ¸ ì–‘ìí™” í™œì„±í™” (ë©”ëª¨ë¦¬ ~2.7GB)")

        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        if self.device == "auto":
            model_kwargs["device_map"] = "auto"
        elif self.device != "cpu":
            model_kwargs["device_map"] = {"": self.device}

        # ëª¨ë¸ ë¡œë“œ
        try:
            print(f"ğŸš€ Sheared-LLaMA ëª¨ë¸ ë¡œë“œ ì¤‘: {self.model_id}")

            model = AutoModelForCausalLM.from_pretrained(
                self.model_id,
                cache_dir=cache_dir,
                torch_dtype=torch.bfloat16
                if not (self.use_4bit or self.use_8bit)
                else None,
                trust_remote_code=False,  # ê³µì‹ transformers ì½”ë“œë§Œ ì‚¬ìš©
                **model_kwargs,
                **kwargs,
            )

            # í‰ê°€ ëª¨ë“œ ì„¤ì •
            model.eval()

            # ëª¨ë¸ ì •ë³´ ì¶œë ¥
            param_count = sum(p.numel() for p in model.parameters())
            print("âœ… Sheared-LLaMA ë¡œë“œ ì™„ë£Œ")
            print(f"   - íŒŒë¼ë¯¸í„°: {param_count:,} ({param_count / 1e9:.1f}B)")
            print(f"   - ë””ë°”ì´ìŠ¤: {next(model.parameters()).device}")
            print("   - ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: 4096 í† í°")
            print("   - ìš©ë„: WMTP Rho1 ì°¸ì¡° ëª¨ë¸")

            return model

        except Exception as e:
            raise RuntimeError(
                f"Sheared-LLaMA ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {self.model_id}\n" f"ì—ëŸ¬: {e}"
            )

    # load_tokenizerëŠ” ë¶€ëª¨ í´ë˜ìŠ¤ì˜ í†µí•© SentencePiece ë©”ì„œë“œ ìƒì†
    # Sheared-LLaMAëŠ” ì›ë³¸ LLaMA-2ì™€ ë™ì¼í•œ tokenizer.model ì‚¬ìš©
