"""
CPU ì „ìš© Facebook MTP ëª¨ë¸ ë¡œë” - ë©”ëª¨ë¦¬ ìµœì í™” ë²„ì „

M3 MacBook Pro 64GB RAM í™˜ê²½ì—ì„œ 27GB Facebook MTP ëª¨ë¸ì„
ì•ˆì „í•˜ê²Œ ë¡œë“œí•˜ê¸° ìœ„í•œ ìµœì í™”ëœ ë¡œë”ì…ë‹ˆë‹¤.

ì£¼ìš” ìµœì í™”:
- Fairscale model parallel ì™„ì „ ì œê±°
- torch.distributed ì´ˆê¸°í™” ìƒëµ
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì  torch.load ì‚¬ìš©
- ë¶ˆí•„ìš”í•œ ìºì‹œ ë° ë©”íƒ€ë°ì´í„° ìµœì†Œí™”
"""

from __future__ import annotations

import json
from pathlib import Path
from typing import Any

import torch

from src.components.loader.base_loader import ModelLoader
from src.components.registry import loader_registry


def detect_optimal_device() -> str:
    """CPU ì „ìš© ë””ë°”ì´ìŠ¤ ê°ì§€."""
    return "cpu"


@loader_registry.register(
    "mtp-native-cpu",
    version="1.0.0",
    description="CPU-optimized Facebook MTP native loader",
)
class MTPNativeCPULoader(ModelLoader):
    """
    CPU ì „ìš© Facebook MTP ëª¨ë¸ ë¡œë” - ë©”ëª¨ë¦¬ ìµœì í™” ë²„ì „

    M3 MacBook Pro 64GBì—ì„œ 27GB Facebook MTP ëª¨ë¸ì„ ì•ˆì „í•˜ê²Œ ë¡œë“œí•©ë‹ˆë‹¤.
    fairscaleê³¼ torch.distributedë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì•„ ë©”ëª¨ë¦¬ ì˜¤ë²„í—¤ë“œë¥¼ ìµœì†Œí™”í•©ë‹ˆë‹¤.
    """

    def __init__(self, config: dict[str, Any] | None = None):
        super().__init__(config or {})

        # CPU ì „ìš© ê°•ì œ ì„¤ì •
        self.device = "cpu"
        self.num_heads = 4

        print("MTPNativeCPULoader initialized - CPU ONLY mode for memory efficiency")

    def _load_native_checkpoint(self, path: Path) -> dict[str, Any]:
        """ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë”©."""
        print(f"ğŸ”§ Loading checkpoint from {path} with memory optimization...")

        # í†µí•© ì²´í¬í¬ì¸íŠ¸ ìš°ì„  í™•ì¸
        consolidated_path = path / "consolidated.pth"
        if consolidated_path.exists():
            print(f"ğŸ“¦ Loading consolidated checkpoint: {consolidated_path}")
            print(
                f"ğŸ“Š File size: {consolidated_path.stat().st_size / (1024**3):.1f} GB"
            )

            # ë©”ëª¨ë¦¬ ìµœì í™”ëœ ë¡œë”©
            checkpoint = torch.load(
                consolidated_path,
                map_location=self.device,
                weights_only=False,  # Facebook ëª¨ë¸ì€ ë³µì¡í•œ ê°ì²´ êµ¬ì¡° ì‚¬ìš©
            )
            print("âœ… Checkpoint loaded successfully")
            return checkpoint

        # ë¶„í•  ì²´í¬í¬ì¸íŠ¸ í™•ì¸
        checkpoint_files = list(path.glob("consolidated.*.pth"))
        if checkpoint_files:
            print(f"ğŸ“¦ Loading split checkpoints: {len(checkpoint_files)} files")
            merged_checkpoint = {}
            for i, ckpt_file in enumerate(sorted(checkpoint_files)):
                print(f"  Loading part {i+1}/{len(checkpoint_files)}: {ckpt_file.name}")
                ckpt = torch.load(
                    ckpt_file, map_location=self.device, weights_only=False
                )
                merged_checkpoint.update(ckpt)
            print("âœ… Split checkpoints merged successfully")
            return merged_checkpoint

        raise FileNotFoundError(f"No checkpoint files found in {path}")

    def _load_model_params(self, path: Path) -> dict[str, Any]:
        """ëª¨ë¸ íŒŒë¼ë¯¸í„° ë¡œë”©."""
        params_path = path / "params.json"
        if params_path.exists():
            with open(params_path) as f:
                params = json.load(f)
                print(
                    f"ğŸ“‹ Model params loaded: {params.get('dim', 'unknown')}D, {params.get('n_layers', 'unknown')} layers"
                )
                return params

        # ê¸°ë³¸ íŒŒë¼ë¯¸í„° (Facebook MTP 7B ê¸°ì¤€)
        default_params = {
            "dim": 4096,
            "n_layers": 32,
            "n_heads": 32,
            "n_kv_heads": 8,
            "vocab_size": 128256,
            "multiple_of": 4096,
            "ffn_dim_multiplier": 1.3,
            "norm_eps": 1e-5,
            "rope_theta": 500000,
            "mtp_heads": self.num_heads,
            "max_batch_size": 1,  # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ì‘ê²Œ ì„¤ì •
            "max_seq_len": 2048,
        }
        print("âš ï¸  Using default parameters for 7B MTP model")
        return default_params

    def _create_mtp_model_cpu_only(
        self, checkpoint: dict[str, Any], params: dict[str, Any]
    ) -> Any:
        """CPU ì „ìš© MTP ëª¨ë¸ ìƒì„± - fairscale ì—†ì´."""
        print("ğŸ—ï¸  Creating CPU-only MTP model...")

        import sys
        from pathlib import Path

        # Facebook MTP ëª¨ë¸ ë””ë ‰í† ë¦¬
        model_dir = Path.cwd() / "models" / "7b_1t_4"
        if not model_dir.exists():
            raise ImportError(
                f"Facebook MTP ëª¨ë¸ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_dir}"
            )

        try:
            # sys.pathì— ëª¨ë¸ ë””ë ‰í† ë¦¬ ì¶”ê°€
            sys.path.insert(0, str(model_dir))

            # âš ï¸ CRITICAL: ìµœì†Œí•œì˜ fairscale ì´ˆê¸°í™” (ë©”ëª¨ë¦¬ íš¨ìœ¨ ëª¨ë“œ)
            print("ğŸ”§ Minimal fairscale initialization for CPU compatibility")

            # fairscale í•„ìˆ˜ ì´ˆê¸°í™” (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
            import fairscale.nn.model_parallel.initialize as fs_init
            import torch.distributed as dist

            # torch.distributed ìµœì†Œ ì´ˆê¸°í™” (CPU ì „ìš©)
            if not dist.is_initialized():
                dist.init_process_group(
                    backend="gloo",  # CPU ì „ìš©
                    init_method="tcp://localhost:29501",  # í¬íŠ¸ ë³€ê²½ìœ¼ë¡œ ì¶©ëŒ ë°©ì§€
                    rank=0,
                    world_size=1,  # ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤
                )
                print("âœ… torch.distributed initialized (CPU gloo backend)")

            # fairscale model parallel ìµœì†Œ ì´ˆê¸°í™”
            if not fs_init.model_parallel_is_initialized():
                fs_init.initialize_model_parallel(1)  # ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤
                print("âœ… fairscale model parallel initialized (single process)")

            # Facebook MTP ëª¨ë¸ í´ë˜ìŠ¤ë§Œ import
            from llama.model import ModelArgs
            from llama.model import Transformer as MTPLlamaModel

            # ModelArgs ìƒì„± - ë©”ëª¨ë¦¬ ì ˆì•½ ì„¤ì •
            model_args = ModelArgs(
                dim=params.get("dim", 4096),
                n_layers=params.get("n_layers", 32),
                n_heads=params.get("n_heads", 32),
                n_kv_heads=params.get("n_kv_heads", 8),
                vocab_size=params.get("vocab_size", 128256),
                multiple_of=params.get("multiple_of", 4096),
                ffn_dim_multiplier=params.get("ffn_dim_multiplier", 1.3),
                norm_eps=params.get("norm_eps", 1e-5),
                rope_theta=params.get("rope_theta", 500000),
                n_future_tokens=params.get("mtp_heads", self.num_heads),
                max_batch_size=1,  # ë©”ëª¨ë¦¬ ì ˆì•½
                max_seq_len=512,  # ë©”ëª¨ë¦¬ ì ˆì•½
            )

            print(
                f"ğŸ“ Model architecture: {model_args.dim}D, {model_args.n_layers} layers, {model_args.n_future_tokens} MTP heads"
            )

            # MTP ëª¨ë¸ ìƒì„± (fairscale ì—†ì´)
            model = MTPLlamaModel(model_args)
            print("âœ… Model structure created")

            # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ê°€ì¤‘ì¹˜ ë¡œë“œ
            print("ğŸ”„ Loading weights from checkpoint...")
            missing_keys, unexpected_keys = model.load_state_dict(
                checkpoint, strict=False
            )

            if missing_keys:
                print(
                    f"âš ï¸  Missing keys: {len(missing_keys)} (this may be normal for MTP models)"
                )
            if unexpected_keys:
                print(
                    f"âš ï¸  Unexpected keys: {len(unexpected_keys)} (this may be normal)"
                )

            # CPUë¡œ ì´ë™ ë° evaluation ëª¨ë“œ
            model = model.to(self.device)
            model.eval()

            print("âœ… Facebook MTP model loaded successfully on CPU")
            print(f"ğŸ“ Device: {next(model.parameters()).device}")

            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬
            param_count = sum(p.numel() for p in model.parameters())
            print(
                f"ğŸ“Š Total parameters: {param_count:,} ({param_count * 4 / (1024**3):.1f} GB in FP32)"
            )

            return model

        except ImportError as e:
            raise ImportError(f"Facebook MTP ëª¨ë¸ import ì‹¤íŒ¨: {e}")
        except Exception as e:
            raise RuntimeError(f"CPU-only MTP ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
        finally:
            # sys.path ì •ë¦¬
            if str(model_dir) in sys.path:
                sys.path.remove(str(model_dir))

    def load_model(self, path: str, **kwargs) -> Any:
        """CPU ìµœì í™”ëœ ëª¨ë¸ ë¡œë”©."""
        print("ğŸš€ Starting CPU-optimized Facebook MTP model loading...")

        local_path = Path(path)

        if not local_path.is_dir():
            raise ValueError(
                f"MTP native models should be in a directory: {local_path}"
            )

        # ì²´í¬í¬ì¸íŠ¸ì™€ íŒŒë¼ë¯¸í„° ë¡œë”©
        checkpoint = self._load_native_checkpoint(local_path)
        params = self._load_model_params(local_path)

        # CPU ì „ìš© ëª¨ë¸ ìƒì„±
        model = self._create_mtp_model_cpu_only(checkpoint, params)

        print("ğŸ‰ Model loading completed successfully!")
        return model

    def load_tokenizer(self, path: str, **kwargs) -> Any:
        """í† í¬ë‚˜ì´ì € ë¡œë”© (ê¸°ì¡´ê³¼ ë™ì¼)."""
        local_path = Path(path)

        # SentencePiece í† í¬ë‚˜ì´ì € ì‹œë„
        tokenizer_path = (
            local_path / "tokenizer.model" if local_path.is_dir() else local_path
        )

        if tokenizer_path.exists():
            try:
                from sentencepiece import SentencePieceProcessor

                tokenizer = SentencePieceProcessor(model_file=str(tokenizer_path))
                print("âœ… SentencePiece tokenizer loaded")
                return tokenizer
            except ImportError:
                pass

        # tiktoken ì‹œë„
        try:
            import tiktoken

            tokenizer = tiktoken.get_encoding("cl100k_base")
            print("âœ… Tiktoken tokenizer loaded")
            return tokenizer
        except ImportError:
            pass

        # HuggingFace tokenizer fallback
        try:
            from transformers import AutoTokenizer

            tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")
            print("âœ… HuggingFace tokenizer loaded")
            return tokenizer
        except Exception:
            pass

        raise RuntimeError(f"Could not load tokenizer from {path}")

    def run(self, ctx: dict[str, Any]) -> dict[str, Any]:
        """CPU ìµœì í™”ëœ ì‹¤í–‰."""
        model_path = ctx.get("model_path")
        if not model_path:
            raise ValueError("model_path is required in context")

        result = {}

        # ëª¨ë¸ ë¡œë”©
        result["model"] = self.load_model(model_path)

        # í† í¬ë‚˜ì´ì € ë¡œë”© ì‹œë„
        try:
            result["tokenizer"] = self.load_tokenizer(model_path)
        except Exception as e:
            print(f"âš ï¸  Could not load tokenizer: {e}")

        return result
