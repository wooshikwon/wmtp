"""
Berkeley Starling-RM-7B-alpha Reward Model ë¡œë”

WMTP Critic ì•Œê³ ë¦¬ì¦˜ì˜ í•µì‹¬ ì»´í¬ë„ŒíŠ¸ì¸ Reward Model ë¡œë”.
Starling-RMì€ Llama-2-7b-chat ê¸°ë°˜ìœ¼ë¡œ ë™ì¼í•œ SentencePiece tokenizer.model ì‚¬ìš©.

ì£¼ìš” íŠ¹ì§•:
- 7B íŒŒë¼ë¯¸í„° Reward Model
- Sequence classification headë¡œ ë³´ìƒ ì ìˆ˜ ì˜ˆì¸¡
- WMTPì˜ í† í°ë³„ ê°€ì¤‘ì¹˜ ê³„ì‚°ì— í™œìš©
- Facebook MTPì™€ ë™ì¼í•œ í† í¬ë‚˜ì´ì €ë¡œ ì™„ë²½ í˜¸í™˜
"""

from __future__ import annotations

from pathlib import Path
from typing import Any

import torch

from src.components.loader.base_loader import ModelLoader
from src.components.registry import loader_registry


@loader_registry.register(
    "starling-rm",
    category="loader",
    version="1.0.0",
    description="Berkeley Starling-RM-7B-alpha reward model loader",
)
class StarlingRMLoader(ModelLoader):
    """
    Starling Reward Model ë¡œë”

    WMTP Critic-WMTP ì•Œê³ ë¦¬ì¦˜ì˜ í•µì‹¬ RM ëª¨ë¸:
    - Value Function ê¸°ë°˜ í† í° ê°€ì¤‘ì¹˜ ê³„ì‚°
    - Î´_t = V_t - V_{t-1} ì°¨ë¶„ê°’ìœ¼ë¡œ ì¤‘ìš”ë„ ì¸¡ì •
    - Facebook MTPì™€ ë™ì¼í•œ SentencePiece í† í¬ë‚˜ì´ì € ì‚¬ìš©

    ëª¨ë¸ êµ¬ì¡°:
    - Base: Llama-2-7b-chat
    - Head: Reward modeling head (sequence classification)
    - Tokenizer: SentencePiece (SHA256: 9e556afd...)
    """

    def __init__(self, config: dict[str, Any] | None = None):
        """
        ì´ˆê¸°í™”

        Args:
            config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
                - device: ì‹¤í–‰ ë””ë°”ì´ìŠ¤ (cuda/cpu/mps)
                - use_4bit: 4ë¹„íŠ¸ ì–‘ìí™” ì—¬ë¶€
                - use_8bit: 8ë¹„íŠ¸ ì–‘ìí™” ì—¬ë¶€
                - cache_dir: ëª¨ë¸ ìºì‹œ ë””ë ‰í† ë¦¬
        """
        super().__init__(config or {})

        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        self.device = config.get("device", "auto") if config else "auto"

        # ì–‘ìí™” ì˜µì…˜
        self.use_4bit = config.get("use_4bit", False) if config else False
        self.use_8bit = config.get("use_8bit", False) if config else False

        # ëª¨ë¸ ID
        self.model_id = "berkeley-nest/Starling-RM-7B-alpha"

    def load_model(self, path: str, **kwargs) -> Any:
        """
        Starling-RM ëª¨ë¸ ë¡œë“œ

        Reward Modelì€ ì‹œí€€ìŠ¤ ë¶„ë¥˜ íƒœìŠ¤í¬ìš© í—¤ë“œë¥¼ í¬í•¨:
        - ì…ë ¥: í”„ë¡¬í”„íŠ¸ + ì‘ë‹µ
        - ì¶œë ¥: ìŠ¤ì¹¼ë¼ ë³´ìƒ ì ìˆ˜

        Args:
            path: ëª¨ë¸ ê²½ë¡œ ë˜ëŠ” ìºì‹œ ë””ë ‰í† ë¦¬
            **kwargs: ì¶”ê°€ ì˜µì…˜
                - force_download: ê°•ì œ ë‹¤ìš´ë¡œë“œ ì—¬ë¶€
                - local_files_only: ë¡œì»¬ íŒŒì¼ë§Œ ì‚¬ìš©

        Returns:
            ë¡œë“œëœ Starling-RM ëª¨ë¸

        Raises:
            ImportError: transformers íŒ¨í‚¤ì§€ ë¯¸ì„¤ì¹˜
            RuntimeError: ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨
        """
        try:
            from transformers import AutoModelForSequenceClassification
        except ImportError:
            raise ImportError(
                "transformers íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤. " "ì„¤ì¹˜: uv pip install transformers"
            )

        # ë¡œì»¬ ê²½ë¡œ í™•ì¸
        local_path = Path(path)
        cache_dir = str(local_path) if local_path.exists() else None

        # ì–‘ìí™” ì„¤ì •
        model_kwargs = {}
        if self.use_4bit or self.use_8bit:
            from transformers import BitsAndBytesConfig

            if self.use_4bit:
                model_kwargs["quantization_config"] = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.bfloat16,
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4",
                )
                print("ğŸ“¦ 4ë¹„íŠ¸ ì–‘ìí™” í™œì„±í™” (NF4)")
            elif self.use_8bit:
                model_kwargs["quantization_config"] = BitsAndBytesConfig(
                    load_in_8bit=True
                )
                print("ğŸ“¦ 8ë¹„íŠ¸ ì–‘ìí™” í™œì„±í™”")

        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        if self.device == "auto":
            model_kwargs["device_map"] = "auto"
        elif self.device != "cpu":
            model_kwargs["device_map"] = {"": self.device}

        # ëª¨ë¸ ë¡œë“œ
        try:
            print(f"ğŸš€ Starling-RM ëª¨ë¸ ë¡œë“œ ì¤‘: {self.model_id}")

            model = AutoModelForSequenceClassification.from_pretrained(
                self.model_id,
                cache_dir=cache_dir,
                torch_dtype=torch.bfloat16
                if not (self.use_4bit or self.use_8bit)
                else None,
                trust_remote_code=True,  # ì»¤ìŠ¤í…€ ì½”ë“œ í—ˆìš©
                **model_kwargs,
                **kwargs,
            )

            # í‰ê°€ ëª¨ë“œ ì„¤ì •
            model.eval()

            # ëª¨ë¸ ì •ë³´ ì¶œë ¥
            param_count = sum(p.numel() for p in model.parameters())
            print("âœ… Starling-RM ë¡œë“œ ì™„ë£Œ")
            print(f"   - íŒŒë¼ë¯¸í„°: {param_count:,} ({param_count / 1e9:.1f}B)")
            print(f"   - ë””ë°”ì´ìŠ¤: {next(model.parameters()).device}")
            print("   - Reward head ì¶œë ¥: ìŠ¤ì¹¼ë¼ ë³´ìƒ ì ìˆ˜")

            return model

        except Exception as e:
            raise RuntimeError(
                f"Starling-RM ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {self.model_id}\n" f"ì—ëŸ¬: {e}"
            )

    # load_tokenizerëŠ” ë¶€ëª¨ í´ë˜ìŠ¤ì˜ í†µí•© SentencePiece ë©”ì„œë“œ ìƒì†
    # Starling-RMì€ Llama-2 ê¸°ë°˜ìœ¼ë¡œ ë™ì¼í•œ tokenizer.model ì‚¬ìš©
