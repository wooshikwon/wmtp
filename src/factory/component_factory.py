"""WMTP ì»´í¬ë„ŒíŠ¸ íŒ©í† ë¦¬ - ì„¤ì • ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ ì»´í¬ë„ŒíŠ¸ ìƒì„±.

ì—°êµ¬ ì² í•™ ì§€ì›: "Not All Tokens Are What You Need"
===============================================

ì´ íŒ©í† ë¦¬ëŠ” WMTPì˜ í•µì‹¬ ì„¤ê³„ ì² í•™ì„ êµ¬í˜„í•©ë‹ˆë‹¤:
ë™ì¼í•œ íŒŒì´í”„ë¼ì¸ êµ¬ì¡°ì—ì„œ ì„œë¡œ ë‹¤ë¥¸ ì»´í¬ë„ŒíŠ¸ ì¡°í•©ì„ í†µí•´
ì„¸ ê°€ì§€ ì•Œê³ ë¦¬ì¦˜(mtp-baseline, critic-wmtp, rho1-wmtp)ì„ ì§€ì›í•©ë‹ˆë‹¤.

íŒ©í† ë¦¬ íŒ¨í„´ì˜ ì¥ì :
  1. ì„¤ì • ê¸°ë°˜ ìƒì„±: YAML ì„¤ì •ì—ì„œ ìë™ìœ¼ë¡œ ì í•©í•œ ì»´í¬ë„ŒíŠ¸ ì„ íƒ
  2. ì•Œê³ ë¦¬ì¦˜ ë¶„ë¦¬: ê° ì•Œê³ ë¦¬ì¦˜ì˜ íŠ¹í™”ëœ ë¡œì§ì„ ì»´í¬ë„ŒíŠ¸ ë‚´ë¶€ì— ìº¡ìŠí™”
  3. í™•ì¥ì„±: ìƒˆë¡œìš´ ì•Œê³ ë¦¬ì¦˜ ì¶”ê°€ ì‹œ Registryì—ë§Œ ë“±ë¡í•˜ë©´ ë¨
  4. ì¼ê´€ì„±: ëª¨ë“  ì•Œê³ ë¦¬ì¦˜ì´ ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤ ì‚¬ìš©

ì»´í¬ë„ŒíŠ¸ ì¡°í•© ì „ëµ:
  - mtp-baseline: BaselineMtpTrainer â†’ ê· ë“± ê°€ì¤‘ì¹˜
  - critic-wmtp: CriticWmtpTrainer â†’ Value Head ì§ì ‘ í†µí•©
  - rho1-wmtp: Rho1WmtpTrainer â†’ Reference Model ì°¨ì´ ê³„ì‚°

ì´ë¥¼ í†µí•´ ì—°êµ¬ìëŠ” ì•Œê³ ë¦¬ì¦˜ ê°„ ê³µì •í•œ ì„±ëŠ¥ ë¹„êµê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.
"""

from typing import Any  # ë²”ìš© íƒ€ì… íŒíŠ¸

# WMTP ì»´í¬ë„ŒíŠ¸ ë² ì´ìŠ¤ í´ë˜ìŠ¤ë“¤ - ëª¨ë“  êµ¬í˜„ì²´ê°€ ìƒì†ë°›ëŠ” ì¶”ìƒ ì¸í„°í˜ì´ìŠ¤
from src.components.base import (
    Evaluator,  # í‰ê°€ ìˆ˜í–‰ ì¸í„°í˜ì´ìŠ¤ (HumanEval, MBPP ë“±)
    Loader,  # ë°ì´í„°/ëª¨ë¸ ë¡œë”© ì¸í„°í˜ì´ìŠ¤
    Optimizer,  # ìµœì í™”ê¸° ì¸í„°í˜ì´ìŠ¤ (AdamW, Lion ë“±)
    # Scorer ì œê±°ë¨ (v2.1.0) - ëª¨ë“  ë¡œì§ì´ Trainerë¡œ í†µí•©
    Trainer,  # í›ˆë ¨ ì‹¤í–‰ ì¸í„°í˜ì´ìŠ¤ (WMTP í†µí•© íŠ¸ë ˆì´ë„ˆ)
)

# ê° ì»´í¬ë„ŒíŠ¸ íƒ€ì…ë³„ Registry - êµ¬í˜„ì²´ë“¤ì„ í‚¤ë¡œ ë“±ë¡/ì¡°íšŒí•˜ëŠ” ì €ì¥ì†Œ
from src.components.registry import (
    evaluator_registry,  # í‰ê°€ê¸° êµ¬í˜„ì²´ë“¤ (meta-mtp, mbpp-v1 ë“±)
    loader_registry,  # ë¡œë” êµ¬í˜„ì²´ë“¤ (hf-model, mtp-native ë“±)
    optimizer_registry,  # ì˜µí‹°ë§ˆì´ì € êµ¬í˜„ì²´ë“¤ (adamw-bf16-fused ë“±)
    tokenizer_registry,  # í† í¬ë‚˜ì´ì € êµ¬í˜„ì²´ë“¤ (unified-sentencepiece ë“±)
    trainer_registry,  # íŠ¸ë ˆì´ë„ˆ êµ¬í˜„ì²´ë“¤ (mtp-weighted-ce-trainer ë“±)
)
from src.settings import Config, Recipe  # Pydantic ì„¤ì • ëª¨ë¸ë“¤

MTP_CONFIG = {
    "n_heads": 4,  # Meta ë…¼ë¬¸ ê¸°ì¤€ ìµœì ê°’
    "horizon": 4,  # ì˜ˆì¸¡ ë²”ìœ„ (t+1, t+2, t+3, t+4)
}


class ComponentFactory:
    """WMTP ì•Œê³ ë¦¬ì¦˜ë³„ ì»´í¬ë„ŒíŠ¸ ìƒì„± íŒ©í† ë¦¬.

    ì—°êµ¬ ì² í•™ "Not All Tokens Are What You Need" êµ¬í˜„ì˜ í•µì‹¬:
        ì´ í´ë˜ìŠ¤ëŠ” ì„¤ì • íŒŒì¼(recipe.yaml)ì˜ ì•Œê³ ë¦¬ì¦˜ ì„ íƒì— ë”°ë¼
        ì í•©í•œ ì»´í¬ë„ŒíŠ¸ ì¡°í•©ì„ ìë™ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.

        ê° WMTP ì•Œê³ ë¦¬ì¦˜ë§ˆë‹¤ ë…ë¦½ëœ íŠ¸ë ˆì´ë„ˆ í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
        - BaselineMtpTrainer: ê· ë“± ê°€ì¤‘ì¹˜
        - CriticWmtpTrainer: Critic ê¸°ë°˜ ê°€ì¤‘ì¹˜
        - Rho1WmtpTrainer: Reference ëª¨ë¸ ê¸°ë°˜ ê°€ì¤‘ì¹˜

    ì„¤ê³„ ì›ì¹™:
        1. í•˜ë“œì½”ë”© ë°©ì§€: ëª¨ë“  ë§¤í•‘ ì •ë³´ë¥¼ í´ë˜ìŠ¤ ìƒìˆ˜ë¡œ ê´€ë¦¬
        2. Registry íŒ¨í„´: ì‹¤ì œ êµ¬í˜„ì²´ëŠ” ë³„ë„ Registryì—ì„œ ì¡°íšŒ
        3. ì„¤ì • ì£¼ë„: recipe.yamlì˜ ê°’ì´ ì»´í¬ë„ŒíŠ¸ ì„ íƒì„ ê²°ì •
        4. ì˜¤ë¥˜ ì²˜ë¦¬: ì˜ëª»ëœ ì„¤ì •ì— ëŒ€í•œ ëª…í™•í•œ ì—ëŸ¬ ë©”ì‹œì§€ ì œê³µ
    """

    # ğŸ¯ ì§ì ‘ í˜¸ì¶œ ë°©ì‹: YAML í‚¤ê°€ ê³§ Registry í‚¤
    # ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ ì œê±° - Pydantic ìŠ¤í‚¤ë§ˆì™€ Registry í‚¤ ì™„ì „ ì¼ì¹˜

    # create_scorer ë©”ì„œë“œëŠ” v2.1.0ë¶€í„° ì œê±°ë¨
    # ëª¨ë“  scorer ë¡œì§ì´ ê°ê°ì˜ Trainer í´ë˜ìŠ¤ë¡œ í†µí•©ë˜ì—ˆìŠµë‹ˆë‹¤.
    # - BaselineMtpTrainer: ê· ë“± ê°€ì¤‘ì¹˜ (scorer ë¶ˆí•„ìš”)
    # - CriticWmtpTrainer: Value Head ì§ì ‘ ê´€ë¦¬
    # - Rho1WmtpTrainer: Reference Model ì°¨ì´ ì§ì ‘ ê³„ì‚°

    @staticmethod
    def create_trainer(recipe: Recipe, config: Config) -> Trainer:
        """íŠ¸ë ˆì´ë„ˆ ìƒì„± - recipe/configë§Œ ì‚¬ìš©, scorer ì˜ì¡´ì„± ìë™ ê´€ë¦¬.

        WMTP ì„¤ê³„ì˜ ìš°ì•„í•¨: "One Trainer, Multiple Scorers"
            ì´ ë©”ì„œë“œëŠ” WMTPì˜ í•µì‹¬ ì„¤ê³„ ì² í•™ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
            Phase 2 ë¦¬íŒ©í† ë§ìœ¼ë¡œ ê° ì•Œê³ ë¦¬ì¦˜ë§ˆë‹¤ ë…ë¦½ëœ íŠ¸ë ˆì´ë„ˆë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
            ê³µí†µ ë¡œì§ì€ BaseWmtpTrainerì— ì¶”ìƒí™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

            ì´ í†µí•© ì ‘ê·¼ë²•ì˜ ì¥ì :
            1. ê³µì •í•œ ë¹„êµ: ì•Œê³ ë¦¬ì¦˜ ê°„ ì°¨ì´ëŠ” ì˜¤ì§ ê°€ì¤‘ì¹˜ ê³„ì‚° ë°©ì‹
            2. ì½”ë“œ ì¤‘ë³µ ì œê±°: í›ˆë ¨ ë¡œì§ì€ í•œ ê³³ì—ë§Œ êµ¬í˜„
            3. ìœ ì§€ë³´ìˆ˜ì„±: ìƒˆ ì•Œê³ ë¦¬ì¦˜ ì¶”ê°€ì‹œ Scorerë§Œ ê°œë°œ
            4. ë²„ê·¸ ìµœì†Œí™”: ê³µí†µ ë¡œì§ì€ í•œ ë²ˆë§Œ í…ŒìŠ¤íŠ¸

        ì•Œê³ ë¦¬ì¦˜ë³„ Trainer ë§¤í•‘:
            - baseline-mtp: BaselineMtpTrainer â†’ ê· ë“± ê°€ì¤‘ì¹˜
            - critic-wmtp: CriticWmtpTrainer â†’ Value Head ì§ì ‘ í†µí•© (v2.1.0+)
            - rho1-wmtp: Rho1WmtpTrainer â†’ Reference Model ì°¨ì´ ì§ì ‘ ê³„ì‚°

        Args:
            recipe: í›ˆë ¨ ë ˆì‹œí”¼ (ì•Œê³ ë¦¬ì¦˜, MTP ì„¤ì •, ì†ì‹¤í•¨ìˆ˜ ë“±)
            config: í™˜ê²½ ì„¤ì • (GPU, ë¶„ì‚°í›ˆë ¨, ë©”ëª¨ë¦¬ ìµœì í™” ë“±)

        Returns:
            ì•Œê³ ë¦¬ì¦˜ë³„ ë…ë¦½ Trainer ì¸ìŠ¤í„´ìŠ¤ (BaseWmtpTrainer ìƒì†)

        Raises:
            ValueError: ì§€ì›ë˜ì§€ ì•ŠëŠ” ì•Œê³ ë¦¬ì¦˜ ìš”ì²­ì‹œ
        """
        # Trainer ì„¤ì • êµ¬ì„±
        trainer_config = {
            "n_heads": MTP_CONFIG["n_heads"],  # ì˜ˆì¸¡ í—¤ë“œ ê°œìˆ˜ (ê³ ì •ê°’ 4)
            "horizon": MTP_CONFIG["horizon"],  # ì˜ˆì¸¡ ë²”ìœ„ (ê³ ì •ê°’ 4)
            "loss_config": {
                "weight_norm": recipe.loss.weight_norm,  # ê°€ì¤‘ì¹˜ ì •ê·œí™” ë°©ì‹
                "lambda": recipe.loss.lambda_weight,  # ì •ê·œí™” ê°•ë„ Î»
                "temperature": recipe.loss.weight_temperature,  # ì†Œí”„íŠ¸ë§¥ìŠ¤ ì˜¨ë„
                "epsilon": recipe.loss.epsilon,  # ìˆ˜ì¹˜ ì•ˆì •ì„±ìš© ì—¡ì‹¤ë¡ 
                "max_weight": recipe.loss.max_weight,  # ìµœëŒ€ ê°€ì¤‘ì¹˜ ì œí•œ
            },
            # í›ˆë ¨ ë°©ì‹ ì„¤ì •
            "full_finetune": recipe.train.full_finetune,  # ì „ì²´ íŒŒì¸íŠœë‹ ì „ìš©
            # ë¶„ì‚° í›ˆë ¨ ë° ë©”ëª¨ë¦¬ ìµœì í™”
            "mixed_precision": config.devices.mixed_precision,  # BF16/FP16 í˜¼í•© ì •ë°€ë„
            # FSDP (Fully Sharded Data Parallel) ì„¤ì •
            "fsdp_config": config.devices.fsdp.model_dump()
            if config.devices.fsdp.enabled
            else None,
        }

        # ì•Œê³ ë¦¬ì¦˜ë³„ íŠ¹í™” ì„¤ì • ì¶”ê°€
        algo = recipe.train.algo
        if algo == "critic-wmtp" and recipe.critic:
            trainer_config["critic_config"] = {
                "discount_lambda": recipe.critic.discount_lambda,
                "gamma": recipe.critic.gamma,
                "gae_lambda": recipe.critic.gae_lambda,
                # Phase 2.2: value_coef â†’ auxiliary_loss_coef (main loss always 1.0)
                "auxiliary_loss_coef": recipe.critic.auxiliary_loss_coef,
                "use_pseudo_rewards": recipe.critic.use_pseudo_rewards,
            }
        elif algo == "rho1-wmtp" and recipe.rho1:
            trainer_config["rho1_config"] = {
                "selection_mode": recipe.rho1.selection_mode,
                "skip_threshold_percentile": recipe.rho1.skip_threshold_percentile,
                "min_ce_diff": recipe.rho1.min_ce_diff,  # Phase 1.2: CE difference threshold
                # Phase 1: rho1.temperature, rho_alpha, rho_beta ì œê±°ë¨
                # temperatureëŠ” loss.weight_temperatureë¡œ í†µí•©
            }

        # Registryì—ì„œ Trainer ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° ë°˜í™˜
        return trainer_registry.create(recipe.train.algo, trainer_config)

    @staticmethod
    def create_optimizer(recipe: Recipe, model_params: Any) -> Optimizer:
        """ìµœì í™”ê¸°(Optimizer) ìƒì„± - ëª¨ë¸ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ ë‹´ë‹¹.

        í˜„ì¬ WMTPì—ì„œëŠ” AdamW + BFloat16 + Fused ì¡°í•©ì„ ì£¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
        ì´ëŠ” ëŒ€ê·œëª¨ ì–¸ì–´ëª¨ë¸ í›ˆë ¨ì—ì„œ ê²€ì¦ëœ ì•ˆì •ì ì´ê³  íš¨ìœ¨ì ì¸ ì¡°í•©ì…ë‹ˆë‹¤.

        ì§€ì› ìµœì í™”ê¸°:
            - adamw: AdamW + BF16 + Fused (ì¶”ì²œ, ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
            - lion: Lion ì˜µí‹°ë§ˆì´ì € (ë¯¸êµ¬í˜„, í–¥í›„ ì¶”ê°€ ì˜ˆì •)
            - sgd: SGD with momentum (ë¯¸êµ¬í˜„, í–¥í›„ ì¶”ê°€ ì˜ˆì •)

        Args:
            recipe: í›ˆë ¨ ë ˆì‹œí”¼ (í•™ìŠµë¥ , ê°€ì¤‘ì¹˜ ê°ì‡  ë“± ì˜µí‹°ë§ˆì´ì € ì„¤ì •)
            model_params: ìµœì í™”í•  ëª¨ë¸ íŒŒë¼ë¯¸í„° (ë³´í†µ model.parameters())

        Returns:
            ì„¤ì •ëœ Optimizer ì¸ìŠ¤í„´ìŠ¤

        Raises:
            ValueError: ì§€ì›ë˜ì§€ ì•ŠëŠ” ì˜µí‹°ë§ˆì´ì € ìš”ì²­ì‹œ
        """
        # ì§ì ‘ í˜¸ì¶œ: YAML optimizer ê°’ì´ ê³§ Registry í‚¤

        # ì˜µí‹°ë§ˆì´ì € ì„¤ì • êµ¬ì„±
        optimizer_config = {
            "params": model_params,  # ìµœì í™”í•  íŒŒë¼ë¯¸í„°
            "lr": recipe.optim.lr,  # í•™ìŠµë¥ 
            "weight_decay": recipe.optim.weight_decay,  # L2 ì •ê·œí™” (ê°€ì¤‘ì¹˜ ê°ì‡ )
            "betas": recipe.optim.betas,  # Adam ëª¨ë©˜í…€ ê³„ìˆ˜ (Î²â‚, Î²â‚‚)
            "grad_clip": recipe.optim.grad_clip,  # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ (í­ë°œ ë°©ì§€)
            "scheduler": recipe.optim.scheduler,  # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ íƒ€ì…
            "warmup_ratio": recipe.optim.warmup_ratio,  # ì›Œë°ì—… ë¹„ìœ¨
        }

        # Registryì—ì„œ Optimizer ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° ë°˜í™˜
        return optimizer_registry.create(recipe.optim.optimizer, optimizer_config)

    @staticmethod
    def create_data_loader(recipe: Recipe, config: Config) -> Loader:
        """ë°ì´í„° ë¡œë” ìƒì„± - recipe/configë§Œ ì‚¬ìš©í•˜ëŠ” í†µí•© íŒ¨í„´.

        WMTPëŠ” ë‹¤ì–‘í•œ ì½”ë“œ ìƒì„± ë²¤ì¹˜ë§ˆí¬ë¥¼ ì§€ì›í•©ë‹ˆë‹¤:
            - MBPP: Python ê¸°ë³¸ í”„ë¡œê·¸ë˜ë° ë¬¸ì œ
            - CodeContests: ì•Œê³ ë¦¬ì¦˜ ê²½ì§„ ëŒ€íšŒ ë¬¸ì œ
            - HumanEval: í•¨ìˆ˜ êµ¬í˜„ í‰ê°€ (OpenAI)
            - Custom: ì‚¬ìš©ì ì •ì˜ ë°ì´í„°ì…‹

        Args:
            recipe: í›ˆë ¨ ë ˆì‹œí”¼ (data.train.sources í•„ë“œ í¬í•¨)
            config: í™˜ê²½ ì„¤ì •

        Returns:
            DataLoader ì¸ìŠ¤í„´ìŠ¤
        """
        # 1. sourceë¥¼ recipeì—ì„œ ìë™ ì¶”ì¶œ (ë” ì´ìƒ ë³„ë„ ì¸ì ë¶ˆí•„ìš”)
        source = recipe.data.train.sources[0]  # ì²« ë²ˆì§¸ í›ˆë ¨ ì†ŒìŠ¤ ì‚¬ìš©

        # 2. ì†ŒìŠ¤ë³„ ë°ì´í„°ì…‹ ê²½ë¡œ ê²°ì • (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
        dataset_path = None
        if source == "mbpp":
            dataset_path = str(config.paths.datasets.mbpp)
        elif source in ["contest", "codecontests"]:
            dataset_path = str(config.paths.datasets.contest)
        else:
            # Custom ë˜ëŠ” ê¸°íƒ€ëŠ” sourceë¥¼ ê·¸ëŒ€ë¡œ ê²½ë¡œë¡œ ì‚¬ìš©
            dataset_path = source

        # 3. í†µí•© ë°ì´í„° ë¡œë” ì„¤ì •
        loader_config = {
            "paths": config.paths.model_dump(),
            "split": "train",  # ê¸°ë³¸ ë¶„í• 
            "dataset_type": source,  # ëª…ì‹œì  íƒ€ì… ì§€ì •
            "dataset_path": dataset_path,  # ê²½ë¡œ ì¶”ê°€
        }

        # S3 ì¸ì¦ ì •ë³´ê°€ ìˆìœ¼ë©´ ì¶”ê°€
        if config.s3_auth:
            loader_config["s3_auth"] = config.s3_auth.model_dump()

        # í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ storage ì •ë³´ ìƒì„± (deprecated)
        if (
            hasattr(config, "storage") and config.storage
        ):  # ë§ˆì´ê·¸ë ˆì´ì…˜ëœ old configì¸ ê²½ìš°
            loader_config["storage"] = config.storage

        # 4. DataLoader ìƒì„±
        return loader_registry.create("unified-data-loader", loader_config)

    @staticmethod
    def create_model_loader(
        config: Config, recipe: Recipe, model_type: str = "base"
    ) -> Loader:
        """í†µí•©ëœ ëª¨ë¸ ë¡œë” ìƒì„± - ìµœëŒ€í•œ ë‹¨ìˆœí™”ëœ ì¸í„°í˜ì´ìŠ¤.

        ëª¨ë“  ëª¨ë¸ì„ ì¼ê´€ëœ ë°©ì‹ìœ¼ë¡œ ë¡œë“œ:
        - "base": Base ëª¨ë¸
        - "aux": ì•Œê³ ë¦¬ì¦˜ì— ë”°ë¼ ìë™ ì„ íƒ (ref ë˜ëŠ” rm)

        Args:
            config: í™˜ê²½ ì„¤ì • (ëª¨ë¸ ê²½ë¡œë“¤)
            recipe: í›ˆë ¨ ë ˆì‹œí”¼ (ì•Œê³ ë¦¬ì¦˜ ì •ë³´)
            model_type: ëª¨ë¸ íƒ€ì… ("base" ë˜ëŠ” "aux")

        Returns:
            ModelLoader ì¸ìŠ¤í„´ìŠ¤ ë˜ëŠ” None (baseline-mtpì˜ auxì¸ ê²½ìš°)

        Usage:
            # ì¼ê´€ëœ í˜¸ì¶œ ë°©ì‹
            base_loader = ComponentFactory.create_model_loader(config, recipe, "base")
            aux_loader = ComponentFactory.create_model_loader(config, recipe, "aux")
        """
        loader_config = config.model_dump()

        # ëª¨ë¸ ê²½ë¡œ ê²°ì •
        if model_type == "base":
            model_path = str(config.paths.models.base)
        elif model_type == "aux":
            # auxëŠ” ì•Œê³ ë¦¬ì¦˜ì— ë”°ë¼ ìë™ ê²°ì •
            algorithm = recipe.train.algo
            if algorithm == "rho1-wmtp":
                model_path = str(config.paths.models.ref)
            elif algorithm == "critic-wmtp":
                model_path = str(config.paths.models.rm)
            elif algorithm in ["baseline-mtp", "mtp-baseline"]:
                # baselineì€ aux ëª¨ë¸ ë¶ˆí•„ìš”
                return None
            else:
                raise ValueError(f"Unknown algorithm for aux model: {algorithm}")
        else:
            raise ValueError(f"model_type must be 'base' or 'aux', got: {model_type}")

        loader_config["model_path"] = model_path
        loader_config["algorithm"] = recipe.train.algo
        loader_config["mtp_config"] = {
            "n_heads": MTP_CONFIG["n_heads"],
            "horizon": MTP_CONFIG["horizon"],
        }

        return loader_registry.create("standardized-model-loader", loader_config)

    @staticmethod
    def create_checkpoint_loader(config: Config) -> Loader:
        """ì²´í¬í¬ì¸íŠ¸ ì „ìš© ë¡œë” ìƒì„± - í›ˆë ¨ ì¬ê°œë¥¼ ìœ„í•œ íŠ¹í™”ëœ ì¸í„°í˜ì´ìŠ¤.

        í›ˆë ¨ ì¬ê°œ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ìœ„í•œ ì „ìš© ë¡œë”:
            - ë©”íƒ€ë°ì´í„° ìë™ ì¶”ì¶œ (epoch, step, mlflow_run_id)
            - S3/ë¡œì»¬ í†µí•© ì§€ì›
            - Rich Console ê¸°ë°˜ ì§„í–‰ìƒí™© í‘œì‹œ
            - ê²¬ê³ í•œ ì˜¤ë¥˜ ì²˜ë¦¬

        UnifiedModelLoaderì™€ì˜ ì°¨ì´ì :
            - ì²´í¬í¬ì¸íŠ¸ ì „ìš© ìµœì í™”
            - í›ˆë ¨ ë©”íƒ€ë°ì´í„° ìë™ íŒŒì‹±
            - ì¬ê°œ ì „ìš© ì¸í„°í˜ì´ìŠ¤

        Args:
            config: í™˜ê²½ ì„¤ì • (S3, ê²½ë¡œ, GPU ì„¤ì •)

        Returns:
            CheckpointLoader ì¸ìŠ¤í„´ìŠ¤

        Usage:
            ```python
            checkpoint_loader = ComponentFactory.create_checkpoint_loader(config)
            checkpoint_loader.setup({})
            result = checkpoint_loader.run({
                "model_path": "s3://wmtp/checkpoints/model.pt",
                "load_metadata": True
            })
            epoch = result["epoch"]
            step = result["step"]
            mlflow_run_id = result["mlflow_run_id"]
            ```
        """
        # ì²´í¬í¬ì¸íŠ¸ ë¡œë” ì„¤ì • êµ¬ì„±
        loader_config = config.model_dump()

        # CheckpointLoader ìƒì„± - ì²´í¬í¬ì¸íŠ¸ ì „ìš© íŠ¹í™” ê¸°ëŠ¥
        return loader_registry.create("checkpoint-loader", loader_config)

    @staticmethod
    def create_evaluator(recipe: Recipe, config: Config) -> Evaluator:
        """í‰ê°€ í”„ë¡œí† ì½œë³„ íŠ¹í™”ëœ í‰ê°€ê¸° ìƒì„±.

        ê° ë²¤ì¹˜ë§ˆí¬ë§ˆë‹¤ ë‹¤ë¥¸ í‰ê°€ ë°©ì‹ê³¼ ë©”íŠ¸ë¦­ì´ í•„ìš”í•©ë‹ˆë‹¤:
            - meta-mtp: Meta MTP ë…¼ë¬¸ ë°©ì‹ (pass@k, ì¶”ë¡  ì†ë„)
            - mbpp: MBPP í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì‹¤í–‰ ê¸°ë°˜ í‰ê°€
            - codecontests: ê²½ì§„ ëŒ€íšŒ ë¬¸ì œ ì •ë‹µ ë¹„êµ

        Args:
            recipe: í›ˆë ¨ ë ˆì‹œí”¼ (í‰ê°€ ì„¤ì •, ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„°)
            config: í™˜ê²½ ì„¤ì • (ë°°ì¹˜ í¬ê¸°, GPU ì„¤ì •)

        Returns:
            í”„ë¡œí† ì½œì— ë§ëŠ” Evaluator ì¸ìŠ¤í„´ìŠ¤

        Raises:
            ValueError: ì§€ì›ë˜ì§€ ì•ŠëŠ” í‰ê°€ í”„ë¡œí† ì½œ
        """
        # ì§ì ‘ í˜¸ì¶œ: YAML protocol ê°’ì´ ê³§ Registry í‚¤
        protocol = recipe.eval.protocol

        # í‰ê°€ê¸° ì„¤ì • êµ¬ì„±
        evaluator_config = {
            "sampling": recipe.eval.sampling.model_dump(),  # ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„°
            "metrics": recipe.eval.metrics,  # í‰ê°€ ë©”íŠ¸ë¦­ ë¦¬ìŠ¤íŠ¸
            "batch_size": recipe.data.eval.batch_size,  # í‰ê°€ ë°°ì¹˜ í¬ê¸°
        }

        # Registryì—ì„œ íŠ¹í™”ëœ í‰ê°€ê¸° ìƒì„±
        return evaluator_registry.create(protocol, evaluator_config)

    @staticmethod
    def create_pretrainer(recipe: Recipe) -> Any:
        """Create pretrainer for algorithms that support multi-stage training.

        Currently only critic-wmtp uses pretraining for value head initialization.

        Args:
            recipe: Training recipe configuration

        Returns:
            Pretrainer instance

        Raises:
            ValueError: If algorithm doesn't support pretraining or pretrain config missing
        """
        algo = recipe.train.algo

        if algo == "critic-wmtp":
            if not recipe.pretrain:
                raise ValueError("critic-wmtp requires pretrain configuration")

            pretrainer_config = {
                # Pretrain section (top-level)
                "num_epochs": recipe.pretrain.num_epochs,
                "max_steps": recipe.pretrain.max_steps,
                "lr": recipe.pretrain.lr,
                # Loss section
                "temperature": recipe.loss.weight_temperature,
                # Critic section (GAE parameters)
                "gamma": recipe.critic.gamma,
                "gae_lambda": recipe.critic.gae_lambda,
                "value_coef": recipe.critic.auxiliary_loss_coef,
                # Critic section (other parameters)
                "target": recipe.critic.target,
                "token_spread": recipe.critic.token_spread,
                "delta_mode": recipe.critic.delta_mode,
                "normalize": recipe.critic.normalize,
                # Early stopping
                "early_stopping": (
                    recipe.pretrain.early_stopping.model_dump()
                    if recipe.pretrain.early_stopping
                    else None
                ),
            }

            from src.components.registry import pretrainer_registry

            return pretrainer_registry.create(
                "critic-head-pretrainer", pretrainer_config
            )
        else:
            raise ValueError(
                f"Algorithm '{algo}' does not support pretraining. "
                f"Only 'critic-wmtp' currently uses pretraining."
            )

    # Phase 3: create_aux_model_loader ë©”ì„œë“œ ì œê±°ë¨
    # ëª¨ë“  ëª¨ë¸ ë¡œë”©ì€ create_model_loaderë¡œ í†µí•©ë˜ì—ˆìŠµë‹ˆë‹¤.
    # Usage:
    #   - Base ëª¨ë¸: create_model_loader(config, recipe, "base")
    #   - Reference ëª¨ë¸: create_model_loader(config, recipe, "ref")
    #   - Reward ëª¨ë¸: create_model_loader(config, recipe, "rm")

    @staticmethod
    def create_tokenizer(recipe: Recipe, config: Config) -> Any:
        """í† í¬ë‚˜ì´ì € ìƒì„± - í™˜ê²½ ê¸°ë°˜ ìë™ ì„ íƒ.

        í™˜ê²½(test/production)ì— ë”°ë¼ í† í¬ë‚˜ì´ì € ìë™ ì„ íƒ:
        1. Test í™˜ê²½ ("test" in path): hf-transformers ì‚¬ìš© (HuggingFace í˜¸í™˜)
        2. Production í™˜ê²½: hf-sentencepiece ì‚¬ìš© (Facebook MTP ëª¨ë¸ìš©)

        Args:
            recipe: í›ˆë ¨ ë ˆì‹œí”¼ (í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€, ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
            config: í™˜ê²½ ì„¤ì • (ëª¨ë¸ ê²½ë¡œì—ì„œ í™˜ê²½ ê°ì§€)

        Returns:
            í† í¬ë‚˜ì´ì € BaseComponent ì¸ìŠ¤í„´ìŠ¤

        Raises:
            ValueError: Registryì—ì„œ í† í¬ë‚˜ì´ì € ìƒì„± ì‹¤íŒ¨ì‹œ
        """
        # 1. í™˜ê²½ ê°ì§€ - base ëª¨ë¸ ê²½ë¡œì—ì„œ test í™˜ê²½ ì—¬ë¶€ íŒë‹¨
        base_model_path = str(config.paths.models.base)
        is_test_env = "test" in base_model_path.lower()

        # 2. í™˜ê²½ë³„ í† í¬ë‚˜ì´ì € ìë™ ê²°ì •
        if is_test_env:
            # í…ŒìŠ¤íŠ¸ í™˜ê²½: HuggingFace transformers í† í¬ë‚˜ì´ì €
            # distilgpt2 ë“± HuggingFace ëª¨ë¸ê³¼ í˜¸í™˜
            registry_key = "hf-transformers"
            print("[í™˜ê²½ ìë™ ê°ì§€] í…ŒìŠ¤íŠ¸ í™˜ê²½ â†’ hf-transformers í† í¬ë‚˜ì´ì € ì‚¬ìš©")
        else:
            # í”„ë¡œë•ì…˜ í™˜ê²½: Facebook MTP ëª¨ë¸ìš© SentencePiece
            # 7B MTP ëª¨ë¸ ë“± native MTP ëª¨ë¸ê³¼ í˜¸í™˜
            registry_key = "hf-sentencepiece"
            print("[í™˜ê²½ ìë™ ê°ì§€] í”„ë¡œë•ì…˜ í™˜ê²½ â†’ hf-sentencepiece í† í¬ë‚˜ì´ì € ì‚¬ìš©")

        # 4. ì„¤ì • êµ¬ì„± - config ê°’ ì§ì ‘ ì‚¬ìš©
        tokenizer_config = config.model_dump()

        # 5. Registry ìƒì„± ë° ë°˜í™˜ - í‘œì¤€ íŒ¨í„´
        return tokenizer_registry.create(registry_key, tokenizer_config)

    @staticmethod
    def create_evaluator_by_type(
        eval_type: str,
        recipe: Recipe,
        config: Config,
    ) -> Evaluator:
        """í‰ê°€ íƒ€ì…ë³„ íŠ¹í™”ëœ í‰ê°€ê¸° ìƒì„± (Meta ë…¼ë¬¸ ì§€ì›).

        Meta 2024 MTP ë…¼ë¬¸ì˜ ëª¨ë“  í‰ê°€ í•­ëª©ì„ ì¬í˜„í•˜ê¸° ìœ„í•œ
        í‰ê°€ê¸° ë™ì  ìƒì„± ë©”ì„œë“œ. evaluation_pipeline.pyì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤.

        Args:
            eval_type: í‰ê°€ íƒ€ì…
                - "meta-mtp": Pass@k ë©”íŠ¸ë¦­
                - "inference-speed": ì¶”ë¡  ì†ë„ ë¹„êµ
                - "per-head-analysis": í—¤ë“œë³„ ì„±ëŠ¥ ë¶„ì„
                - "token-accuracy": í† í° ìœ„ì¹˜ë³„ ì •í™•ë„
            recipe: í‰ê°€ ë ˆì‹œí”¼ ì„¤ì •
            config: í™˜ê²½ ì„¤ì •

        Returns:
            í‰ê°€ íƒ€ì…ì— ë§ëŠ” Evaluator ì¸ìŠ¤í„´ìŠ¤

        Raises:
            ValueError: ì§€ì›ë˜ì§€ ì•ŠëŠ” í‰ê°€ íƒ€ì…
        """
        # torch import for CUDA check
        import torch

        # í‰ê°€ íƒ€ì…ë³„ ì„¤ì • êµ¬ì„±
        eval_configs = {
            "meta-mtp": {
                "metrics": recipe.eval.metrics,
                "sampling": recipe.eval.sampling.model_dump(),
                "batch_size": recipe.data.eval.batch_size,
                "device": "cuda" if torch.cuda.is_available() else "cpu",
            },
            "inference-speed": {
                "batch_sizes": [1, 4, 8, 16],
                "sequence_lengths": [512, 1024, 2048],
                "num_trials": 10,
                "warmup_steps": 3,
                "device": "cuda" if torch.cuda.is_available() else "cpu",
            },
            "per-head-analysis": {
                "analyze_positions": True,
                "compute_confidence": True,
                "head_comparison": True,
                "position_buckets": [(0, 128), (128, 512), (512, 1024), (1024, 2048)],
                "batch_size": recipe.data.eval.batch_size,
                "device": "cuda" if torch.cuda.is_available() else "cpu",
            },
            "token-accuracy": {
                "position_range": (0, 100),
                "token_types": ["code", "text", "special"],
                "accuracy_threshold": 0.5,
                "granularity": 10,
                "analyze_token_types": True,
                "batch_size": recipe.data.eval.batch_size,
                "device": "cuda" if torch.cuda.is_available() else "cpu",
            },
            "self-speculative": {
                "num_sequences": 100,
                "max_tokens": 512,
                "temperature": recipe.eval.sampling.temperature
                if hasattr(recipe.eval.sampling, "temperature")
                else 0.8,
                "top_p": recipe.eval.sampling.top_p
                if hasattr(recipe.eval.sampling, "top_p")
                else 0.95,
                "measure_speedup": True,
                "device": "cuda" if torch.cuda.is_available() else "cpu",
            },
            "perplexity-measurer": {
                "batch_size": recipe.data.eval.batch_size,
                "max_length": 2048,
                "position_buckets": [[0, 128], [128, 512], [512, 1024], [1024, 2048]],
                "analyze_token_types": True,
                "compute_head_perplexity": True,
                "device": "cuda" if torch.cuda.is_available() else "cpu",
            },
            "metrics-visualizer": {
                "output_dir": "./figures",
                "save_formats": ["png", "pdf"],
                "use_plotly": True,
                "upload_to_mlflow": True,
                "figure_size": [10, 6],
            },
        }

        if eval_type not in eval_configs:
            raise ValueError(
                f"ì§€ì›ë˜ì§€ ì•ŠëŠ” í‰ê°€ íƒ€ì…: {eval_type}. "
                f"ì‚¬ìš© ê°€ëŠ¥í•œ ì˜µì…˜: {list(eval_configs.keys())}"
            )

        # í‰ê°€ê¸° ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        eval_config = eval_configs[eval_type]

        # Registryì—ì„œ í‰ê°€ê¸° ìƒì„±
        return evaluator_registry.create(eval_type, eval_config)
