"""WMTP ì»´í¬ë„ŒíŠ¸ íŒ©í† ë¦¬ - ì„¤ì • ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ ì»´í¬ë„ŒíŠ¸ ìƒì„±.

ì—°êµ¬ ì² í•™ ì§€ì›: "Not All Tokens Are What You Need"
===============================================

ì´ íŒ©í† ë¦¬ëŠ” WMTPì˜ í•µì‹¬ ì„¤ê³„ ì² í•™ì„ êµ¬í˜„í•©ë‹ˆë‹¤:
ë™ì¼í•œ íŒŒì´í”„ë¼ì¸ êµ¬ì¡°ì—ì„œ ì„œë¡œ ë‹¤ë¥¸ ì»´í¬ë„ŒíŠ¸ ì¡°í•©ì„ í†µí•´
ì„¸ ê°€ì§€ ì•Œê³ ë¦¬ì¦˜(mtp-baseline, critic-wmtp, rho1-wmtp)ì„ ì§€ì›í•©ë‹ˆë‹¤.

íŒ©í† ë¦¬ íŒ¨í„´ì˜ ì¥ì :
  1. ì„¤ì • ê¸°ë°˜ ìƒì„±: YAML ì„¤ì •ì—ì„œ ìë™ìœ¼ë¡œ ì í•©í•œ ì»´í¬ë„ŒíŠ¸ ì„ íƒ
  2. ì•Œê³ ë¦¬ì¦˜ ë¶„ë¦¬: ê° ì•Œê³ ë¦¬ì¦˜ì˜ íŠ¹í™”ëœ ë¡œì§ì„ ì»´í¬ë„ŒíŠ¸ ë‚´ë¶€ì— ìº¡ìŠí™”
  3. í™•ì¥ì„±: ìƒˆë¡œìš´ ì•Œê³ ë¦¬ì¦˜ ì¶”ê°€ ì‹œ Registryì—ë§Œ ë“±ë¡í•˜ë©´ ë¨
  4. ì¼ê´€ì„±: ëª¨ë“  ì•Œê³ ë¦¬ì¦˜ì´ ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤ ì‚¬ìš©

ì»´í¬ë„ŒíŠ¸ ì¡°í•© ì „ëµ:
  - mtp-baseline: BaselineMtpTrainer â†’ ê· ë“± ê°€ì¤‘ì¹˜
  - critic-wmtp: CriticWmtpTrainer â†’ Value Head ì§ì ‘ í†µí•©
  - rho1-wmtp: Rho1WmtpTrainer â†’ Reference Model ì°¨ì´ ê³„ì‚°

ì´ë¥¼ í†µí•´ ì—°êµ¬ìëŠ” ì•Œê³ ë¦¬ì¦˜ ê°„ ê³µì •í•œ ì„±ëŠ¥ ë¹„êµê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.
"""

from pathlib import Path  # ê²½ë¡œ ì¡°ì‘ìš©
from typing import Any  # ë²”ìš© íƒ€ì… íŒíŠ¸

# WMTP ì»´í¬ë„ŒíŠ¸ ë² ì´ìŠ¤ í´ë˜ìŠ¤ë“¤ - ëª¨ë“  êµ¬í˜„ì²´ê°€ ìƒì†ë°›ëŠ” ì¶”ìƒ ì¸í„°í˜ì´ìŠ¤
from src.components.base import (
    Evaluator,  # í‰ê°€ ìˆ˜í–‰ ì¸í„°í˜ì´ìŠ¤ (HumanEval, MBPP ë“±)
    Loader,  # ë°ì´í„°/ëª¨ë¸ ë¡œë”© ì¸í„°í˜ì´ìŠ¤
    Optimizer,  # ìµœì í™”ê¸° ì¸í„°í˜ì´ìŠ¤ (AdamW, Lion ë“±)
    # Scorer ì œê±°ë¨ (v2.1.0) - ëª¨ë“  ë¡œì§ì´ Trainerë¡œ í†µí•©
    Trainer,  # í›ˆë ¨ ì‹¤í–‰ ì¸í„°í˜ì´ìŠ¤ (WMTP í†µí•© íŠ¸ë ˆì´ë„ˆ)
)

# ê° ì»´í¬ë„ŒíŠ¸ íƒ€ì…ë³„ Registry - êµ¬í˜„ì²´ë“¤ì„ í‚¤ë¡œ ë“±ë¡/ì¡°íšŒí•˜ëŠ” ì €ì¥ì†Œ
from src.components.registry import (
    evaluator_registry,  # í‰ê°€ê¸° êµ¬í˜„ì²´ë“¤ (meta-mtp, mbpp-v1 ë“±)
    loader_registry,  # ë¡œë” êµ¬í˜„ì²´ë“¤ (hf-model, mtp-native ë“±)
    optimizer_registry,  # ì˜µí‹°ë§ˆì´ì € êµ¬í˜„ì²´ë“¤ (adamw-bf16-fused ë“±)
    registry,  # í†µí•© ë ˆì§€ìŠ¤íŠ¸ë¦¬ (ì§ì ‘ ì ‘ê·¼ìš©)
    # scorer_registry ì œê±°ë¨ (v2.1.0) - ëª¨ë“  scorer ë¡œì§ì´ trainerë¡œ í†µí•©
    tokenizer_registry,  # í† í¬ë‚˜ì´ì € êµ¬í˜„ì²´ë“¤ (unified-sentencepiece ë“±)
    trainer_registry,  # íŠ¸ë ˆì´ë„ˆ êµ¬í˜„ì²´ë“¤ (mtp-weighted-ce-trainer ë“±)
)
from src.settings import Config, Recipe  # Pydantic ì„¤ì • ëª¨ë¸ë“¤


class ComponentFactory:
    """WMTP ì•Œê³ ë¦¬ì¦˜ë³„ ì»´í¬ë„ŒíŠ¸ ìƒì„± íŒ©í† ë¦¬.

    ì—°êµ¬ ì² í•™ "Not All Tokens Are What You Need" êµ¬í˜„ì˜ í•µì‹¬:
        ì´ í´ë˜ìŠ¤ëŠ” ì„¤ì • íŒŒì¼(recipe.yaml)ì˜ ì•Œê³ ë¦¬ì¦˜ ì„ íƒì— ë”°ë¼
        ì í•©í•œ ì»´í¬ë„ŒíŠ¸ ì¡°í•©ì„ ìë™ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.

        Phase 2 ë¦¬íŒ©í† ë§: ê° WMTP ì•Œê³ ë¦¬ì¦˜ë§ˆë‹¤ ë…ë¦½ëœ íŠ¸ë ˆì´ë„ˆ í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
        - BaselineMtpTrainer: ê· ë“± ê°€ì¤‘ì¹˜
        - CriticWmtpTrainer: Critic ê¸°ë°˜ ê°€ì¤‘ì¹˜  
        - Rho1WmtpTrainer: Reference ëª¨ë¸ ê¸°ë°˜ ê°€ì¤‘ì¹˜

    ì„¤ê³„ ì›ì¹™:
        1. í•˜ë“œì½”ë”© ë°©ì§€: ëª¨ë“  ë§¤í•‘ ì •ë³´ë¥¼ í´ë˜ìŠ¤ ìƒìˆ˜ë¡œ ê´€ë¦¬
        2. Registry íŒ¨í„´: ì‹¤ì œ êµ¬í˜„ì²´ëŠ” ë³„ë„ Registryì—ì„œ ì¡°íšŒ
        3. ì„¤ì • ì£¼ë„: recipe.yamlì˜ ê°’ì´ ì»´í¬ë„ŒíŠ¸ ì„ íƒì„ ê²°ì •
        4. ì˜¤ë¥˜ ì²˜ë¦¬: ì˜ëª»ëœ ì„¤ì •ì— ëŒ€í•œ ëª…í™•í•œ ì—ëŸ¬ ë©”ì‹œì§€ ì œê³µ
    """

    # ğŸ¯ ì§ì ‘ í˜¸ì¶œ ë°©ì‹: YAML í‚¤ê°€ ê³§ Registry í‚¤
    # ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ ì œê±° - Pydantic ìŠ¤í‚¤ë§ˆì™€ Registry í‚¤ ì™„ì „ ì¼ì¹˜

    # create_scorer ë©”ì„œë“œëŠ” v2.1.0ë¶€í„° ì œê±°ë¨
    # ëª¨ë“  scorer ë¡œì§ì´ ê°ê°ì˜ Trainer í´ë˜ìŠ¤ë¡œ í†µí•©ë˜ì—ˆìŠµë‹ˆë‹¤.
    # - BaselineMtpTrainer: ê· ë“± ê°€ì¤‘ì¹˜ (scorer ë¶ˆí•„ìš”)
    # - CriticWmtpTrainer: Value Head ì§ì ‘ ê´€ë¦¬
    # - Rho1WmtpTrainer: Reference Model ì°¨ì´ ì§ì ‘ ê³„ì‚°

    @staticmethod
    def create_trainer(recipe: Recipe, config: Config) -> Trainer:
        """íŠ¸ë ˆì´ë„ˆ ìƒì„± - recipe/configë§Œ ì‚¬ìš©, scorer ì˜ì¡´ì„± ìë™ ê´€ë¦¬.

        WMTP ì„¤ê³„ì˜ ìš°ì•„í•¨: "One Trainer, Multiple Scorers"
            ì´ ë©”ì„œë“œëŠ” WMTPì˜ í•µì‹¬ ì„¤ê³„ ì² í•™ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
            Phase 2 ë¦¬íŒ©í† ë§ìœ¼ë¡œ ê° ì•Œê³ ë¦¬ì¦˜ë§ˆë‹¤ ë…ë¦½ëœ íŠ¸ë ˆì´ë„ˆë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
            ê³µí†µ ë¡œì§ì€ BaseWmtpTrainerì— ì¶”ìƒí™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

            ì´ í†µí•© ì ‘ê·¼ë²•ì˜ ì¥ì :
            1. ê³µì •í•œ ë¹„êµ: ì•Œê³ ë¦¬ì¦˜ ê°„ ì°¨ì´ëŠ” ì˜¤ì§ ê°€ì¤‘ì¹˜ ê³„ì‚° ë°©ì‹
            2. ì½”ë“œ ì¤‘ë³µ ì œê±°: í›ˆë ¨ ë¡œì§ì€ í•œ ê³³ì—ë§Œ êµ¬í˜„
            3. ìœ ì§€ë³´ìˆ˜ì„±: ìƒˆ ì•Œê³ ë¦¬ì¦˜ ì¶”ê°€ì‹œ Scorerë§Œ ê°œë°œ
            4. ë²„ê·¸ ìµœì†Œí™”: ê³µí†µ ë¡œì§ì€ í•œ ë²ˆë§Œ í…ŒìŠ¤íŠ¸

        ì•Œê³ ë¦¬ì¦˜ë³„ Trainer ë§¤í•‘:
            - baseline-mtp: BaselineMtpTrainer â†’ ê· ë“± ê°€ì¤‘ì¹˜
            - critic-wmtp: CriticWmtpTrainer â†’ Value Head ì§ì ‘ í†µí•© (v2.1.0+)
            - rho1-wmtp: Rho1WmtpTrainer â†’ Reference Model ì°¨ì´ ì§ì ‘ ê³„ì‚°

        Args:
            recipe: í›ˆë ¨ ë ˆì‹œí”¼ (ì•Œê³ ë¦¬ì¦˜, MTP ì„¤ì •, ì†ì‹¤í•¨ìˆ˜ ë“±)
            config: í™˜ê²½ ì„¤ì • (GPU, ë¶„ì‚°í›ˆë ¨, ë©”ëª¨ë¦¬ ìµœì í™” ë“±)

        Returns:
            ì•Œê³ ë¦¬ì¦˜ë³„ ë…ë¦½ Trainer ì¸ìŠ¤í„´ìŠ¤ (BaseWmtpTrainer ìƒì†)

        Raises:
            ValueError: ì§€ì›ë˜ì§€ ì•ŠëŠ” ì•Œê³ ë¦¬ì¦˜ ìš”ì²­ì‹œ
        """
        # Trainer ì„¤ì • êµ¬ì„±
        trainer_config = {
            # MTP ëª¨ë¸ ê´€ë ¨ ì„¤ì •
            "n_heads": recipe.model.mtp.n_heads,  # ì˜ˆì¸¡ í—¤ë“œ ê°œìˆ˜ (ë³´í†µ 4)
            "horizon": recipe.model.mtp.horizon,  # ì˜ˆì¸¡ ë²”ìœ„ (t+1, t+2, t+3, t+4)
            # ì†ì‹¤ í•¨ìˆ˜ ì„¤ì • - WMTP ê³µì‹ L_WMTP = Î£ w_{t+k} Ã— CE_k
            "loss_config": {
                "weight_norm": recipe.loss.weight_norm,  # ê°€ì¤‘ì¹˜ ì •ê·œí™” ë°©ì‹
                "lambda": recipe.loss.lambda_weight,  # ì •ê·œí™” ê°•ë„ Î»
                "temperature": recipe.loss.temperature,  # ì†Œí”„íŠ¸ë§¥ìŠ¤ ì˜¨ë„
                "epsilon": recipe.loss.epsilon,  # ìˆ˜ì¹˜ ì•ˆì •ì„±ìš© ì—¡ì‹¤ë¡ 
                "max_weight": recipe.loss.max_weight,  # ìµœëŒ€ ê°€ì¤‘ì¹˜ ì œí•œ
            },
            # í›ˆë ¨ ë°©ì‹ ì„¤ì •
            "full_finetune": recipe.train.full_finetune,  # ì „ì²´ íŒŒì¸íŠœë‹ ì „ìš©
            # ë¶„ì‚° í›ˆë ¨ ë° ë©”ëª¨ë¦¬ ìµœì í™”
            "mixed_precision": config.devices.mixed_precision,  # BF16/FP16 í˜¼í•© ì •ë°€ë„
            # FSDP (Fully Sharded Data Parallel) ì„¤ì •
            "fsdp_config": config.devices.fsdp.model_dump()
            if config.devices.fsdp.enabled
            else None
        }

        # Registryì—ì„œ Trainer ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° ë°˜í™˜
        return trainer_registry.create(recipe.train.algo, trainer_config)

    @staticmethod
    def create_optimizer(recipe: Recipe, model_params: Any) -> Optimizer:
        """ìµœì í™”ê¸°(Optimizer) ìƒì„± - ëª¨ë¸ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ ë‹´ë‹¹.

        í˜„ì¬ WMTPì—ì„œëŠ” AdamW + BFloat16 + Fused ì¡°í•©ì„ ì£¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
        ì´ëŠ” ëŒ€ê·œëª¨ ì–¸ì–´ëª¨ë¸ í›ˆë ¨ì—ì„œ ê²€ì¦ëœ ì•ˆì •ì ì´ê³  íš¨ìœ¨ì ì¸ ì¡°í•©ì…ë‹ˆë‹¤.

        ì§€ì› ìµœì í™”ê¸°:
            - adamw: AdamW + BF16 + Fused (ì¶”ì²œ, ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
            - lion: Lion ì˜µí‹°ë§ˆì´ì € (ë¯¸êµ¬í˜„, í–¥í›„ ì¶”ê°€ ì˜ˆì •)
            - sgd: SGD with momentum (ë¯¸êµ¬í˜„, í–¥í›„ ì¶”ê°€ ì˜ˆì •)

        Args:
            recipe: í›ˆë ¨ ë ˆì‹œí”¼ (í•™ìŠµë¥ , ê°€ì¤‘ì¹˜ ê°ì‡  ë“± ì˜µí‹°ë§ˆì´ì € ì„¤ì •)
            model_params: ìµœì í™”í•  ëª¨ë¸ íŒŒë¼ë¯¸í„° (ë³´í†µ model.parameters())

        Returns:
            ì„¤ì •ëœ Optimizer ì¸ìŠ¤í„´ìŠ¤

        Raises:
            ValueError: ì§€ì›ë˜ì§€ ì•ŠëŠ” ì˜µí‹°ë§ˆì´ì € ìš”ì²­ì‹œ
        """
        # ì§ì ‘ í˜¸ì¶œ: YAML optimizer ê°’ì´ ê³§ Registry í‚¤

        # ì˜µí‹°ë§ˆì´ì € ì„¤ì • êµ¬ì„±
        optimizer_config = {
            "params": model_params,  # ìµœì í™”í•  íŒŒë¼ë¯¸í„°
            "lr": recipe.optim.lr,  # í•™ìŠµë¥ 
            "weight_decay": recipe.optim.weight_decay,  # L2 ì •ê·œí™” (ê°€ì¤‘ì¹˜ ê°ì‡ )
            "betas": recipe.optim.betas,  # Adam ëª¨ë©˜í…€ ê³„ìˆ˜ (Î²â‚, Î²â‚‚)
            "grad_clip": recipe.optim.grad_clip,  # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ (í­ë°œ ë°©ì§€)
            "scheduler": recipe.optim.scheduler,  # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ íƒ€ì…
            "warmup_ratio": recipe.optim.warmup_ratio,  # ì›Œë°ì—… ë¹„ìœ¨
        }

        # Registryì—ì„œ Optimizer ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° ë°˜í™˜
        return optimizer_registry.create(recipe.optim.optimizer, optimizer_config)

    @staticmethod
    def create_data_loader(recipe: Recipe, config: Config) -> Loader:
        """ë°ì´í„° ë¡œë” ìƒì„± - recipe/configë§Œ ì‚¬ìš©í•˜ëŠ” í†µí•© íŒ¨í„´.

        WMTPëŠ” ë‹¤ì–‘í•œ ì½”ë“œ ìƒì„± ë²¤ì¹˜ë§ˆí¬ë¥¼ ì§€ì›í•©ë‹ˆë‹¤:
            - MBPP: Python ê¸°ë³¸ í”„ë¡œê·¸ë˜ë° ë¬¸ì œ
            - CodeContests: ì•Œê³ ë¦¬ì¦˜ ê²½ì§„ ëŒ€íšŒ ë¬¸ì œ
            - HumanEval: í•¨ìˆ˜ êµ¬í˜„ í‰ê°€ (OpenAI)
            - Custom: ì‚¬ìš©ì ì •ì˜ ë°ì´í„°ì…‹

        Args:
            recipe: í›ˆë ¨ ë ˆì‹œí”¼ (data.train.sources í•„ë“œ í¬í•¨)
            config: í™˜ê²½ ì„¤ì •

        Returns:
            UnifiedDataLoader ì¸ìŠ¤í„´ìŠ¤
        """
        # 1. sourceë¥¼ recipeì—ì„œ ìë™ ì¶”ì¶œ (ë” ì´ìƒ ë³„ë„ ì¸ì ë¶ˆí•„ìš”)
        source = recipe.data.train.sources[0]  # ì²« ë²ˆì§¸ í›ˆë ¨ ì†ŒìŠ¤ ì‚¬ìš©

        # 2. ì†ŒìŠ¤ë³„ ë°ì´í„°ì…‹ ê²½ë¡œ ê²°ì • (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
        dataset_path = None
        if source == "mbpp":
            dataset_path = str(config.paths.datasets.mbpp)
        elif source in ["contest", "codecontests"]:
            dataset_path = str(config.paths.datasets.contest)
        else:
            # Custom ë˜ëŠ” ê¸°íƒ€ëŠ” sourceë¥¼ ê·¸ëŒ€ë¡œ ê²½ë¡œë¡œ ì‚¬ìš©
            dataset_path = source

        # 3. í†µí•© ë°ì´í„° ë¡œë” ì„¤ì • (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
        loader_config = {
            "storage": config.storage.model_dump(),
            "paths": config.paths.model_dump(),
            "split": "train",  # ê¸°ë³¸ ë¶„í• 
            "dataset_type": source,  # ëª…ì‹œì  íƒ€ì… ì§€ì •
            "dataset_path": dataset_path,  # ê²½ë¡œ ì¶”ê°€
        }

        # 4. UnifiedDataLoader ìƒì„±
        return loader_registry.create("unified-data-loader", loader_config)

    @staticmethod
    def create_model_loader(config: Config, recipe: Recipe = None) -> Loader:
        """í†µí•© ëª¨ë¸ ë¡œë”ë§Œ ë°˜í™˜ - Phase 2 ë¦¬íŒ©í† ë§ ì ìš©.

        WMTPëŠ” Facebookì˜ native MTP ëª¨ë¸ì„ ê¸°ë³¸ìœ¼ë¡œ ì‚¬ìš©í•˜ë˜,
        ë‹¤ì–‘í•œ ëª¨ë¸ ì†ŒìŠ¤ì™€ í¬ë§·ì„ ì§€ì›í•©ë‹ˆë‹¤:
            - mtp-native: Facebook native MTP (consolidated.pth)
            - hf-model: HuggingFace ë³€í™˜ëœ ëª¨ë¸
            - checkpoint: í›ˆë ¨ ì¤‘ë‹¨ì  íŒŒì¼ (.pt/.pth)
            - sheared-llama: Princeton ê²½ëŸ‰í™” ëª¨ë¸
            - starling-rm: Berkeley ë³´ìƒ ëª¨ë¸

        Args:
            config: í™˜ê²½ ì„¤ì • (ëª¨ë¸ ê²½ë¡œ, GPU ì„¤ì • ë“±)
            recipe: í›ˆë ¨ ë ˆì‹œí”¼ (ì„ íƒ)

        Returns:
            UnifiedModelLoader ì¸ìŠ¤í„´ìŠ¤
        """
        # í†µí•© ëª¨ë¸ ë¡œë” ì„¤ì •
        loader_config = config.model_dump()

        # UnifiedModelLoader ìƒì„± - ëª¨ë“  ëª¨ë¸ íƒ€ì…ì„ í•˜ë‚˜ì˜ ë¡œë”ë¡œ ì²˜ë¦¬
        return loader_registry.create("unified-model-loader", loader_config)

    @staticmethod
    def create_checkpoint_loader(config: Config) -> Loader:
        """ì²´í¬í¬ì¸íŠ¸ ì „ìš© ë¡œë” ìƒì„± - í›ˆë ¨ ì¬ê°œë¥¼ ìœ„í•œ íŠ¹í™”ëœ ì¸í„°í˜ì´ìŠ¤.

        í›ˆë ¨ ì¬ê°œ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ìœ„í•œ ì „ìš© ë¡œë”:
            - ë©”íƒ€ë°ì´í„° ìë™ ì¶”ì¶œ (epoch, step, mlflow_run_id)
            - S3/ë¡œì»¬ í†µí•© ì§€ì›
            - Rich Console ê¸°ë°˜ ì§„í–‰ìƒí™© í‘œì‹œ
            - ê²¬ê³ í•œ ì˜¤ë¥˜ ì²˜ë¦¬

        UnifiedModelLoaderì™€ì˜ ì°¨ì´ì :
            - ì²´í¬í¬ì¸íŠ¸ ì „ìš© ìµœì í™”
            - í›ˆë ¨ ë©”íƒ€ë°ì´í„° ìë™ íŒŒì‹±
            - ì¬ê°œ ì „ìš© ì¸í„°í˜ì´ìŠ¤

        Args:
            config: í™˜ê²½ ì„¤ì • (S3, ê²½ë¡œ, GPU ì„¤ì •)

        Returns:
            CheckpointLoader ì¸ìŠ¤í„´ìŠ¤

        Usage:
            ```python
            checkpoint_loader = ComponentFactory.create_checkpoint_loader(config)
            checkpoint_loader.setup({})
            result = checkpoint_loader.run({
                "model_path": "s3://wmtp/checkpoints/model.pt",
                "load_metadata": True
            })
            epoch = result["epoch"]
            step = result["step"]
            mlflow_run_id = result["mlflow_run_id"]
            ```
        """
        # ì²´í¬í¬ì¸íŠ¸ ë¡œë” ì„¤ì • êµ¬ì„±
        loader_config = config.model_dump()

        # CheckpointLoader ìƒì„± - ì²´í¬í¬ì¸íŠ¸ ì „ìš© íŠ¹í™” ê¸°ëŠ¥
        return loader_registry.create("checkpoint-loader", loader_config)

    @staticmethod
    def create_evaluator(recipe: Recipe, config: Config) -> Evaluator:
        """í‰ê°€ í”„ë¡œí† ì½œë³„ íŠ¹í™”ëœ í‰ê°€ê¸° ìƒì„±.

        ê° ë²¤ì¹˜ë§ˆí¬ë§ˆë‹¤ ë‹¤ë¥¸ í‰ê°€ ë°©ì‹ê³¼ ë©”íŠ¸ë¦­ì´ í•„ìš”í•©ë‹ˆë‹¤:
            - meta-mtp: Meta MTP ë…¼ë¬¸ ë°©ì‹ (pass@k, ì¶”ë¡  ì†ë„)
            - mbpp: MBPP í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì‹¤í–‰ ê¸°ë°˜ í‰ê°€
            - codecontests: ê²½ì§„ ëŒ€íšŒ ë¬¸ì œ ì •ë‹µ ë¹„êµ

        Args:
            recipe: í›ˆë ¨ ë ˆì‹œí”¼ (í‰ê°€ ì„¤ì •, ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„°)
            config: í™˜ê²½ ì„¤ì • (ë°°ì¹˜ í¬ê¸°, GPU ì„¤ì •)

        Returns:
            í”„ë¡œí† ì½œì— ë§ëŠ” Evaluator ì¸ìŠ¤í„´ìŠ¤

        Raises:
            ValueError: ì§€ì›ë˜ì§€ ì•ŠëŠ” í‰ê°€ í”„ë¡œí† ì½œ
        """
        # ì§ì ‘ í˜¸ì¶œ: YAML protocol ê°’ì´ ê³§ Registry í‚¤
        protocol = recipe.eval.protocol

        # í‰ê°€ê¸° ì„¤ì • êµ¬ì„±
        evaluator_config = {
            "sampling": recipe.eval.sampling.model_dump(),  # ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„°
            "metrics": recipe.eval.metrics,  # í‰ê°€ ë©”íŠ¸ë¦­ ë¦¬ìŠ¤íŠ¸
            "batch_size": recipe.data.eval.batch_size,  # í‰ê°€ ë°°ì¹˜ í¬ê¸°
        }

        # Registryì—ì„œ íŠ¹í™”ëœ í‰ê°€ê¸° ìƒì„±
        return evaluator_registry.create(protocol, evaluator_config)

    @staticmethod
    def create_pretrainer(recipe: Recipe) -> Any:
        """ì•Œê³ ë¦¬ì¦˜ë³„ ì‚¬ì „í›ˆë ¨ê¸° ìƒì„± - ComponentFactory íŒ¨í„´ ì¼ê´€ì„± ìœ ì§€.

        í˜„ì¬ëŠ” critic-wmtpì˜ Stage1 pretrainerë§Œ ì§€ì›í•˜ì§€ë§Œ,
        í–¥í›„ ë‹¤ë¥¸ ì•Œê³ ë¦¬ì¦˜ì˜ multi-stage í•™ìŠµì„ ìœ„í•´ í™•ì¥ ê°€ëŠ¥í•œ êµ¬ì¡°ë¡œ ì„¤ê³„.

        ì•Œê³ ë¦¬ì¦˜ë³„ Pretrainer ë§¤í•‘:
            - critic-wmtp: Stage1 Value Head í›ˆë ¨ê¸°
            - rho1-wmtp: í˜„ì¬ ë¯¸ì§€ì› (ë‹¨ì¼ ìŠ¤í…Œì´ì§€)
            - mtp-baseline: í˜„ì¬ ë¯¸ì§€ì› (ë‹¨ì¼ ìŠ¤í…Œì´ì§€)

        Args:
            recipe: í›ˆë ¨ ë ˆì‹œí”¼ ì„¤ì • (ì•Œê³ ë¦¬ì¦˜ ë° critic ì„¤ì • í¬í•¨)

        Returns:
            ì„ íƒëœ ì•Œê³ ë¦¬ì¦˜ì— ë§ëŠ” Pretrainer ì¸ìŠ¤í„´ìŠ¤

        Raises:
            ValueError: ì§€ì›ë˜ì§€ ì•ŠëŠ” ì•Œê³ ë¦¬ì¦˜ì´ ìš”ì²­ëœ ê²½ìš°
        """
        algo = recipe.train.algo

        if algo == "critic-wmtp":
            # Critic: Stage1 Value Head í›ˆë ¨ì„ ìœ„í•œ ì„¤ì •
            pretrainer_config = {
                # ë³´ìƒ íƒ€ê²Ÿ: "rm_sequence" (ì‹œí€€ìŠ¤ ë ˆë²¨ ë³´ìƒ ì‚¬ìš©)
                "target": getattr(recipe.critic, "target", "rm_sequence")
                if hasattr(recipe, "critic")
                else "rm_sequence",
                # í† í° í™•ì‚° ë°©ì‹: "gae" (Generalized Advantage Estimation)
                "token_spread": getattr(recipe.critic, "token_spread", "gae")
                if hasattr(recipe, "critic")
                else "gae",
                # ë¸íƒ€ ê³„ì‚° ëª¨ë“œ: "td" (Temporal Difference)
                "delta_mode": getattr(recipe.critic, "delta_mode", "td")
                if hasattr(recipe, "critic")
                else "td",
                # ì •ê·œí™” ë°©ì‹: "zscore" (í‘œì¤€í™”)
                "normalize": getattr(recipe.critic, "normalize", "zscore")
                if hasattr(recipe, "critic")
                else "zscore",
                "temperature": recipe.loss.temperature,  # ì†Œí”„íŠ¸ë§¥ìŠ¤ ì˜¨ë„
                "lr": 1e-4,  # Stage1 ì „ìš© í•™ìŠµë¥  (ë³´í†µ ë©”ì¸ë³´ë‹¤ ë‚®ìŒ)
            }

            # Registryì—ì„œ Stage1 Pretrainer ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° ë°˜í™˜
            from src.components.registry import pretrainer_registry
            return pretrainer_registry.create("critic-head-pretrainer", pretrainer_config)

        else:
            # ë‹¤ë¥¸ ì•Œê³ ë¦¬ì¦˜ë“¤ì€ ë‹¨ì¼ ìŠ¤í…Œì´ì§€ì´ë¯€ë¡œ pretrainer ë¶ˆí•„ìš”
            raise ValueError(
                f"Algorithm '{algo}' does not support multi-stage training. "
                f"Only 'critic-wmtp' currently requires pretrainer."
            )

    @staticmethod
    def create_aux_model_loader(recipe: Recipe, config: Config, aux_type: str) -> Loader:
        """ì•Œê³ ë¦¬ì¦˜ë³„ ë³´ì¡° ëª¨ë¸ ë¡œë” ìƒì„± - ref/rm ëª¨ë¸ ì „ìš©.

        WMTP ì•Œê³ ë¦¬ì¦˜ë³„ ë³´ì¡° ëª¨ë¸ ì²˜ë¦¬ë¥¼ ìœ„í•œ íŠ¹í™”ëœ ë¡œë”:
            - rho1-wmtp: Reference Model ë¡œë”© (CE ì°¨ì´ ê³„ì‚°ìš©)
            - critic-wmtp: Reward Model ë¡œë”© (Value Head í›ˆë ¨ìš©)
            - mtp-baseline: ë³´ì¡° ëª¨ë¸ ë¶ˆí•„ìš” (ì—ëŸ¬ ë°œìƒ)

        create_model_loaderì™€ì˜ ì°¨ì´ì :
            1. ì•Œê³ ë¦¬ì¦˜ë³„ íŠ¹í™”: recipe.train.algoì— ë”°ë¥¸ ê²€ì¦
            2. ë³´ì¡° ëª¨ë¸ ì „ìš©: base ëª¨ë¸ê³¼ êµ¬ë¶„ëœ ì²˜ë¦¬
            3. ê²½ë¡œ ìë™ ë§¤í•‘: aux_typeì— ë”°ë¥¸ config ê²½ë¡œ ìë™ ì„ íƒ
            4. íƒ€ì… ì•ˆì „ì„±: ì˜ëª»ëœ ì•Œê³ ë¦¬ì¦˜-ëª¨ë¸ ì¡°í•© ì°¨ë‹¨

        Args:
            recipe: í›ˆë ¨ ë ˆì‹œí”¼ (ì•Œê³ ë¦¬ì¦˜ íƒ€ì… í™•ì¸ìš©)
            config: í™˜ê²½ ì„¤ì • (ëª¨ë¸ ê²½ë¡œë“¤ í¬í•¨)
            aux_type: ë³´ì¡° ëª¨ë¸ íƒ€ì… ("ref" | "rm")

        Returns:
            ì•Œê³ ë¦¬ì¦˜ì— ë§ëŠ” UnifiedModelLoader ì¸ìŠ¤í„´ìŠ¤

        Raises:
            ValueError: ì˜ëª»ëœ ì•Œê³ ë¦¬ì¦˜-ë³´ì¡°ëª¨ë¸ ì¡°í•©

        Usage:
            ```python
            # Rho1 ì•Œê³ ë¦¬ì¦˜ì˜ ì°¸ì¡° ëª¨ë¸
            ref_loader = ComponentFactory.create_aux_model_loader(
                recipe, config, "ref"
            )

            # Critic ì•Œê³ ë¦¬ì¦˜ì˜ ë³´ìƒ ëª¨ë¸
            rm_loader = ComponentFactory.create_aux_model_loader(
                recipe, config, "rm"
            )
            ```
        """
        algo = recipe.train.algo

        # ì•Œê³ ë¦¬ì¦˜ë³„ ë³´ì¡° ëª¨ë¸ í•„ìš”ì„± ê²€ì¦
        if algo == "baseline-mtp":
            raise ValueError(
                f"Algorithm '{algo}' does not require auxiliary models. "
                f"Use create_model_loader() for base model only."
            )
        elif algo == "rho1-wmtp" and aux_type != "ref":
            raise ValueError(
                f"Algorithm '{algo}' only supports aux_type='ref' for reference model. "
                f"Got aux_type='{aux_type}'"
            )
        elif algo == "critic-wmtp" and aux_type != "rm":
            raise ValueError(
                f"Algorithm '{algo}' only supports aux_type='rm' for reward model. "
                f"Got aux_type='{aux_type}'"
            )
        elif algo not in ["rho1-wmtp", "critic-wmtp"]:
            raise ValueError(
                f"Algorithm '{algo}' is not supported for auxiliary model loading. "
                f"Supported algorithms: 'rho1-wmtp', 'critic-wmtp'"
            )

        # aux_typeì— ë”°ë¥¸ ëª¨ë¸ ê²½ë¡œ ìë™ ë§¤í•‘
        aux_model_paths = {
            "ref": config.paths.models.ref,
            "rm": config.paths.models.rm,
        }

        if aux_type not in aux_model_paths:
            raise ValueError(
                f"Invalid aux_type '{aux_type}'. "
                f"Supported types: {list(aux_model_paths.keys())}"
            )

        # UnifiedModelLoader ì„¤ì • - base loaderì™€ ë™ì¼í•˜ë‚˜ ê²½ë¡œ ë° ë©”íƒ€ë°ì´í„° ì°¨ë³„í™”
        loader_config = config.model_dump()

        # ë³´ì¡° ëª¨ë¸ ë©”íƒ€ë°ì´í„° ì¶”ê°€
        loader_config["_aux_model_info"] = {
            "algorithm": algo,
            "aux_type": aux_type,
            "model_path": str(aux_model_paths[aux_type]),
            "description": f"{algo} auxiliary {aux_type} model loader",
        }

        # UnifiedModelLoader ìƒì„± - ë™ì¼í•œ ë¡œë” í´ë˜ìŠ¤ ì‚¬ìš©
        return loader_registry.create("unified-model-loader", loader_config)

    @staticmethod
    def create_tokenizer(recipe: Recipe, config: Config) -> Any:
        """í† í¬ë‚˜ì´ì € ìƒì„± - recipe/configë§Œ ì‚¬ìš©í•˜ëŠ” í†µí•© íŒ¨í„´.

        ë‘ ê°€ì§€ í† í¬ë‚˜ì´ì € ì¤‘ recipe ì„¤ì •ì— ë”°ë¼ ìë™ ì„ íƒ:
        1. "hf": HfSentencePieceTokenizer - HuggingFace í˜¸í™˜ ì¸í„°í˜ì´ìŠ¤
        2. "raw": SentencePieceTokenizer - Raw SentencePiece ì¸í„°í˜ì´ìŠ¤

        Args:
            recipe: í›ˆë ¨ ë ˆì‹œí”¼ (tokenizer_type í•„ë“œ í¬í•¨)
            config: í™˜ê²½ ì„¤ì • (í† í¬ë‚˜ì´ì € ê²½ë¡œ ì •ë³´ í¬í•¨)

        Returns:
            í† í¬ë‚˜ì´ì € BaseComponent ì¸ìŠ¤í„´ìŠ¤

        Raises:
            ValueError: ì§€ì›ë˜ì§€ ì•ŠëŠ” tokenizer_type
        """
        # 1. tokenizer_typeì„ recipeì—ì„œ ê°€ì ¸ì˜´ (ë” ì´ìƒ ë³„ë„ ì¸ì ë¶ˆí•„ìš”)
        tokenizer_type = recipe.model.tokenizer_type

        # 2. Registry í‚¤ ê²°ì • - recipe ê¸°ë°˜ tokenizer_type
        if tokenizer_type in ["hf", "huggingface", "hf-sentencepiece"]:
            registry_key = "hf"
        elif tokenizer_type in ["raw", "sentencepiece", "default"]:
            registry_key = "default"
        else:
            raise ValueError(
                f"ì§€ì›ë˜ì§€ ì•ŠëŠ” tokenizer_type: {tokenizer_type}. "
                f"ì‚¬ìš© ê°€ëŠ¥í•œ ì˜µì…˜: 'hf', 'huggingface', 'raw', 'sentencepiece'"
            )

        # 3. ì„¤ì • êµ¬ì„± - config ê°’ ì§ì ‘ ì‚¬ìš©
        tokenizer_config = config.model_dump()

        # 4. Registry ìƒì„± ë° ë°˜í™˜ - í‘œì¤€ íŒ¨í„´
        return tokenizer_registry.create(registry_key, tokenizer_config)

    @staticmethod
    def create_evaluator_by_type(eval_type: str, recipe: Recipe, config: Config) -> Evaluator:
        """í‰ê°€ íƒ€ì…ë³„ íŠ¹í™”ëœ í‰ê°€ê¸° ìƒì„± (Meta ë…¼ë¬¸ ì§€ì›).

        Meta 2024 MTP ë…¼ë¬¸ì˜ ëª¨ë“  í‰ê°€ í•­ëª©ì„ ì¬í˜„í•˜ê¸° ìœ„í•œ
        í‰ê°€ê¸° ë™ì  ìƒì„± ë©”ì„œë“œ. evaluation_pipeline.pyì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤.

        Args:
            eval_type: í‰ê°€ íƒ€ì…
                - "meta-mtp": Pass@k ë©”íŠ¸ë¦­
                - "inference-speed": ì¶”ë¡  ì†ë„ ë¹„êµ
                - "per-head-analysis": í—¤ë“œë³„ ì„±ëŠ¥ ë¶„ì„
                - "token-accuracy": í† í° ìœ„ì¹˜ë³„ ì •í™•ë„
            recipe: í‰ê°€ ë ˆì‹œí”¼ ì„¤ì •
            config: í™˜ê²½ ì„¤ì •

        Returns:
            í‰ê°€ íƒ€ì…ì— ë§ëŠ” Evaluator ì¸ìŠ¤í„´ìŠ¤

        Raises:
            ValueError: ì§€ì›ë˜ì§€ ì•ŠëŠ” í‰ê°€ íƒ€ì…
        """
        # torch import for CUDA check
        import torch

        # í‰ê°€ íƒ€ì…ë³„ ì„¤ì • êµ¬ì„±
        eval_configs = {
            "meta-mtp": {
                "metrics": recipe.eval.metrics,
                "sampling": recipe.eval.sampling.model_dump(),
                "batch_size": recipe.data.eval.batch_size,
                "device": "cuda" if torch.cuda.is_available() else "cpu"
            },
            "inference-speed": {
                "batch_sizes": [1, 4, 8, 16],
                "sequence_lengths": [512, 1024, 2048],
                "num_trials": 10,
                "warmup_steps": 3,
                "device": "cuda" if torch.cuda.is_available() else "cpu"
            },
            "per-head-analysis": {
                "analyze_positions": True,
                "compute_confidence": True,
                "head_comparison": True,
                "position_buckets": [(0, 128), (128, 512), (512, 1024), (1024, 2048)],
                "batch_size": recipe.data.eval.batch_size,
                "device": "cuda" if torch.cuda.is_available() else "cpu"
            },
            "token-accuracy": {
                "position_range": (0, 100),
                "token_types": ["code", "text", "special"],
                "accuracy_threshold": 0.5,
                "granularity": 10,
                "analyze_token_types": True,
                "batch_size": recipe.data.eval.batch_size,
                "device": "cuda" if torch.cuda.is_available() else "cpu"
            },
            # Phase 2 í‰ê°€ê¸° ì¶”ê°€
            "self-speculative": {
                "num_sequences": 100,
                "max_tokens": 512,
                "temperature": recipe.eval.sampling.temperature if hasattr(recipe.eval.sampling, 'temperature') else 0.8,
                "top_p": recipe.eval.sampling.top_p if hasattr(recipe.eval.sampling, 'top_p') else 0.95,
                "measure_speedup": True,
                "device": "cuda" if torch.cuda.is_available() else "cpu"
            },
            "perplexity-measurer": {
                "batch_size": recipe.data.eval.batch_size,
                "max_length": 2048,
                "position_buckets": [[0, 128], [128, 512], [512, 1024], [1024, 2048]],
                "analyze_token_types": True,
                "compute_head_perplexity": True,
                "device": "cuda" if torch.cuda.is_available() else "cpu"
            },
            "metrics-visualizer": {
                "output_dir": "./figures",
                "save_formats": ["png", "pdf"],
                "use_plotly": True,
                "upload_to_mlflow": True,
                "figure_size": [10, 6]
            }
        }

        if eval_type not in eval_configs:
            raise ValueError(
                f"ì§€ì›ë˜ì§€ ì•ŠëŠ” í‰ê°€ íƒ€ì…: {eval_type}. "
                f"ì‚¬ìš© ê°€ëŠ¥í•œ ì˜µì…˜: {list(eval_configs.keys())}"
            )

        # í‰ê°€ê¸° ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        eval_config = eval_configs[eval_type]

        # Registryì—ì„œ í‰ê°€ê¸° ìƒì„±
        return evaluator_registry.create(eval_type, eval_config)
