"""
MPS (Metal Performance Shaders) ë°±ì—”ë“œ ìµœì í™” ìœ í‹¸ë¦¬í‹°
Apple Silicon GPUë¥¼ ìœ„í•œ 4D í…ì„œ ì²˜ë¦¬ ìµœì í™”

WMTP ì—°êµ¬ ë§¥ë½:
WMTP(Weighted Multi-Token Prediction)ëŠ” 4D í…ì„œ [B, S, H, V]ë¥¼ ì‚¬ìš©í•˜ì—¬
ì—¬ëŸ¬ ë¯¸ë˜ í† í°ì„ ë™ì‹œì— ì˜ˆì¸¡í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ MPS ë°±ì—”ë“œëŠ” 4Dâ†’2D view() ì—°ì‚°ì—ì„œ
ë¸”ë¡œí‚¹ì´ ë°œìƒí•˜ëŠ” ë¬¸ì œê°€ ìˆì–´, ì´ë¥¼ ìš°íšŒí•˜ëŠ” ìµœì í™”ê°€ í•„ìš”í•©ë‹ˆë‹¤.

í•µì‹¬ ê¸°ëŠ¥:
- ì„¤ì • ê¸°ë°˜ MPS ê²½ë¡œ ìë™ íŒë‹¨
- 4D í…ì„œ ìŠ¤íƒ ìµœì í™” (ì—°ì† ë©”ëª¨ë¦¬ ë³´ì¥)
- Cross-Entropy ê³„ì‚° ìµœì í™” (í—¤ë“œë³„ 3D ì²˜ë¦¬)

ì„±ëŠ¥ ìµœì í™”:
- MPSì—ì„œ 4Dâ†’2D flatten ëŒ€ì‹  í—¤ë“œë³„ 3D ì²˜ë¦¬ë¡œ ë¸”ë¡œí‚¹ íšŒí”¼
- ì—°ì† ë©”ëª¨ë¦¬ í• ë‹¹ìœ¼ë¡œ MPS ì»¤ë„ íš¨ìœ¨ì„± í–¥ìƒ
- CPU ëŒ€ë¹„ 2-3ë°° ì„±ëŠ¥ í–¥ìƒ ëª©í‘œ

ì‚¬ìš© ì˜ˆì‹œ:
    >>> from src.utils.mps_optimizer import MPSOptimizer
    >>>
    >>> # ì„¤ì • ê¸°ë°˜ MPS ê²½ë¡œ íŒë‹¨
    >>> config = {"launcher": {"resources": {"gpu_type": "mps"}}}
    >>> use_mps = MPSOptimizer.should_use_mps_path(config)
    >>>
    >>> # 4D í…ì„œ ìŠ¤íƒ ìµœì í™”
    >>> tensors = [torch.randn(2, 128, 50257) for _ in range(4)]
    >>> mtp_logits = MPSOptimizer.optimize_4d_stack(tensors, dim=2, use_mps=use_mps)
"""

from typing import Any, Callable

import torch


class MPSOptimizer:
    """MPS íŠ¹í™” í…ì„œ ì—°ì‚° ìµœì í™” í´ë˜ìŠ¤

    Apple Silicon GPUë¥¼ ìœ„í•œ WMTP 4D í…ì„œ ì²˜ë¦¬ ìµœì í™”ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
    ì„¤ì • ê¸°ë°˜ìœ¼ë¡œ ìë™ìœ¼ë¡œ MPS ìµœì í™” ê²½ë¡œë¥¼ ì„ íƒí•©ë‹ˆë‹¤.
    """

    @staticmethod
    def should_use_mps_path(config: dict[str, Any]) -> bool:
        """
        ì„¤ì • ê¸°ë°˜ MPS ê²½ë¡œ ì‚¬ìš© ì—¬ë¶€ ê²°ì •

        íŒë‹¨ ê¸°ì¤€ (OR ì¡°ê±´):
        1. launcher.resources.gpu_type == "mps"
        2. devices.compute_backend == "mps"
        3. ì‹¤ì œ deviceê°€ mps

        Args:
            config: ì „ì²´ ì„¤ì • ë”•ì…”ë„ˆë¦¬

        Returns:
            bool: MPS ìµœì í™” ê²½ë¡œ ì‚¬ìš© ì—¬ë¶€

        Example:
            >>> config = {"launcher": {"resources": {"gpu_type": "mps"}}}
            >>> MPSOptimizer.should_use_mps_path(config)
            True
        """
        # 1. launcherì˜ gpu_type ì²´í¬
        gpu_type = (
            config.get("launcher", {}).get("resources", {}).get("gpu_type", "")
        ).lower()

        # 2. devicesì˜ compute_backend ì²´í¬
        compute_backend = (config.get("devices", {}).get("compute_backend", "")).lower()

        # 3. ì‹¤ì œ device ì²´í¬ (ëŸ°íƒ€ì„)
        device_str = str(config.get("device", "")).lower()
        device_type = device_str.split(":")[0] if device_str else ""

        # 4. MPS ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        mps_available = torch.backends.mps.is_available()

        # ìµœì¢… íŒë‹¨
        use_mps = (
            gpu_type == "mps" or compute_backend == "mps" or device_type == "mps"
        ) and mps_available

        if use_mps:
            print(
                f"ğŸ MPS optimization enabled (gpu_type={gpu_type}, "
                f"backend={compute_backend}, device={device_type})"
            )

        return use_mps

    @staticmethod
    def optimize_4d_stack(
        tensors: list[torch.Tensor], dim: int, use_mps: bool
    ) -> torch.Tensor:
        """
        MPS ìµœì í™”ëœ 4D í…ì„œ ìŠ¤íƒ

        MPS ë¬¸ì œ: torch.stack()ì´ ë¶ˆì—°ì† ë©”ëª¨ë¦¬ ìƒì„±ìœ¼ë¡œ ë¸”ë¡œí‚¹ ë°œìƒ
        í•´ê²°ì±…: ì—°ì† ë©”ëª¨ë¦¬ì— ì§ì ‘ í• ë‹¹ í›„ ë³µì‚¬

        Args:
            tensors: ìŠ¤íƒí•  3D í…ì„œ ë¦¬ìŠ¤íŠ¸ [B, S, V]
            dim: ìŠ¤íƒ ì°¨ì› (2 for MTP)
            use_mps: MPS ìµœì í™” ì‚¬ìš© ì—¬ë¶€

        Returns:
            torch.Tensor: 4D í…ì„œ [B, S, H, V]

        Example:
            >>> tensors = [torch.randn(2, 128, 50257) for _ in range(4)]
            >>> result = MPSOptimizer.optimize_4d_stack(tensors, dim=2, use_mps=True)
            >>> result.shape
            torch.Size([2, 128, 4, 50257])
        """
        if use_mps and dim == 2:
            # MPS ìµœì í™”: ì—°ì† ë©”ëª¨ë¦¬ í• ë‹¹ í›„ ë³µì‚¬
            B, S, V = tensors[0].shape
            H = len(tensors)

            # 1. ì—°ì† ë©”ëª¨ë¦¬ ë¯¸ë¦¬ í• ë‹¹
            result = torch.zeros(
                B, S, H, V, dtype=tensors[0].dtype, device=tensors[0].device
            )

            # 2. ê° í—¤ë“œ ì§ì ‘ ë³µì‚¬ (ë©”ëª¨ë¦¬ ì—°ì†ì„± ë³´ì¥)
            for i, tensor in enumerate(tensors):
                result[:, :, i, :] = tensor

            # 3. ëª…ì‹œì  ì—°ì†ì„± ë³´ì¥
            return result.contiguous()
        else:
            # ê¸°ì¡´ CUDA/CPU ë°©ì‹
            return torch.stack(tensors, dim=dim)

    @staticmethod
    def compute_ce_per_head_mps(
        logits: torch.Tensor, target_labels: torch.Tensor, ignore_index: int = -100
    ) -> torch.Tensor:
        """
        MPS ìµœì í™” Cross-Entropy ê³„ì‚°

        MPS ë¬¸ì œ: 4Dâ†’2D view() ì—°ì‚°ì—ì„œ ë¬´í•œ ë¸”ë¡œí‚¹
        í•´ê²°ì±…: í—¤ë“œë³„ 3D ì²˜ë¦¬ (view ì—†ì´)

        Args:
            logits: 4D í…ì„œ [B, S, H, V]
            target_labels: 3D í…ì„œ [B, S, H]
            ignore_index: ë¬´ì‹œí•  ë¼ë²¨ ì¸ë±ìŠ¤

        Returns:
            torch.Tensor: CE per head [B, S, H]

        Example:
            >>> logits = torch.randn(2, 128, 4, 50257)
            >>> labels = torch.randint(0, 50257, (2, 128, 4))
            >>> ce = MPSOptimizer.compute_ce_per_head_mps(logits, labels)
            >>> ce.shape
            torch.Size([2, 128, 4])
        """
        B, S, H, V = logits.shape
        ce_list = []

        # í—¤ë“œë³„ 3D ì²˜ë¦¬ (MPS ì¹œí™”ì )
        for h in range(H):
            # 3D ìŠ¬ë¼ì´ìŠ¤ ì¶”ì¶œ (view ì—†ìŒ!)
            logits_h = logits[:, :, h, :].contiguous()  # [B, S, V]
            labels_h = target_labels[:, :, h].contiguous()  # [B, S]

            # 3D Cross-Entropy (MPSê°€ ì˜ ì²˜ë¦¬)
            # transposeë¡œ [B, V, S] í˜•íƒœë¡œ ë³€ê²½
            ce_h = torch.nn.functional.cross_entropy(
                logits_h.transpose(1, 2),  # [B, V, S]
                labels_h,  # [B, S]
                ignore_index=ignore_index,
                reduction="none",
            )  # [B, S]

            ce_list.append(ce_h)

        # í—¤ë“œ ì°¨ì›ìœ¼ë¡œ ìŠ¤íƒ
        ce_per_head = torch.stack(ce_list, dim=2)  # [B, S, H]
        return ce_per_head.contiguous()

    @staticmethod
    def get_device_info() -> dict[str, Any]:
        """
        í˜„ì¬ ì‚¬ìš© ê°€ëŠ¥í•œ ë””ë°”ì´ìŠ¤ ì •ë³´ ë°˜í™˜

        Returns:
            dict: ë””ë°”ì´ìŠ¤ ì •ë³´ ë”•ì…”ë„ˆë¦¬
                - mps_available: MPS ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€
                - cuda_available: CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€
                - current_device: í˜„ì¬ ì¶”ì²œ ë””ë°”ì´ìŠ¤
                - device_name: ë””ë°”ì´ìŠ¤ ì´ë¦„

        Example:
            >>> info = MPSOptimizer.get_device_info()
            >>> print(info['current_device'])
            mps
        """
        info = {
            "mps_available": torch.backends.mps.is_available(),
            "cuda_available": torch.cuda.is_available(),
            "current_device": "cpu",
            "device_name": "CPU",
        }

        if info["mps_available"]:
            info["current_device"] = "mps"
            info["device_name"] = "Apple Silicon GPU"
        elif info["cuda_available"]:
            info["current_device"] = "cuda"
            info["device_name"] = (
                torch.cuda.get_device_name(0)
                if torch.cuda.device_count() > 0
                else "CUDA"
            )

        return info

    @staticmethod
    def validate_tensor_compatibility(tensor: torch.Tensor, device_type: str) -> bool:
        """
        í…ì„œê°€ íŠ¹ì • ë””ë°”ì´ìŠ¤ì™€ í˜¸í™˜ë˜ëŠ”ì§€ í™•ì¸

        Args:
            tensor: í™•ì¸í•  í…ì„œ
            device_type: ëŒ€ìƒ ë””ë°”ì´ìŠ¤ íƒ€ì… ("mps", "cuda", "cpu")

        Returns:
            bool: í˜¸í™˜ì„± ì—¬ë¶€

        Example:
            >>> tensor = torch.randn(2, 3, device="cpu")
            >>> MPSOptimizer.validate_tensor_compatibility(tensor, "mps")
            True
        """
        # í…ì„œ ë””ë°”ì´ìŠ¤ í™•ì¸
        tensor_device = str(tensor.device).split(":")[0]

        # ë””ë°”ì´ìŠ¤ í˜¸í™˜ì„± ì²´í¬
        if tensor_device != device_type and device_type != "cpu":
            print(f"âš ï¸ Tensor device ({tensor_device}) != target device ({device_type})")
            return False

        # MPS íŠ¹ë³„ ì²˜ë¦¬
        if device_type == "mps":
            # MPSëŠ” íŠ¹ì • dtypeë§Œ ì§€ì›
            supported_dtypes = [torch.float32, torch.float16]
            if tensor.dtype not in supported_dtypes:
                print(f"âš ï¸ MPS does not support {tensor.dtype}. Converting to float32.")
                return False

            # í…ì„œ í¬ê¸° ì œí•œ í™•ì¸ (MPSëŠ” ë§¤ìš° í° í…ì„œì—ì„œ ë¬¸ì œ ë°œìƒ ê°€ëŠ¥)
            if tensor.numel() > 1e9:  # 1B elements
                print(f"âš ï¸ Tensor too large for MPS: {tensor.numel()} elements")
                return False

        return True

    @staticmethod
    def benchmark_operation(
        operation_func: Callable[..., Any], *args, num_iterations: int = 10, warmup: int = 3
    ) -> float:
        """
        íŠ¹ì • ì—°ì‚°ì˜ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

        Args:
            operation_func: ë²¤ì¹˜ë§ˆí¬í•  í•¨ìˆ˜
            *args: í•¨ìˆ˜ ì¸ìë“¤
            num_iterations: ì¸¡ì • ë°˜ë³µ íšŸìˆ˜
            warmup: ì›Œë°ì—… ë°˜ë³µ íšŸìˆ˜

        Returns:
            float: í‰ê·  ì‹¤í–‰ ì‹œê°„ (ì´ˆ)

        Example:
            >>> def test_op(x): return x * 2
            >>> tensor = torch.randn(1000, 1000, device="mps")
            >>> time = MPSOptimizer.benchmark_operation(test_op, tensor)
            >>> print(f"Average time: {time:.4f}s")
        """
        import time

        # Warmup
        for _ in range(warmup):
            operation_func(*args)
            if torch.cuda.is_available():
                torch.cuda.synchronize()
            elif torch.backends.mps.is_available():
                torch.mps.synchronize()

        # Benchmark
        times = []
        for _ in range(num_iterations):
            start = time.perf_counter()
            operation_func(*args)

            # ë™ê¸°í™” (GPU ì—°ì‚° ì™„ë£Œ ëŒ€ê¸°)
            if torch.cuda.is_available():
                torch.cuda.synchronize()
            elif torch.backends.mps.is_available():
                torch.mps.synchronize()

            end = time.perf_counter()
            times.append(end - start)

        return sum(times) / len(times)
