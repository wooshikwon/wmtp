"""
ë¶„ì‚° S3 ë‹¤ìš´ë¡œë“œ ì‹œìŠ¤í…œ - ëŒ€ìš©ëŸ‰ ëª¨ë¸ ê³ ì† ì „ì†¡

í•µì‹¬ ìµœì í™”:
1. Range ìš”ì²­ ê¸°ë°˜ ì²­í¬ ë¶„í•  ë‹¤ìš´ë¡œë“œ
2. ë©€í‹°ìŠ¤ë ˆë“œ/ë©€í‹°í”„ë¡œì„¸ìŠ¤ ë³‘ë ¬ ì²˜ë¦¬
3. ë™ì  ì›Œì»¤ ìˆ˜ ì¡°ì •
4. ì¤‘ë‹¨ëœ ë‹¤ìš´ë¡œë“œ ì¬ê°œ ì§€ì›
5. ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ìŠ¤íŠ¸ë¦¬ë°
"""

import hashlib
import json
import multiprocessing as mp
import os
import time
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed
from pathlib import Path
from threading import Lock
from typing import Any

import boto3
import psutil
from botocore.config import Config
from rich.console import Console
from rich.progress import (
    BarColumn,
    DownloadColumn,
    Progress,
    SpinnerColumn,
    TaskID,
    TextColumn,
    TimeRemainingColumn,
    TransferSpeedColumn,
)
from rich.table import Table

console = Console()


class DistributedS3Transfer:
    """ë¶„ì‚° S3 ì „ì†¡ í´ë˜ìŠ¤ - ìµœëŒ€ ì„±ëŠ¥ ì¶”ì¶œ"""

    def __init__(
        self,
        s3_client=None,
        bucket: str = None,
        max_workers: int | None = None,
        use_multiprocess: bool = False,
        chunk_size_mb: int = 50,
        enable_acceleration: bool = True,
    ):
        """
        Args:
            s3_client: boto3 S3 client (ì—†ìœ¼ë©´ ìë™ ìƒì„±)
            bucket: S3 ë²„í‚· ì´ë¦„
            max_workers: ìµœëŒ€ ì›Œì»¤ ìˆ˜ (Noneì´ë©´ ìë™ ê²°ì •)
            use_multiprocess: ë©€í‹°í”„ë¡œì„¸ìŠ¤ ì‚¬ìš© ì—¬ë¶€ (CPU ì§‘ì•½ì  ì‘ì—…ìš©)
            chunk_size_mb: ì²­í¬ í¬ê¸° (MB)
            enable_acceleration: S3 Transfer Acceleration ì‚¬ìš© ì—¬ë¶€
        """
        self.bucket = bucket or os.getenv("S3_BUCKET_NAME", "wmtp")

        # S3 í´ë¼ì´ì–¸íŠ¸ ì„¤ì • (ì—°ê²° í’€ í™•ëŒ€)
        if s3_client is None:
            config = Config(
                max_pool_connections=100,  # ì—°ê²° í’€ í¬ê¸° ì¦ê°€
                retries={
                    "max_attempts": 3,
                    "mode": "adaptive",  # ì ì‘í˜• ì¬ì‹œë„
                },
            )

            # Transfer Acceleration ì—”ë“œí¬ì¸íŠ¸ ì‚¬ìš©
            if enable_acceleration:
                self.s3_client = boto3.client(
                    "s3",
                    aws_access_key_id=os.getenv("AWS_ACCESS_KEY_ID"),
                    aws_secret_access_key=os.getenv("AWS_SECRET_ACCESS_KEY"),
                    region_name=os.getenv("AWS_DEFAULT_REGION", "eu-north-1"),
                    config=config,
                    endpoint_url=f"https://{self.bucket}.s3-accelerate.amazonaws.com",
                )
            else:
                self.s3_client = boto3.client(
                    "s3",
                    aws_access_key_id=os.getenv("AWS_ACCESS_KEY_ID"),
                    aws_secret_access_key=os.getenv("AWS_SECRET_ACCESS_KEY"),
                    region_name=os.getenv("AWS_DEFAULT_REGION", "eu-north-1"),
                    config=config,
                )
        else:
            self.s3_client = s3_client

        # ì›Œì»¤ ìˆ˜ ìë™ ê²°ì • (ë©”ëª¨ë¦¬ ì•ˆì „ ì„¤ì •)
        if max_workers is None:
            cpu_count = mp.cpu_count()
            # ë©”ëª¨ë¦¬ ì•ˆì „ì„ ìœ„í•´ ë³´ìˆ˜ì ìœ¼ë¡œ ì„¤ì •
            self.max_workers = min(cpu_count, 8)  # ìµœëŒ€ 8ê°œ ì›Œì»¤ë¡œ ì œí•œ
        else:
            self.max_workers = min(max_workers, 8)  # ì‚¬ìš©ì ì§€ì •ë„ 8ê°œë¡œ ì œí•œ

        self.use_multiprocess = use_multiprocess
        self.chunk_size = chunk_size_mb * 1024 * 1024  # MB to bytes

        # ì§„í–‰ë¥  ì¶”ì ìš© ë½
        self.progress_lock = Lock()
        self.download_stats = {}

        # ì¬ê°œ ê°€ëŠ¥í•œ ë‹¤ìš´ë¡œë“œë¥¼ ìœ„í•œ ë©”íƒ€ë°ì´í„° ì €ì¥
        self.metadata_dir = Path(".wmtp_download_cache")
        self.metadata_dir.mkdir(exist_ok=True)

    def get_optimal_workers(self, file_size: int) -> int:
        """íŒŒì¼ í¬ê¸°ì— ë”°ë¥¸ ìµœì  ì›Œì»¤ ìˆ˜ ê²°ì • (ë©”ëª¨ë¦¬ ì•ˆì „)"""
        size_gb = file_size / (1024**3)

        if size_gb < 1:
            return min(2, self.max_workers)  # ì‘ì€ íŒŒì¼: 2 ì›Œì»¤
        elif size_gb < 5:
            return min(4, self.max_workers)  # ì¤‘ê°„ íŒŒì¼: 4 ì›Œì»¤
        elif size_gb < 10:
            return min(6, self.max_workers)  # í° íŒŒì¼: 6 ì›Œì»¤
        else:
            return min(8, self.max_workers)  # ì´ˆëŒ€í˜• íŒŒì¼: ìµœëŒ€ 8 ì›Œì»¤

    def get_file_info(self, s3_key: str) -> dict[str, Any]:
        """S3 íŒŒì¼ ì •ë³´ ì¡°íšŒ"""
        try:
            response = self.s3_client.head_object(Bucket=self.bucket, Key=s3_key)
            return {
                "size": response["ContentLength"],
                "etag": response.get("ETag", "").strip('"'),
                "last_modified": response.get("LastModified"),
                "content_type": response.get("ContentType"),
            }
        except Exception as e:
            console.print(f"[red]Error getting file info for {s3_key}: {e}[/red]")
            return None

    def download_chunk(
        self,
        s3_key: str,
        start_byte: int,
        end_byte: int,
        chunk_file: Path,
        progress: Progress | None = None,
        task_id: TaskID | None = None,
    ) -> tuple[bool, str | None]:
        """ë‹¨ì¼ ì²­í¬ ë‹¤ìš´ë¡œë“œ"""
        try:
            # Range ìš”ì²­ìœ¼ë¡œ íŠ¹ì • ë°”ì´íŠ¸ ë²”ìœ„ë§Œ ë‹¤ìš´ë¡œë“œ
            response = self.s3_client.get_object(
                Bucket=self.bucket, Key=s3_key, Range=f"bytes={start_byte}-{end_byte}"
            )

            # ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì“°ê¸° (ì‘ì€ ë²„í¼ ì‚¬ìš©)
            with open(chunk_file, "wb") as f:
                for chunk in response["Body"].iter_chunks(
                    chunk_size=256 * 1024
                ):  # 256KBì”© (ë©”ëª¨ë¦¬ ì ˆì•½)
                    f.write(chunk)
                    if progress and task_id:
                        with self.progress_lock:
                            progress.update(task_id, advance=len(chunk))

            return True, None

        except Exception as e:
            error_msg = f"Failed to download chunk {start_byte}-{end_byte}: {e}"
            return False, error_msg

    def merge_chunks(self, chunk_files: list[Path], output_file: Path) -> bool:
        """ì²­í¬ íŒŒì¼ë“¤ì„ í•˜ë‚˜ë¡œ ë³‘í•©"""
        try:
            with open(output_file, "wb") as output:
                for chunk_file in sorted(chunk_files):
                    with open(chunk_file, "rb") as chunk:
                        # íš¨ìœ¨ì ì¸ íŒŒì¼ ë³µì‚¬
                        while True:
                            data = chunk.read(1024 * 1024 * 10)  # 10MB ë²„í¼
                            if not data:
                                break
                            output.write(data)
                    # ì²­í¬ íŒŒì¼ ì‚­ì œ
                    chunk_file.unlink()
            return True
        except Exception as e:
            console.print(f"[red]Error merging chunks: {e}[/red]")
            return False

    def download_file_distributed(
        self,
        s3_key: str,
        local_path: Path,
        progress: Progress | None = None,
        resume: bool = True,
    ) -> tuple[bool, str | None]:
        """ë¶„ì‚° ë‹¤ìš´ë¡œë“œ ë©”ì¸ í•¨ìˆ˜"""

        # íŒŒì¼ ì •ë³´ ì¡°íšŒ
        file_info = self.get_file_info(s3_key)
        if not file_info:
            return False, f"Failed to get file info for {s3_key}"

        file_size = file_info["size"]
        etag = file_info["etag"]

        # ë©”íƒ€ë°ì´í„° íŒŒì¼ ê²½ë¡œ
        meta_file = (
            self.metadata_dir / f"{hashlib.md5(s3_key.encode()).hexdigest()}.json"
        )

        # ì¬ê°œ ê°€ëŠ¥í•œ ë‹¤ìš´ë¡œë“œ í™•ì¸
        completed_chunks = set()
        if resume and meta_file.exists():
            with open(meta_file) as f:
                meta = json.load(f)
                if meta.get("etag") == etag:
                    completed_chunks = set(meta.get("completed_chunks", []))
                    console.print(
                        f"[yellow]Resuming download: {len(completed_chunks)} chunks already done[/yellow]"
                    )

        # ìµœì  ì›Œì»¤ ìˆ˜ ê²°ì •
        num_workers = self.get_optimal_workers(file_size)

        # ì²­í¬ ë¶„í•  ê³„ì‚°
        num_chunks = max(1, (file_size + self.chunk_size - 1) // self.chunk_size)
        num_workers = min(num_workers, num_chunks)  # ì²­í¬ ìˆ˜ë³´ë‹¤ ë§ì€ ì›Œì»¤ëŠ” ë¶ˆí•„ìš”

        console.print(f"[cyan]ğŸ“Š File: {s3_key}[/cyan]")
        console.print(f"[cyan]   Size: {file_size/(1024**3):.2f}GB[/cyan]")
        console.print(
            f"[cyan]   Chunks: {num_chunks} Ã— {self.chunk_size/(1024**2):.0f}MB[/cyan]"
        )
        console.print(f"[cyan]   Workers: {num_workers} parallel downloads[/cyan]")

        # ì²­í¬ íŒŒì¼ ì¤€ë¹„
        temp_dir = local_path.parent / f".tmp_{local_path.name}"
        temp_dir.mkdir(exist_ok=True, parents=True)

        # ì§„í–‰ë¥  í‘œì‹œ
        task_id = None
        if progress:
            task_id = progress.add_task(
                f"ğŸ“¥ {local_path.name}",
                total=file_size - len(completed_chunks) * self.chunk_size,
            )

        # ì²­í¬ ë‹¤ìš´ë¡œë“œ ì‘ì—… ìƒì„±
        chunk_tasks = []
        for i in range(num_chunks):
            if i in completed_chunks:
                continue  # ì´ë¯¸ ì™„ë£Œëœ ì²­í¬ ìŠ¤í‚µ

            start_byte = i * self.chunk_size
            end_byte = min(start_byte + self.chunk_size - 1, file_size - 1)
            chunk_file = temp_dir / f"chunk_{i:06d}"

            chunk_tasks.append((i, start_byte, end_byte, chunk_file))

        # ë³‘ë ¬ ë‹¤ìš´ë¡œë“œ ì‹¤í–‰
        failed_chunks = []
        completed_new_chunks = []

        # ì‹¤í–‰ì ì„ íƒ (í”„ë¡œì„¸ìŠ¤ vs ìŠ¤ë ˆë“œ)
        Executor = ProcessPoolExecutor if self.use_multiprocess else ThreadPoolExecutor

        with Executor(max_workers=num_workers) as executor:
            futures = {
                executor.submit(
                    self.download_chunk,
                    s3_key,
                    start,
                    end,
                    chunk_file,
                    progress,
                    task_id,
                ): (chunk_id, chunk_file)
                for chunk_id, start, end, chunk_file in chunk_tasks
            }

            for future in as_completed(futures):
                chunk_id, chunk_file = futures[future]
                success, error = future.result()

                if success:
                    completed_new_chunks.append(chunk_id)
                    completed_chunks.add(chunk_id)

                    # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ (ì¬ê°œ ì§€ì›)
                    with open(meta_file, "w") as f:
                        json.dump(
                            {
                                "s3_key": s3_key,
                                "etag": etag,
                                "completed_chunks": list(completed_chunks),
                                "total_chunks": num_chunks,
                            },
                            f,
                        )
                else:
                    failed_chunks.append((chunk_id, error))

        # ë‹¤ìš´ë¡œë“œ ì„±ê³µ ì—¬ë¶€ í™•ì¸
        if failed_chunks:
            console.print(
                f"[red]âŒ Failed to download {len(failed_chunks)} chunks[/red]"
            )
            for chunk_id, error in failed_chunks:
                console.print(f"[red]   Chunk {chunk_id}: {error}[/red]")
            return False, "Some chunks failed to download"

        # ëª¨ë“  ì²­í¬ íŒŒì¼ ë³‘í•©
        console.print(f"[yellow]ğŸ”€ Merging {num_chunks} chunks...[/yellow]")
        all_chunk_files = [temp_dir / f"chunk_{i:06d}" for i in range(num_chunks)]

        if self.merge_chunks(all_chunk_files, local_path):
            # ì„ì‹œ ë””ë ‰í† ë¦¬ ë° ë©”íƒ€ë°ì´í„° ì •ë¦¬
            temp_dir.rmdir()
            meta_file.unlink(missing_ok=True)

            console.print(f"[green]âœ… Successfully downloaded: {local_path}[/green]")
            return True, None
        else:
            return False, "Failed to merge chunks"

    def download_directory_distributed(
        self,
        s3_prefix: str,
        local_dir: Path,
        file_pattern: str | None = None,
        show_progress: bool = True,
    ) -> tuple[bool, list[str]]:
        """ë””ë ‰í† ë¦¬ ì „ì²´ë¥¼ ë¶„ì‚° ë‹¤ìš´ë¡œë“œ"""

        # S3 ê°ì²´ ëª©ë¡ ì¡°íšŒ
        console.print(
            f"[cyan]ğŸ“‚ Listing objects in s3://{self.bucket}/{s3_prefix}[/cyan]"
        )

        files_to_download = []
        paginator = self.s3_client.get_paginator("list_objects_v2")

        for page in paginator.paginate(Bucket=self.bucket, Prefix=s3_prefix):
            if "Contents" not in page:
                continue

            for obj in page["Contents"]:
                key = obj["Key"]
                size = obj["Size"]

                # ë””ë ‰í† ë¦¬ ì œì™¸
                if key.endswith("/"):
                    continue

                # íŒ¨í„´ í•„í„°ë§
                if file_pattern and not Path(key).match(file_pattern):
                    continue

                files_to_download.append((key, size))

        if not files_to_download:
            console.print(f"[yellow]No files found in {s3_prefix}[/yellow]")
            return False, []

        # í†µê³„ í‘œì‹œ
        total_size = sum(size for _, size in files_to_download)
        console.print(
            f"[cyan]ğŸ“Š Found {len(files_to_download)} files, {total_size/(1024**3):.2f}GB total[/cyan]"
        )

        # íŒŒì¼ í¬ê¸°ë³„ë¡œ ì •ë ¬ (í° íŒŒì¼ ë¨¼ì € - ë¡œë“œ ë°¸ëŸ°ì‹±)
        files_to_download.sort(key=lambda x: x[1], reverse=True)

        downloaded_files = []
        failed_files = []

        # ì§„í–‰ë¥  í‘œì‹œ
        if show_progress:
            with Progress(
                SpinnerColumn(),
                TextColumn("[bold blue]{task.description}"),
                BarColumn(),
                "[progress.percentage]{task.percentage:>3.1f}%",
                DownloadColumn(),
                TransferSpeedColumn(),
                TimeRemainingColumn(),
                console=console,
                refresh_per_second=2,
            ) as progress:
                # ì „ì²´ ì§„í–‰ë¥ 
                overall_task = progress.add_task(
                    "ğŸ“¦ Overall Progress", total=len(files_to_download)
                )

                # ê° íŒŒì¼ ë‹¤ìš´ë¡œë“œ
                for s3_key, _file_size in files_to_download:
                    # ë¡œì»¬ ê²½ë¡œ ê³„ì‚°
                    relative_path = s3_key[len(s3_prefix) :].lstrip("/")
                    local_path = local_dir / relative_path
                    local_path.parent.mkdir(parents=True, exist_ok=True)

                    # ë¶„ì‚° ë‹¤ìš´ë¡œë“œ ì‹¤í–‰
                    success, error = self.download_file_distributed(
                        s3_key, local_path, progress=progress, resume=True
                    )

                    if success:
                        downloaded_files.append(str(local_path))
                    else:
                        failed_files.append((s3_key, error))

                    progress.update(overall_task, advance=1)
        else:
            # ì§„í–‰ë¥  ì—†ì´ ë‹¤ìš´ë¡œë“œ
            for s3_key, _file_size in files_to_download:
                relative_path = s3_key[len(s3_prefix) :].lstrip("/")
                local_path = local_dir / relative_path
                local_path.parent.mkdir(parents=True, exist_ok=True)

                success, error = self.download_file_distributed(
                    s3_key, local_path, resume=True
                )

                if success:
                    downloaded_files.append(str(local_path))
                else:
                    failed_files.append((s3_key, error))

        # ê²°ê³¼ ìš”ì•½
        if failed_files:
            console.print(
                f"[yellow]âš ï¸ {len(failed_files)} files failed to download[/yellow]"
            )
            for key, error in failed_files[:5]:  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ
                console.print(f"[red]   {key}: {error}[/red]")

        console.print(
            f"[green]âœ… Downloaded {len(downloaded_files)}/{len(files_to_download)} files[/green]"
        )

        return len(downloaded_files) > 0, downloaded_files

    def benchmark_download_speed(self, test_file_size_mb: int = 100) -> float:
        """ë‹¤ìš´ë¡œë“œ ì†ë„ ë²¤ì¹˜ë§ˆí¬"""
        test_key = f"benchmark/test_{test_file_size_mb}mb.bin"
        test_file = Path(f"/tmp/benchmark_test_{test_file_size_mb}mb.bin")

        console.print("[cyan]ğŸƒ Running download speed benchmark...[/cyan]")

        try:
            # í…ŒìŠ¤íŠ¸ íŒŒì¼ ìƒì„± (ì—†ìœ¼ë©´)
            try:
                self.get_file_info(test_key)
            except Exception:  # noqa: S110
                console.print("[yellow]Creating test file in S3...[/yellow]")
                test_data = os.urandom(test_file_size_mb * 1024 * 1024)
                self.s3_client.put_object(
                    Bucket=self.bucket, Key=test_key, Body=test_data
                )

            # ë‹¤ìš´ë¡œë“œ ì†ë„ ì¸¡ì •
            start_time = time.time()
            success, _ = self.download_file_distributed(
                test_key, test_file, resume=False
            )
            elapsed_time = time.time() - start_time

            if success:
                speed_mbps = (test_file_size_mb * 8) / elapsed_time
                console.print(
                    f"[green]âœ… Download speed: {speed_mbps:.2f} Mbps[/green]"
                )

                # ì •ë¦¬
                test_file.unlink(missing_ok=True)
                return speed_mbps
            else:
                console.print("[red]âŒ Benchmark failed[/red]")
                return 0.0

        except Exception as e:
            console.print(f"[red]âŒ Benchmark error: {e}[/red]")
            return 0.0

    def get_system_stats(self) -> dict[str, Any]:
        """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ìƒíƒœ í™•ì¸"""
        return {
            "cpu_percent": psutil.cpu_percent(interval=1),
            "memory_percent": psutil.virtual_memory().percent,
            "network_mbps": self.estimate_network_bandwidth(),
            "disk_free_gb": psutil.disk_usage("/").free / (1024**3),
        }

    def estimate_network_bandwidth(self) -> float:
        """ë„¤íŠ¸ì›Œí¬ ëŒ€ì—­í­ ì¶”ì •"""
        net_stats = psutil.net_io_counters()
        time.sleep(1)
        net_stats_after = psutil.net_io_counters()

        bytes_recv = net_stats_after.bytes_recv - net_stats.bytes_recv
        mbps = (bytes_recv * 8) / (1024 * 1024)
        return mbps

    def auto_optimize_settings(self):
        """ì‹œìŠ¤í…œ ìƒíƒœì— ë”°ë¼ ì„¤ì • ìë™ ìµœì í™”"""
        stats = self.get_system_stats()

        # CPU ì‚¬ìš©ë¥ ì´ ë†’ìœ¼ë©´ ì›Œì»¤ ìˆ˜ ê°ì†Œ
        if stats["cpu_percent"] > 80:
            self.max_workers = max(4, self.max_workers // 2)
            console.print(
                f"[yellow]High CPU usage, reducing workers to {self.max_workers}[/yellow]"
            )

        # ë©”ëª¨ë¦¬ ë¶€ì¡±ì‹œ ì²­í¬ í¬ê¸° ê°ì†Œ
        if stats["memory_percent"] > 85:
            self.chunk_size = self.chunk_size // 2
            console.print(
                f"[yellow]High memory usage, reducing chunk size to {self.chunk_size/(1024**2):.0f}MB[/yellow]"
            )

        # ë””ìŠ¤í¬ ê³µê°„ í™•ì¸
        if stats["disk_free_gb"] < 10:
            console.print(
                f"[red]âš ï¸ Low disk space: {stats['disk_free_gb']:.2f}GB free[/red]"
            )

        return stats


def display_download_summary(stats: dict[str, Any]):
    """ë‹¤ìš´ë¡œë“œ ìš”ì•½ ì •ë³´ í‘œì‹œ"""
    table = Table(title="Download Statistics", show_header=True)
    table.add_column("Metric", style="cyan")
    table.add_column("Value", style="green")

    table.add_row("Total Files", str(stats.get("total_files", 0)))
    table.add_row("Downloaded", str(stats.get("downloaded_files", 0)))
    table.add_row("Failed", str(stats.get("failed_files", 0)))
    table.add_row("Total Size", f"{stats.get('total_size_gb', 0):.2f} GB")
    table.add_row("Elapsed Time", f"{stats.get('elapsed_time', 0):.1f} seconds")
    table.add_row("Average Speed", f"{stats.get('avg_speed_mbps', 0):.2f} Mbps")

    console.print(table)
