"""
Early Stopping Utilities for WMTP Training.

ì—°êµ¬ì œì•ˆì„œ ê¸°ë°˜ ì¡°ê¸° ì¢…ë£Œ ì „ëµ êµ¬í˜„:
- Stage 1 (Value Head Pretraining): ë‹¤ì¤‘ ê¸°ì¤€ ì²´í¬ (loss, gradient, variance)
- Stage 2 (Main Training): Loss convergence ì²´í¬

Version 2.0 (Phase 1):
- Flexible mode: "any" (practical) | "all" (conservative) | "loss_only"
- Window-based gradient instability detection
- Improved state management with gradient history

Reference: PPO best practices (TRL/TRLX)
- patience=100 (100 steps without improvement)
- min_delta=1e-4 (minimum improvement threshold)
"""

from __future__ import annotations

from abc import ABC, abstractmethod
from collections import deque
from typing import Any


class BaseEarlyStopping(ABC):
    """ì¡°ê¸° ì¢…ë£Œ ì „ëµ ë² ì´ìŠ¤ í´ë˜ìŠ¤.

    ëª¨ë“  early stopping ì „ëµì˜ ê³µí†µ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
    """

    def __init__(self, config: dict[str, Any] | None = None):
        """Early stopping ì´ˆê¸°í™”.

        Args:
            config: ì¡°ê¸° ì¢…ë£Œ ì„¤ì • ë”•ì…”ë„ˆë¦¬
        """
        self.config = config or {}
        self.enabled = self.config.get("enabled", False)
        self.best_value: float | None = None
        self.counter = 0
        self.should_stop_flag = False
        self.stop_reason: str | None = None

    @abstractmethod
    def should_stop(self, metrics: dict[str, float]) -> bool:
        """ì¡°ê¸° ì¢…ë£Œ ì—¬ë¶€ë¥¼ íŒë‹¨.

        Args:
            metrics: í˜„ì¬ ìŠ¤í…ì˜ ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬

        Returns:
            ì¡°ê¸° ì¢…ë£Œ ì—¬ë¶€
        """
        pass

    def reset(self) -> None:
        """Early stopping ìƒíƒœ ì´ˆê¸°í™”."""
        self.best_value = None
        self.counter = 0
        self.should_stop_flag = False
        self.stop_reason = None

    def get_state(self) -> dict[str, Any]:
        """í˜„ì¬ ìƒíƒœë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜ (ì²´í¬í¬ì¸íŠ¸ ì €ì¥ìš©)."""
        return {
            "best_value": self.best_value,
            "counter": self.counter,
            "should_stop_flag": self.should_stop_flag,
            "stop_reason": self.stop_reason,
        }

    def load_state(self, state: dict[str, Any]) -> None:
        """ì²´í¬í¬ì¸íŠ¸ì—ì„œ ìƒíƒœ ë³µì›."""
        self.best_value = state.get("best_value")
        self.counter = state.get("counter", 0)
        self.should_stop_flag = state.get("should_stop_flag", False)
        self.stop_reason = state.get("stop_reason")


class LossEarlyStopping(BaseEarlyStopping):
    """Loss convergence ê¸°ë°˜ ì¡°ê¸° ì¢…ë£Œ (Stage 2 ì „ìš©).

    ì—°êµ¬ì œì•ˆì„œ Stage 2 ìš”êµ¬ì‚¬í•­:
    - Lossê°€ patience íšŸìˆ˜ë§Œí¼ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ ì¡°ê¸° ì¢…ë£Œ
    - min_delta ë¯¸ë§Œì˜ ê°œì„ ì€ ë¬´ì‹œ

    ì‚¬ìš© ì˜ˆì‹œ:
        >>> early_stopping = LossEarlyStopping({
        ...     "enabled": True,
        ...     "patience": 100,
        ...     "min_delta": 1e-5,
        ...     "monitor": "loss"
        ... })
        >>> for step in range(max_steps):
        ...     metrics = train_step()
        ...     if early_stopping.should_stop(metrics):
        ...         print(f"Early stopping: {early_stopping.stop_reason}")
        ...         break
    """

    def __init__(self, config: dict[str, Any] | None = None):
        """Loss early stopping ì´ˆê¸°í™”.

        Args:
            config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
                - enabled: ì¡°ê¸° ì¢…ë£Œ í™œì„±í™” (ê¸°ë³¸ê°’: False)
                - patience: ê°œì„  ì—†ì´ í—ˆìš©í•  ìŠ¤í… ìˆ˜ (ê¸°ë³¸ê°’: 100)
                - min_delta: ê°œì„ ìœ¼ë¡œ ê°„ì£¼í•  ìµœì†Œ ë³€í™”ëŸ‰ (ê¸°ë³¸ê°’: 1e-5)
                - monitor: ëª¨ë‹ˆí„°ë§í•  ë©”íŠ¸ë¦­ ì´ë¦„ (ê¸°ë³¸ê°’: "loss")
        """
        super().__init__(config)
        self.patience = self.config.get("patience", 100)
        self.min_delta = self.config.get("min_delta", 1e-5)
        self.monitor = self.config.get("monitor", "loss")

    def should_stop(self, metrics: dict[str, float]) -> bool:
        """Loss ê°œì„  ì—¬ë¶€ë¥¼ í™•ì¸í•˜ì—¬ ì¡°ê¸° ì¢…ë£Œ íŒë‹¨.

        Args:
            metrics: í˜„ì¬ ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬ (monitor key í•„ìˆ˜)

        Returns:
            ì¡°ê¸° ì¢…ë£Œ ì—¬ë¶€
        """
        if not self.enabled:
            return False

        # ëª¨ë‹ˆí„°ë§ ë©”íŠ¸ë¦­ ì¶”ì¶œ
        current_value = metrics.get(self.monitor)
        if current_value is None:
            # ë©”íŠ¸ë¦­ì´ ì—†ìœ¼ë©´ ì¡°ê¸° ì¢…ë£Œí•˜ì§€ ì•ŠìŒ
            return False

        # ì²« ë²ˆì§¸ ê°’ ì €ì¥
        if self.best_value is None:
            self.best_value = current_value
            return False

        # Loss ê°œì„  ì—¬ë¶€ í™•ì¸ (ì‘ì„ìˆ˜ë¡ ì¢‹ìŒ)
        improvement = self.best_value - current_value

        if improvement > self.min_delta:
            # ê°œì„ ë¨: ì¹´ìš´í„° ë¦¬ì…‹
            self.best_value = current_value
            self.counter = 0
        else:
            # ê°œì„  ì—†ìŒ: ì¹´ìš´í„° ì¦ê°€
            self.counter += 1

        # Patience ì´ˆê³¼ ì‹œ ì¡°ê¸° ì¢…ë£Œ
        if self.counter >= self.patience:
            self.should_stop_flag = True
            self.stop_reason = (
                f"Loss convergence: no improvement for {self.patience} steps "
                f"(best={self.best_value:.6f}, current={current_value:.6f})"
            )
            return True

        return False


class ValueHeadEarlyStopping(BaseEarlyStopping):
    """Value Head í•™ìŠµ ì¡°ê¸° ì¢…ë£Œ (Stage 1 ì „ìš©).

    ì—°êµ¬ì œì•ˆì„œ Stage 1 ìš”êµ¬ì‚¬í•­ (v2.0):
    1. Value loss convergence
    2. Gradient instability detection (window-based)
    3. Variance out of range detection

    Stopping Modes:
    - "any" (ê¸°ë³¸ê°’, ì‹¤ìš©ì ): ì¡°ê±´ ì¤‘ í•˜ë‚˜ë¼ë„ ë§Œì¡±í•˜ë©´ ì¤‘ë‹¨
    - "all" (ë³´ìˆ˜ì ): ëª¨ë“  ì¡°ê±´ì´ ë§Œì¡±í•´ì•¼ ì¤‘ë‹¨
    - "loss_only": Loss convergenceë§Œ ì²´í¬

    ì‚¬ìš© ì˜ˆì‹œ:
        >>> early_stopping = ValueHeadEarlyStopping({
        ...     "enabled": True,
        ...     "mode": "any",
        ...     "patience": 10,
        ...     "min_delta": 1e-4,
        ...     "monitor": "value_loss",
        ...     "grad_norm_threshold": 50.0,
        ...     "grad_norm_window_size": 10,
        ...     "grad_norm_threshold_ratio": 0.7,
        ...     "variance_min": 0.1,
        ...     "variance_max": 5.0
        ... })
        >>> for step in range(max_steps):
        ...     metrics = {
        ...         "value_loss": loss.item(),
        ...         "grad_norm": total_norm,
        ...         "value_variance": pred_variance
        ...     }
        ...     if early_stopping.should_stop(metrics):
        ...         reason = early_stopping.stop_reason
        ...         print(f"Early stopping: {reason}")
        ...         break
    """

    def __init__(self, config: dict[str, Any] | None = None):
        """Value Head early stopping ì´ˆê¸°í™”.

        Args:
            config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
                - enabled: ì¡°ê¸° ì¢…ë£Œ í™œì„±í™” (ê¸°ë³¸ê°’: False)
                - mode: ì¤‘ë‹¨ ëª¨ë“œ "any" | "all" | "loss_only" (ê¸°ë³¸ê°’: "any")
                - patience: Loss ê°œì„  ì—†ì´ í—ˆìš©í•  ìŠ¤í… ìˆ˜ (ê¸°ë³¸ê°’: 10)
                - min_delta: Loss ê°œì„  ìµœì†Œ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 1e-4)
                - monitor: ëª¨ë‹ˆí„°ë§í•  loss ë©”íŠ¸ë¦­ (ê¸°ë³¸ê°’: "value_loss")
                - grad_norm_threshold: Gradient norm ì„ê³„ê°’ (ê¸°ë³¸ê°’: 50.0)
                - grad_norm_window_size: Gradient ì²´í¬ ìœˆë„ìš° í¬ê¸° (ê¸°ë³¸ê°’: 10)
                - grad_norm_threshold_ratio: ìœˆë„ìš° ë‚´ ì´ˆê³¼ ë¹„ìœ¨ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.7)
                - variance_min: Value ë¶„ì‚° ìµœì†Œê°’ (ê¸°ë³¸ê°’: 0.1)
                - variance_max: Value ë¶„ì‚° ìµœëŒ€ê°’ (ê¸°ë³¸ê°’: 5.0)
        """
        super().__init__(config)

        # Mode ì„¤ì •
        self.mode = self.config.get("mode", "any")
        if self.mode not in ["any", "all", "loss_only"]:
            raise ValueError(
                f"Invalid mode: {self.mode}. Must be 'any', 'all', or 'loss_only'"
            )

        # Loss convergence ì„¤ì •
        self.patience = self.config.get("patience", 10)
        self.min_delta = self.config.get("min_delta", 1e-4)
        self.monitor = self.config.get("monitor", "value_loss")

        # Gradient instability ì„¤ì • (window-based)
        self.grad_norm_threshold = self.config.get("grad_norm_threshold", 50.0)
        self.grad_norm_window_size = self.config.get("grad_norm_window_size", 10)
        self.grad_norm_threshold_ratio = self.config.get(
            "grad_norm_threshold_ratio", 0.7
        )
        self.grad_norm_history: deque[bool] = deque(maxlen=self.grad_norm_window_size)

        # Variance range ì„¤ì •
        self.variance_min = self.config.get("variance_min", 0.1)
        self.variance_max = self.config.get("variance_max", 5.0)

    def should_stop(self, metrics: dict[str, float]) -> bool:
        """ì¢…í•©ì ì¸ ì¡°ê¸° ì¢…ë£Œ íŒë‹¨ (modeì— ë”°ë¼ ìœ ì—°í•˜ê²Œ ì²˜ë¦¬).

        Args:
            metrics: í˜„ì¬ ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬
                - {monitor}: Value loss (ì˜ˆ: "value_loss")
                - "grad_norm": Gradient norm (optional)
                - "value_variance": Value ì˜ˆì¸¡ ë¶„ì‚° (optional)

        Returns:
            ì¡°ê¸° ì¢…ë£Œ ì—¬ë¶€
        """
        if not self.enabled:
            return False

        # í•„ìˆ˜ ë©”íŠ¸ë¦­ í™•ì¸
        value_loss = metrics.get(self.monitor)
        grad_norm = metrics.get("grad_norm")
        value_variance = metrics.get("value_variance")

        if value_loss is None:
            # Loss ë©”íŠ¸ë¦­ì´ ì—†ìœ¼ë©´ ì¡°ê¸° ì¢…ë£Œí•˜ì§€ ì•ŠìŒ
            return False

        # ğŸ¯ ê° ì¡°ê±´ ë…ë¦½ì ìœ¼ë¡œ ì²´í¬
        loss_converged = self._check_loss_convergence(value_loss)
        grad_unstable = self._check_gradient_instability(grad_norm)
        variance_invalid = self._check_variance_invalid(value_variance)

        # ğŸ¯ Modeë³„ ì¤‘ë‹¨ ê²°ì • ë° ì´ìœ  ìˆ˜ì§‘
        should_stop = False
        reasons = []

        if self.mode == "any":
            # í•˜ë‚˜ë¼ë„ ë§Œì¡±í•˜ë©´ ì¤‘ë‹¨ (ì‹¤ìš©ì )
            if loss_converged:
                reasons.append(
                    f"loss converged ({value_loss:.6f}, patience={self.patience})"
                )
            if grad_unstable:
                reasons.append(
                    f"gradient unstable (threshold={self.grad_norm_threshold}, ratio={self.grad_norm_threshold_ratio})"
                )
            if variance_invalid:
                reasons.append(
                    f"variance out of range ({value_variance:.4f}, range=[{self.variance_min}, {self.variance_max}])"
                )

            should_stop = len(reasons) > 0

        elif self.mode == "all":
            # ëª¨ë‘ ë§Œì¡±í•´ì•¼ ì¤‘ë‹¨ (ë³´ìˆ˜ì )
            if loss_converged and not grad_unstable and not variance_invalid:
                reasons.append(
                    f"all conditions met: loss={value_loss:.6f}, grad stable, variance valid"
                )
                should_stop = True

        else:  # "loss_only"
            # Loss convergenceë§Œ ì²´í¬
            if loss_converged:
                reasons.append(
                    f"loss converged ({value_loss:.6f}, patience={self.patience})"
                )
                should_stop = True

        # ì¤‘ë‹¨ ê²°ì •
        if should_stop:
            self.should_stop_flag = True
            self.stop_reason = (
                f"Stage 1 early stop ({self.mode} mode): {'; '.join(reasons)}"
            )
            return True

        return False

    def _check_loss_convergence(self, current_loss: float) -> bool:
        """Loss convergence ì²´í¬ (LossEarlyStoppingê³¼ ë™ì¼ ë¡œì§).

        Args:
            current_loss: í˜„ì¬ loss ê°’

        Returns:
            Lossê°€ ìˆ˜ë ´í–ˆëŠ”ì§€ ì—¬ë¶€
        """
        # ì²« ë²ˆì§¸ ê°’ ì €ì¥
        if self.best_value is None:
            self.best_value = current_loss
            return False

        # Loss ê°œì„  ì—¬ë¶€ í™•ì¸
        improvement = self.best_value - current_loss

        if improvement > self.min_delta:
            # ê°œì„ ë¨: ì¹´ìš´í„° ë¦¬ì…‹
            self.best_value = current_loss
            self.counter = 0
            return False
        else:
            # ê°œì„  ì—†ìŒ: ì¹´ìš´í„° ì¦ê°€
            self.counter += 1

        # Patience ì´ˆê³¼ ì‹œ ìˆ˜ë ´ìœ¼ë¡œ íŒë‹¨
        return self.counter >= self.patience

    def _check_gradient_instability(self, grad_norm: float | None) -> bool:
        """Gradient ë¶ˆì•ˆì •ì„± ì²´í¬ (window-based).

        ìœˆë„ìš° ë‚´ ì¼ì • ë¹„ìœ¨ ì´ìƒì´ thresholdë¥¼ ì´ˆê³¼í•˜ë©´ ë¶ˆì•ˆì •ìœ¼ë¡œ íŒë‹¨.

        Args:
            grad_norm: Gradient L2 norm

        Returns:
            Gradientê°€ ë¶ˆì•ˆì •í•œì§€ ì—¬ë¶€ (Trueë©´ ì¡°ê¸° ì¢…ë£Œ ì‚¬ìœ )
        """
        if grad_norm is None or self.grad_norm_threshold is None:
            # Gradient normì´ ì—†ìœ¼ë©´ ì²´í¬í•˜ì§€ ì•ŠìŒ (ë¶ˆì•ˆì •í•˜ì§€ ì•ŠìŒ)
            return False

        # Windowì— ì´ˆê³¼ ì—¬ë¶€ ê¸°ë¡
        exceeds_threshold = grad_norm > self.grad_norm_threshold
        self.grad_norm_history.append(exceeds_threshold)

        # Windowê°€ ì°¨ê¸° ì „ì—” íŒë‹¨í•˜ì§€ ì•ŠìŒ
        if len(self.grad_norm_history) < self.grad_norm_window_size:
            return False

        # ìµœê·¼ window ë‚´ ì´ˆê³¼ ë¹„ìœ¨ ê³„ì‚°
        unstable_ratio = sum(self.grad_norm_history) / len(self.grad_norm_history)

        # ì¼ì • ë¹„ìœ¨ ì´ìƒ ì´ˆê³¼í•˜ë©´ ë¶ˆì•ˆì •
        return unstable_ratio >= self.grad_norm_threshold_ratio

    def _check_variance_invalid(self, variance: float | None) -> bool:
        """Variance ë²”ìœ„ ì´íƒˆ ì²´í¬.

        Args:
            variance: Value ì˜ˆì¸¡ ë¶„ì‚°

        Returns:
            ë¶„ì‚°ì´ ë²”ìœ„ë¥¼ ë²—ì–´ë‚¬ëŠ”ì§€ ì—¬ë¶€ (Trueë©´ ì¡°ê¸° ì¢…ë£Œ ì‚¬ìœ )
        """
        if variance is None:
            # Varianceê°€ ì—†ìœ¼ë©´ ì²´í¬í•˜ì§€ ì•ŠìŒ (ìœ íš¨í•¨)
            return False

        # ë²”ìœ„ë¥¼ ë²—ì–´ë‚¬ìœ¼ë©´ True (invalid)
        return not (self.variance_min <= variance <= self.variance_max)

    def reset(self) -> None:
        """Early stopping ìƒíƒœ ì´ˆê¸°í™” (gradient history í¬í•¨)."""
        super().reset()
        self.grad_norm_history.clear()

    def get_state(self) -> dict[str, Any]:
        """í˜„ì¬ ìƒíƒœë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜ (ì²´í¬í¬ì¸íŠ¸ ì €ì¥ìš©)."""
        state = super().get_state()
        state["grad_norm_history"] = list(self.grad_norm_history)  # deque â†’ list
        return state

    def load_state(self, state: dict[str, Any]) -> None:
        """ì²´í¬í¬ì¸íŠ¸ì—ì„œ ìƒíƒœ ë³µì›."""
        super().load_state(state)
        history = state.get("grad_norm_history", [])
        self.grad_norm_history = deque(history, maxlen=self.grad_norm_window_size)
