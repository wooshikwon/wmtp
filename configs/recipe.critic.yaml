# Recipe Configuration for WMTP Fine-Tuning Framework
# This file contains model training and evaluation settings

# Run metadata
run:
  name: "critic-wmtp_mbpp_exp1"
  tags:
    - "critic"
    - "wmtp"
    - "mbpp"
    - "baseline"

# Model configuration
model:
  base_id: "facebook/multi-token-prediction"
  rm_id: "sfair/Llama-3-8B-RM-Reward-Model"
  ref_id: "codellama/CodeLlama-7b-Python-hf"  # CodeLlama for better CE distribution matching
  tokenizer_pad_side: "right"
  mtp:
    n_heads: 4
    horizon: 4

# Training configuration
train:
  algo: "critic-wmtp"  # Options: critic-wmtp, rho1-wmtp
  full_finetune: true
  lora:
    enabled: false
    r: 16
    alpha: 32
    dropout: 0.05
    target_modules:
      - "q_proj"
      - "v_proj"
      - "o_proj"
      - "k_proj"
      - "gate_proj"
      - "up_proj"
      - "down_proj"

# Optimizer configuration
optim:
  optimizer: "adamw"
  lr: 1.2e-5
  weight_decay: 0.1
  betas: [0.9, 0.95]
  grad_clip: 1.0
  scheduler: "cosine"
  warmup_ratio: 0.03

# Data configuration
data:
  train:
    sources: ["mbpp"]
    max_length: 2048
    batch_size: 8
    pack_sequences: true
  eval:
    sources: ["mbpp", "contest"]
    max_length: 2048
    batch_size: 8
    pack_sequences: false

# Batching configuration
batching:
  global_batch_tokens: 4000000  # 4M tokens per batch
  micro_batch_size: 1
  grad_accum_steps: 64

# Loss configuration
loss:
  weight_norm: "mean1.0_clip"
  lambda: 0.3
  temperature: 0.7
  epsilon: 0.05
  max_weight: 3.0

# Critic-specific configuration (used when algo=critic-wmtp)
critic:
  target: "rm_sequence"
  token_spread: "gae"  # Options: uniform, length, attention, gae
  delta_mode: "td"  # Options: td, diff
  normalize: "zscore"  # Options: zscore, minmax, none

# Evaluation configuration
eval:
  protocol: "meta-mtp"
  sampling:
    temperature: 0.2
    top_p: 0.95
    n: 1
  metrics:
    - "mbpp_exact"
    - "contest_pass@1"
    - "contest_pass@5"
