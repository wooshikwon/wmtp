# Critic WMTP 실험 - 10분 내 완료용
# 가치 함수 기반 토큰 가중치를 사용하는 WMTP

run:
  name: "critic_quick_exp"
  tags:
    - "critic"
    - "wmtp"
    - "quick"
    - "comparison"

# Facebook MTP + Reward Model 사용
model:
  base_id: "facebook/multi-token-prediction"
  rm_id: "sfair/Llama-3-8B-RM-Reward-Model"    # Critic용 RM
  ref_id: "codellama/CodeLlama-7b-Python-hf"
  tokenizer_pad_side: "right"
  mtp:
    n_heads: 4
    horizon: 4

# Critic 알고리즘 훈련 설정
train:
  algo: "critic-wmtp"
  full_finetune: true
  lora:
    enabled: false

# 적당한 학습률 (가중치 학습 고려)
optim:
  optimizer: "adamw"
  lr: 5.0e-5              # 중간 학습률
  weight_decay: 0.05
  betas: [0.9, 0.95]
  grad_clip: 1.0
  scheduler: "constant"
  warmup_ratio: 0.0

# 작은 데이터셋으로 빠른 실험
data:
  train:
    sources: ["mbpp"]
    max_length: 512
    batch_size: 2
    pack_sequences: false
  eval:
    sources: ["mbpp"]
    max_length: 512
    batch_size: 4
    pack_sequences: false

# 작은 배치 설정
batching:
  global_batch_tokens: 50000
  micro_batch_size: 1
  grad_accum_steps: 4

# WMTP 손실 - 토큰 가중치 적용
loss:
  weight_norm: "mean1.0_clip"   # 평균 1.0으로 정규화, 클리핑
  lambda: 0.3                   # 가중치 강도
  temperature: 0.7              # 온도 스케일링
  epsilon: 0.05
  max_weight: 3.0               # 최대 3배 증폭

# Critic 특화 설정 - 가치 함수 기반 가중치
critic:
  target: "rm_sequence"         # RM 시퀀스 레벨 점수 사용
  token_spread: "gae"           # GAE(Generalized Advantage Estimation)로 토큰 분배
  delta_mode: "td"              # Temporal Difference 방식
  normalize: "zscore"           # Z-score 정규화

# 평가 설정
eval:
  protocol: "meta-mtp"
  sampling:
    temperature: 0.2            # 낮은 온도 (더 확정적)
    top_p: 0.95
    n: 1
  metrics:
    - "mbpp_exact"
