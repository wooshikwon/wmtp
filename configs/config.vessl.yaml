# VESSL GPU Production Configuration for Critic WMTP Algorithm
# Optimized for VESSL cluster with A100 x4 GPUs

# Project metadata
project: "wmtp_prod_critic"
seed: 42
log_interval: 100  # Production: 100 step마다 출력

# Path configurations - Container mounted paths
paths:
  models:
    base: "file:///app/models/llama-7b-mtp/"
    rm: "file:///app/models/starling-rm-7b/"
    ref: "file:///app/models/sheared-llama-2.7b/"
  datasets:
    mbpp: "file:///app/datasets/mbpp"
    contest: "file:///app/datasets/contest"
    humaneval: "file:///app/datasets/humaneval"
  checkpoints:
    base_path: "s3://wmtp/checkpoints"  # S3 production checkpoint storage
    save_interval: 1000                  # Save every 100 steps for production
    keep_last: 5                       # Keep last 5 checkpoints for recovery
    save_final: true                   # Save final production model

# MLflow configuration - Production tracking
mlflow:
  experiment: "wmtp/prod"
  tracking_uri: "s3://wmtp/mlruns"
  registry_uri: "s3://wmtp/mlruns"

# Launcher configuration - VESSL A100x4
launcher:
  target: "vessl"
  resources:
    gpus: 4
    gpu_type: "A100"
    cpu_limit: "32"
    memory_limit: "256Gi"
    shm_size: "32Gi"

# Device configuration for A100 GPUs
devices:
  compute_backend: "cuda"
  mixed_precision: "bf16"  # A100 supports bfloat16
  num_proc: 8  # Tokenizer 병렬 처리용 CPU 프로세스 수 (A100x4 환경)
  distributed:
    enabled: true                 # 분산 학습 활성화
    backend: "nccl"               # GPU 간 고속 통신
    init_method: "env://"         # torchrun 환경변수 사용
    timeout: 1800                 # NCCL 타임아웃 (30분)
    find_unused_parameters: false # FSDP와 함께 사용 시 False

  # 기존 FSDP 설정 유지
  fsdp:
    enabled: true
    auto_wrap: true
    activation_ckpt: true
    sharding: "full_shard"
    offload_params: false
    offload_grads: false
