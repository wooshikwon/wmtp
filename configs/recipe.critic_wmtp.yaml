# Production Recipe - Critic WMTP Algorithm
# Value function delta-based dynamic weighting on A100x4

# Run metadata
run:
  name: "prod_critic_wmtp"
  tags:
    - "production"
    - "vessl"
    - "critic"
    - "wmtp"
    - "a100"
    - "value-function"

# Data configuration
data:
  train:
    sources: ["mbpp", "contest"]
    max_length: 2048
    batch_size: 2  # Per GPU: 2 x 4 = 8 total
    pack_sequences: true
  eval:
    sources: ["mbpp", "humaneval"]
    max_length: 2048
    batch_size: 2
    pack_sequences: false

# Pretraining configuration (Stage 1: Value Head Pretraining)
pretrain:
  enabled: true
  num_epochs: 3
  max_steps: 10000
  lr: 1e-4
  save_value_head: true

  early_stopping:
    enabled: true
    mode: "loss_only"  # Most reliable: loss convergence only
    patience: 50
    min_delta: 1e-4
    monitor: "value_loss"

# Training configuration (Stage 2: Main Training)
train:
  algo: "critic-wmtp"
  full_finetune: true
  num_epochs: 3  # SFT 표준: 데이터 3회 반복
  max_steps: 30000  # 안전장치 (10K samples * 3 epochs = 30K steps 예상)

  early_stopping:
    enabled: true
    patience: 200
    min_delta: 1e-5
    monitor: "wmtp_loss"

# Critic-specific configuration
critic:
  target: "rm_sequence"
  token_spread: "gae"
  delta_mode: "td"
  normalize: "zscore"
  discount_lambda: 0.95
  gamma: 0.99
  gae_lambda: 0.95
  auxiliary_loss_coef: 0.1
  value_lr: 2.0e-6
  use_pseudo_rewards: true

# Optimizer configuration
optim:
  optimizer: "adamw"
  lr: 2.0e-5
  weight_decay: 0.01
  betas: [0.9, 0.999]
  grad_clip: 1.0
  scheduler: "cosine"
  warmup_ratio: 0.03

# Loss configuration
loss:
  weight_norm: "mean1.0_clip"
  lambda: 0.3
  weight_temperature: 0.7
  epsilon: 0.05
  max_weight: 3.0

# Evaluation configuration
eval:
  protocol: "meta-mtp"
  sampling:
    temperature: 0.7
    top_p: 0.9
    n: 20
  metrics:
    - "mbpp_exact"
    - "humaneval_exact"
    - "contest_exact"
