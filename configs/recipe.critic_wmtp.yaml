# Production Recipe - Critic WMTP Algorithm
# Value-based dynamic weighting with Stage 1 pretraining on A100x2

# Run metadata
run:
  name: "prod_critic_wmtp"
  tags:
    - "production"
    - "vessl"
    - "critic"
    - "wmtp"
    - "a100"
    - "two-stage"

# Model configuration
model:
  base_id: "mtp-7b-1t-4"  # Facebook MTP 7B model
  tokenizer_type: "hf"
  tokenizer_pad_side: "right"
  mtp:
    n_heads: 4
    horizon: 4

# Training configuration
train:
  algo: "critic-wmtp"  # Value-based dynamic weighting
  full_finetune: true
  max_steps: 12000  # Longer for two-stage training
  eval_interval: 500
  save_interval: 1000
  checkpointing:
    save_interval: 1000
    keep_last: 5
    save_final: true
  # Stage 1 configuration
  stage1:
    enabled: true
    max_steps: 2000
    lr: 1.0e-4
    save_value_head: true

# Optimizer configuration
optim:
  optimizer: "adamw"
  lr: 1.5e-5  # Slightly lower for critic
  weight_decay: 0.01
  betas: [0.9, 0.999]
  grad_clip: 1.0
  scheduler: "cosine"
  warmup_ratio: 0.03

# Data configuration
data:
  train:
    sources: ["mbpp", "contest"]
    max_length: 2048
    batch_size: 4  # Per GPU: 4 x 2 = 8 total
    pack_sequences: true
    num_workers: 8
  eval:
    sources: ["mbpp", "humaneval"]
    max_length: 2048
    batch_size: 4
    pack_sequences: false
    num_workers: 4

# Loss configuration - critic with value loss
loss:
  weight_norm: "softmax"
  lambda: 0.01
  temperature: 1.0
  epsilon: 1.0e-8
  max_weight: 10.0
  # Value loss configuration
  value_coef: 0.1  # V_loss weight
  gamma: 0.99      # GAE discount
  gae_lambda: 0.95 # GAE lambda

# Evaluation configuration
eval:
  protocol: "meta-mtp"
  sampling:
    temperature: 0.7
    top_p: 0.9
    n: 20  # For pass@k evaluation
  metrics:
    - "mbpp_exact"
    - "humaneval_exact"
    - "contest_exact"