#!/usr/bin/env python3
"""
WMTP ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ ë¹„êµ ë¶„ì„ í”„ë ˆì„ì›Œí¬

"Not All Tokens Are What You Need" ì² í•™ì„ ë°”íƒ•ìœ¼ë¡œ í•œ
ì„¸ ê°€ì§€ WMTP ì•Œê³ ë¦¬ì¦˜ì˜ ì„±ëŠ¥ì„ ì¢…í•©ì ìœ¼ë¡œ ë¹„êµ ë¶„ì„í•©ë‹ˆë‹¤.

ë¶„ì„ ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜:
1. MTP Baseline: ê· ë“±í•œ í† í° ê°€ì¤‘ì¹˜ (ë¹„êµ ê¸°ì¤€)
2. Critic WMTP: Value Function ê¸°ë°˜ ë™ì  í† í° ê°€ì¤‘ì¹˜
3. Rho-1 WMTP: Reference Model ì°¨ì´ ê¸°ë°˜ í† í° ê°€ì¤‘ì¹˜
"""

import json
from dataclasses import dataclass
from datetime import datetime
from typing import Any

import mlflow
from rich.console import Console
from rich.panel import Panel
from rich.progress import Progress, SpinnerColumn, TextColumn
from rich.table import Table

console = Console()


@dataclass
class ExperimentResult:
    """ë‹¨ì¼ ì‹¤í—˜ ê²°ê³¼ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤."""

    algorithm: str
    run_id: str
    run_name: str
    start_time: datetime
    end_time: datetime | None
    duration_seconds: float
    status: str
    final_loss: float | None
    metrics: dict[str, float]
    parameters: dict[str, Any]
    tags: dict[str, str]


@dataclass
class ComparisonReport:
    """ì•Œê³ ë¦¬ì¦˜ ë¹„êµ ë¶„ì„ ë³´ê³ ì„œ."""

    baseline_result: ExperimentResult | None
    critic_result: ExperimentResult | None
    rho1_result: ExperimentResult | None
    comparison_metrics: dict[str, float]
    performance_ranking: list[str]
    convergence_analysis: dict[str, Any]
    token_weighting_effectiveness: dict[str, float]


class WMTPPerformanceAnalyzer:
    """WMTP ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ ë¶„ì„ê¸°."""

    def __init__(self, mlflow_tracking_uri: str = "./mlflow_runs"):
        """
        ì„±ëŠ¥ ë¶„ì„ê¸° ì´ˆê¸°í™”.

        Args:
            mlflow_tracking_uri: MLflow ì¶”ì  URI
        """
        self.mlflow_uri = mlflow_tracking_uri
        mlflow.set_tracking_uri(mlflow_tracking_uri)
        self.experiment_name = "wmtp/quick_comparison"

        console.print("[bold blue]ğŸ”¬ WMTP ì„±ëŠ¥ ë¶„ì„ê¸° ì´ˆê¸°í™”[/bold blue]")
        console.print(f"MLflow URI: {mlflow_tracking_uri}")

    def discover_experiment_runs(self) -> list[ExperimentResult]:
        """MLflowì—ì„œ ì‹¤í—˜ ê²°ê³¼ë“¤ì„ ìë™ ë°œê²¬."""
        console.print("\n[bold yellow]ğŸ” ì‹¤í—˜ ê²°ê³¼ ìë™ ë°œê²¬ ì¤‘...[/bold yellow]")

        try:
            # ì‹¤í—˜ ê°€ì ¸ì˜¤ê¸°
            experiment = mlflow.get_experiment_by_name(self.experiment_name)
            if not experiment:
                console.print(
                    f"[red]âŒ ì‹¤í—˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {self.experiment_name}[/red]"
                )
                return []

            # ëª¨ë“  ì‹¤í–‰ ê°€ì ¸ì˜¤ê¸°
            runs = mlflow.search_runs(
                experiment_ids=[experiment.experiment_id], output_format="list"
            )

            results = []
            for run in runs:
                # ì•Œê³ ë¦¬ì¦˜ íƒ€ì… ì¶”ë¡ 
                algorithm = self._infer_algorithm_type(run)
                if not algorithm:
                    continue

                # ì‹¤í—˜ ê²°ê³¼ ê°ì²´ ìƒì„±
                result = ExperimentResult(
                    algorithm=algorithm,
                    run_id=run.info.run_id,
                    run_name=run.info.run_name or f"{algorithm}_{run.info.run_id[:8]}",
                    start_time=datetime.fromtimestamp(run.info.start_time / 1000),
                    end_time=datetime.fromtimestamp(run.info.end_time / 1000)
                    if run.info.end_time
                    else None,
                    duration_seconds=(run.info.end_time - run.info.start_time) / 1000
                    if run.info.end_time
                    else 0,
                    status=run.info.status,
                    final_loss=run.data.metrics.get("final_loss"),
                    metrics=dict(run.data.metrics),
                    parameters=dict(run.data.params),
                    tags=dict(run.data.tags),
                )
                results.append(result)

            console.print(f"âœ… ë°œê²¬ëœ ì‹¤í—˜: {len(results)}ê°œ")
            return results

        except Exception as e:
            console.print(f"[red]âŒ ì‹¤í—˜ ë°œê²¬ ì‹¤íŒ¨: {e}[/red]")
            return []

    def _infer_algorithm_type(self, run) -> str | None:
        """ì‹¤í–‰ ì •ë³´ë¡œë¶€í„° ì•Œê³ ë¦¬ì¦˜ íƒ€ì… ì¶”ë¡ ."""
        run_name = (run.info.run_name or "").lower()
        tags = {k.lower(): v.lower() for k, v in run.data.tags.items()}

        # íƒœê·¸ ê¸°ë°˜ ì¶”ë¡ 
        if "baseline" in tags.values() or "mtp baseline" in run_name:
            return "MTP Baseline"
        elif "critic" in tags.values() or "critic" in run_name:
            return "Critic WMTP"
        elif "rho1" in tags.values() or "rho-1" in run_name or "rho1" in run_name:
            return "Rho-1 WMTP"

        # íŒŒë¼ë¯¸í„° ê¸°ë°˜ ì¶”ë¡ 
        algo_param = run.data.params.get("algorithm", "").lower()
        if "baseline" in algo_param:
            return "MTP Baseline"
        elif "critic" in algo_param:
            return "Critic WMTP"
        elif "rho1" in algo_param:
            return "Rho-1 WMTP"

        return None

    def analyze_convergence(self, results: list[ExperimentResult]) -> dict[str, Any]:
        """ìˆ˜ë ´ íŒ¨í„´ ë¶„ì„."""
        console.print("\n[bold blue]ğŸ“ˆ ìˆ˜ë ´ íŒ¨í„´ ë¶„ì„[/bold blue]")

        convergence_data = {}

        for result in results:
            if result.status != "FINISHED" or not result.final_loss:
                continue

            # ê¸°ë³¸ ìˆ˜ë ´ ë©”íŠ¸ë¦­
            convergence_data[result.algorithm] = {
                "final_loss": result.final_loss,
                "training_time": result.duration_seconds,
                "convergence_speed": result.final_loss
                / max(result.duration_seconds, 1),  # loss/time
                "stability": self._calculate_stability(result),
            }

        return convergence_data

    def _calculate_stability(self, result: ExperimentResult) -> float:
        """í›ˆë ¨ ì•ˆì •ì„± ê³„ì‚° (ë”ë¯¸ êµ¬í˜„)."""
        # ì‹¤ì œë¡œëŠ” loss curveì˜ ë¶„ì‚°ì„ ê³„ì‚°í•´ì•¼ í•¨
        # í˜„ì¬ëŠ” final_loss ê¸°ë°˜ ì¶”ì •
        if result.final_loss and result.final_loss > 0:
            return 1.0 / (1.0 + result.final_loss)  # ë‚®ì€ loss = ë†’ì€ ì•ˆì •ì„±
        return 0.0

    def compare_token_weighting_effectiveness(
        self, results: list[ExperimentResult]
    ) -> dict[str, float]:
        """í† í° ê°€ì¤‘ì¹˜ íš¨ê³¼ì„± ë¹„êµ."""
        console.print("\n[bold blue]âš–ï¸  í† í° ê°€ì¤‘ì¹˜ íš¨ê³¼ì„± ë¶„ì„[/bold blue]")

        effectiveness = {}
        baseline_loss = None

        # ê¸°ì¤€ì„  ì°¾ê¸°
        for result in results:
            if result.algorithm == "MTP Baseline" and result.final_loss:
                baseline_loss = result.final_loss
                break

        if not baseline_loss:
            console.print("[yellow]âš ï¸  ê¸°ì¤€ì„  ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ[/yellow]")
            return {}

        # ìƒëŒ€ì  ê°œì„ ë„ ê³„ì‚°
        for result in results:
            if not result.final_loss:
                continue

            if result.algorithm == "MTP Baseline":
                effectiveness[result.algorithm] = 0.0  # ê¸°ì¤€ì„ 
            else:
                improvement = (baseline_loss - result.final_loss) / baseline_loss * 100
                effectiveness[result.algorithm] = improvement

        return effectiveness

    def generate_performance_ranking(
        self, results: list[ExperimentResult]
    ) -> list[str]:
        """ì„±ëŠ¥ ìˆœìœ„ ìƒì„±."""
        # final_loss ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
        finished_results = [
            r for r in results if r.status == "FINISHED" and r.final_loss
        ]

        if not finished_results:
            return []

        sorted_results = sorted(finished_results, key=lambda x: x.final_loss)
        return [r.algorithm for r in sorted_results]

    def create_comparison_report(
        self, results: list[ExperimentResult]
    ) -> ComparisonReport:
        """ì¢…í•© ë¹„êµ ë³´ê³ ì„œ ìƒì„±."""
        console.print("\n[bold green]ğŸ“Š ì¢…í•© ë¹„êµ ë³´ê³ ì„œ ìƒì„± ì¤‘...[/bold green]")

        # ì•Œê³ ë¦¬ì¦˜ë³„ ìµœì‹  ê²°ê³¼ ì„ íƒ
        baseline_result = None
        critic_result = None
        rho1_result = None

        for result in results:
            if result.algorithm == "MTP Baseline":
                baseline_result = result
            elif result.algorithm == "Critic WMTP":
                critic_result = result
            elif result.algorithm == "Rho-1 WMTP":
                rho1_result = result

        # ë¶„ì„ ìˆ˜í–‰
        convergence_analysis = self.analyze_convergence(results)
        token_weighting_effectiveness = self.compare_token_weighting_effectiveness(
            results
        )
        performance_ranking = self.generate_performance_ranking(results)

        # ë¹„êµ ë©”íŠ¸ë¦­ ê³„ì‚°
        comparison_metrics = {}
        if baseline_result and baseline_result.final_loss:
            comparison_metrics["baseline_final_loss"] = baseline_result.final_loss

            for result in results:
                if result.algorithm != "MTP Baseline" and result.final_loss:
                    key = f"{result.algorithm.lower().replace(' ', '_')}_improvement"
                    improvement = (
                        baseline_result.final_loss - result.final_loss
                    ) / baseline_result.final_loss
                    comparison_metrics[key] = improvement

        return ComparisonReport(
            baseline_result=baseline_result,
            critic_result=critic_result,
            rho1_result=rho1_result,
            comparison_metrics=comparison_metrics,
            performance_ranking=performance_ranking,
            convergence_analysis=convergence_analysis,
            token_weighting_effectiveness=token_weighting_effectiveness,
        )

    def display_analysis_results(self, report: ComparisonReport):
        """ë¶„ì„ ê²°ê³¼ë¥¼ Rich í…Œì´ë¸”ë¡œ í‘œì‹œ."""
        console.print("\n[bold green]ğŸ‰ WMTP ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ ë¹„êµ ë¶„ì„ ê²°ê³¼[/bold green]")

        # 1. ê¸°ë³¸ ì •ë³´ í…Œì´ë¸”
        basic_table = Table(title="ğŸ“‹ ì‹¤í—˜ ê¸°ë³¸ ì •ë³´")
        basic_table.add_column("ì•Œê³ ë¦¬ì¦˜", style="cyan")
        basic_table.add_column("ìƒíƒœ", style="green")
        basic_table.add_column("ìµœì¢… ì†ì‹¤", style="yellow")
        basic_table.add_column("í›ˆë ¨ ì‹œê°„", style="magenta")
        basic_table.add_column("ì„¤ëª…", style="dim")

        algorithms_info = {
            "MTP Baseline": ("ê¸°ë³¸ MTP (ë¹„êµ ê¸°ì¤€)", report.baseline_result),
            "Critic WMTP": ("Value Function ê¸°ë°˜", report.critic_result),
            "Rho-1 WMTP": ("Reference Model ê¸°ë°˜", report.rho1_result),
        }

        for algo, (description, result) in algorithms_info.items():
            if result:
                status = "âœ… ì™„ë£Œ" if result.status == "FINISHED" else "âŒ ì‹¤íŒ¨"
                loss = f"{result.final_loss:.4f}" if result.final_loss else "N/A"
                time = (
                    f"{result.duration_seconds:.1f}s"
                    if result.duration_seconds
                    else "N/A"
                )
            else:
                status = "â“ ì—†ìŒ"
                loss = "N/A"
                time = "N/A"

            basic_table.add_row(algo, status, loss, time, description)

        console.print(basic_table)

        # 2. ì„±ëŠ¥ ìˆœìœ„
        if report.performance_ranking:
            console.print("\n[bold blue]ğŸ† ì„±ëŠ¥ ìˆœìœ„ (ìµœì¢… ì†ì‹¤ ê¸°ì¤€)[/bold blue]")
            for i, algo in enumerate(report.performance_ranking, 1):
                medal = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰"
                console.print(f"{medal} {i}ìœ„: {algo}")

        # 3. í† í° ê°€ì¤‘ì¹˜ íš¨ê³¼ì„±
        if report.token_weighting_effectiveness:
            console.print("\n[bold blue]âš–ï¸  í† í° ê°€ì¤‘ì¹˜ íš¨ê³¼ì„±[/bold blue]")
            for algo, effectiveness in report.token_weighting_effectiveness.items():
                if effectiveness > 0:
                    console.print(f"â€¢ {algo}: +{effectiveness:.2f}% ê°œì„  ğŸ”¥")
                elif effectiveness < 0:
                    console.print(f"â€¢ {algo}: {effectiveness:.2f}% ì•…í™” ğŸ“‰")
                else:
                    console.print(f"â€¢ {algo}: ê¸°ì¤€ì„  ğŸ“Š")

        # 4. ìˆ˜ë ´ ë¶„ì„
        if report.convergence_analysis:
            conv_table = Table(title="ğŸ“ˆ ìˆ˜ë ´ íŒ¨í„´ ë¶„ì„")
            conv_table.add_column("ì•Œê³ ë¦¬ì¦˜", style="cyan")
            conv_table.add_column("ìˆ˜ë ´ ì†ë„", style="yellow")
            conv_table.add_column("ì•ˆì •ì„±", style="green")
            conv_table.add_column("í‰ê°€", style="magenta")

            for algo, data in report.convergence_analysis.items():
                speed = f"{data['convergence_speed']:.6f}"
                stability = f"{data['stability']:.3f}"

                # í‰ê°€ ìƒì„±
                if data["stability"] > 0.8:
                    evaluation = "ìš°ìˆ˜ â­â­â­"
                elif data["stability"] > 0.6:
                    evaluation = "ì–‘í˜¸ â­â­"
                else:
                    evaluation = "ê°œì„ í•„ìš” â­"

                conv_table.add_row(algo, speed, stability, evaluation)

            console.print(f"\n{conv_table}")

    def save_detailed_report(
        self, report: ComparisonReport, output_path: str = "wmtp_analysis_report.json"
    ):
        """ìƒì„¸ ë¶„ì„ ë³´ê³ ì„œë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥."""
        console.print("\n[bold blue]ğŸ’¾ ìƒì„¸ ë³´ê³ ì„œ ì €ì¥ ì¤‘...[/bold blue]")

        # ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
        report_data = {
            "timestamp": datetime.now().isoformat(),
            "summary": {
                "total_experiments": sum(
                    1
                    for r in [
                        report.baseline_result,
                        report.critic_result,
                        report.rho1_result,
                    ]
                    if r
                ),
                "performance_ranking": report.performance_ranking,
                "comparison_metrics": report.comparison_metrics,
            },
            "algorithms": {},
            "convergence_analysis": report.convergence_analysis,
            "token_weighting_effectiveness": report.token_weighting_effectiveness,
        }

        # ê° ì•Œê³ ë¦¬ì¦˜ ê²°ê³¼ ì¶”ê°€
        for name, result in [
            ("baseline", report.baseline_result),
            ("critic", report.critic_result),
            ("rho1", report.rho1_result),
        ]:
            if result:
                report_data["algorithms"][name] = {
                    "algorithm": result.algorithm,
                    "run_id": result.run_id,
                    "status": result.status,
                    "final_loss": result.final_loss,
                    "duration_seconds": result.duration_seconds,
                    "metrics": result.metrics,
                    "parameters": result.parameters,
                }

        # JSON íŒŒì¼ë¡œ ì €ì¥
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)

        console.print(f"âœ… ë³´ê³ ì„œ ì €ì¥ ì™„ë£Œ: {output_path}")


def main():
    """ë©”ì¸ ë¶„ì„ í•¨ìˆ˜."""
    console.print(
        Panel.fit(
            "[bold green]ğŸ§ª WMTP ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ ë¹„êµ ë¶„ì„[/bold green]\n\n"
            '"Not All Tokens Are What You Need" ì² í•™ì„ ë°”íƒ•ìœ¼ë¡œ í•œ\n'
            "ì„¸ ê°€ì§€ ê°€ì¤‘ì¹˜ ì „ëµì˜ ì¢…í•©ì  ì„±ëŠ¥ ë¶„ì„",
            title="WMTP Performance Analysis",
        )
    )

    # ë¶„ì„ê¸° ì´ˆê¸°í™”
    analyzer = WMTPPerformanceAnalyzer()

    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        console=console,
    ) as progress:
        # ì‹¤í—˜ ê²°ê³¼ ë°œê²¬
        task = progress.add_task("ì‹¤í—˜ ê²°ê³¼ ìˆ˜ì§‘ ì¤‘...", total=None)
        results = analyzer.discover_experiment_runs()
        progress.update(task, description="âœ… ì‹¤í—˜ ê²°ê³¼ ìˆ˜ì§‘ ì™„ë£Œ")

        if not results:
            console.print("[red]âŒ ë¶„ì„í•  ì‹¤í—˜ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.[/red]")
            console.print("ë¨¼ì € ì‹¤í—˜ì„ ì‹¤í–‰í•˜ì„¸ìš”: python run_quick_experiment.py")
            return

        # ë¹„êµ ë¶„ì„ ìˆ˜í–‰
        progress.update(task, description="ì„±ëŠ¥ ë¶„ì„ ì¤‘...")
        report = analyzer.create_comparison_report(results)

        progress.update(task, description="âœ… ë¶„ì„ ì™„ë£Œ")

    # ê²°ê³¼ í‘œì‹œ
    analyzer.display_analysis_results(report)

    # ìƒì„¸ ë³´ê³ ì„œ ì €ì¥
    analyzer.save_detailed_report(report)

    # ë‹¤ìŒ ë‹¨ê³„ ì•ˆë‚´
    console.print("\n[bold cyan]ğŸš€ ë‹¤ìŒ ë‹¨ê³„:[/bold cyan]")
    console.print("1. ë” ê¸´ í›ˆë ¨ì„ ìœ„í•´ configsì—ì„œ max_steps ì¦ê°€")
    console.print("2. ë‹¤ë¥¸ ë°ì´í„°ì…‹ìœ¼ë¡œ ì¼ë°˜í™” ì„±ëŠ¥ í™•ì¸")
    console.print("3. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ìœ¼ë¡œ ì„±ëŠ¥ ìµœì í™”")


if __name__ == "__main__":
    main()
