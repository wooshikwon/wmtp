name: CI/CD Pipeline

# 트리거 조건:
# - push: 모든 브랜치에서 테스트 실행
# - pull_request: PR에서 테스트 실행
# - tag: v*.*.* 형식의 tag가 생성되면 Docker 이미지 빌드 및 푸시
on:
  push:
    branches:
      - '**'  # 모든 브랜치
    paths:
      - 'src/**'
      - 'configs/**'
      - 'tests/**'
      - 'docker/**'
      - 'pyproject.toml'
      - 'uv.lock'
    tags:
      - 'v*.*.*'  # v1.0.0, v0.1.0-alpha 등
  pull_request:
    branches:
      - main
      - master
  workflow_dispatch:  # 수동 실행 허용

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  # Job 1: 코드 린트 및 타입 체크
  lint-and-type-check:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          enable-cache: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          uv sync --frozen

      - name: Run linting
        run: |
          uv run ruff check src/
        continue-on-error: true  # 린트 실패해도 계속 진행

      - name: Run type checking
        run: |
          uv run mypy src/ --ignore-missing-imports
        continue-on-error: true  # 타입 체크 실패해도 계속 진행

  # Job 2: 테스트 모델 준비 및 Docker 이미지 빌드
  build-test-image:
    runs-on: ubuntu-latest
    needs: lint-and-type-check
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install transformers for model download
        run: |
          pip install transformers torch --quiet

      - name: Download test models from HuggingFace
        run: |
          mkdir -p tests/test_models

          python -c "
          import json
          from pathlib import Path
          from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig

          # 1. distilgpt2 (reference model)
          print('Downloading distilgpt2 (reference)...')
          save_path = 'tests/test_models/distilgpt2'
          AutoConfig.from_pretrained('distilgpt2').save_pretrained(save_path)
          AutoTokenizer.from_pretrained('distilgpt2').save_pretrained(save_path)
          AutoModelForCausalLM.from_pretrained('distilgpt2').save_pretrained(save_path)
          print('✓ Downloaded distilgpt2')

          # 2. distilgpt2-mtp (base model with MTP config)
          print('Creating distilgpt2-mtp...')
          save_path = 'tests/test_models/distilgpt2-mtp'
          AutoConfig.from_pretrained('distilgpt2').save_pretrained(save_path)
          AutoTokenizer.from_pretrained('distilgpt2').save_pretrained(save_path)
          AutoModelForCausalLM.from_pretrained('distilgpt2').save_pretrained(save_path)

          # config.json에 mtp_config 추가
          config_path = Path(save_path) / 'config.json'
          with open(config_path) as f:
              config = json.load(f)
          config['mtp_config'] = {
              'n_future_tokens': 4,
              'architecture': 'DistilGPT2MTP',
              'base_model': 'distilgpt2'
          }
          config['architectures'] = ['DistilGPT2MTP']
          with open(config_path, 'w') as f:
              json.dump(config, f, indent=2)
          print('✓ Created distilgpt2-mtp with mtp_config')

          # 3. tiny-reward-model-gpt2 (reward model)
          print('Creating tiny-reward-model-gpt2...')
          save_path = 'tests/test_models/tiny-reward-model-gpt2'
          AutoConfig.from_pretrained('distilgpt2').save_pretrained(save_path)
          AutoTokenizer.from_pretrained('distilgpt2').save_pretrained(save_path)
          AutoModelForCausalLM.from_pretrained('distilgpt2').save_pretrained(save_path)
          print('✓ Created tiny-reward-model-gpt2')
          "

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build Docker image
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./docker/Dockerfile
          push: false
          load: true
          tags: wmtp-test:latest
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Save Docker image
        run: |
          docker save wmtp-test:latest | gzip > wmtp-test-image.tar.gz

      - name: Upload Docker image artifact
        uses: actions/upload-artifact@v4
        with:
          name: docker-image
          path: wmtp-test-image.tar.gz
          retention-days: 1

  # Job 3: Docker 이미지로 테스트 실행
  test:
    runs-on: ubuntu-latest
    needs: build-test-image
    strategy:
      matrix:
        recipe: [mtp_baseline, critic_wmtp, rho1_wmtp_weighted, rho1_wmtp_tokenskip]
    steps:
      - name: Download Docker image artifact
        uses: actions/download-artifact@v4
        with:
          name: docker-image

      - name: Load Docker image
        run: |
          docker load < wmtp-test-image.tar.gz

      - name: Test ${{ matrix.recipe }}
        run: |
          docker run --rm \
            -v /tmp/mlflow_ci:/tmp/mlflow_m3 \
            -v /tmp/test_checkpoints:/app/test_checkpoints \
            wmtp-test:latest \
            uv run python -m src.cli.train \
              --config tests/configs/config.local_test.yaml \
              --recipe tests/configs/recipe.${{ matrix.recipe }}.yaml \
              --verbose

  # Job 4: Docker 이미지 푸시 (tag 생성 시에만)
  docker-push:
    runs-on: ubuntu-latest
    needs: test
    if: startsWith(github.ref, 'refs/tags/v')
    permissions:
      contents: read
      packages: write
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install transformers for model download
        run: |
          pip install transformers torch --quiet

      - name: Download test models from HuggingFace
        run: |
          mkdir -p tests/test_models

          python -c "
          import json
          from pathlib import Path
          from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig

          # 1. distilgpt2 (reference model)
          print('Downloading distilgpt2 (reference)...')
          save_path = 'tests/test_models/distilgpt2'
          AutoConfig.from_pretrained('distilgpt2').save_pretrained(save_path)
          AutoTokenizer.from_pretrained('distilgpt2').save_pretrained(save_path)
          AutoModelForCausalLM.from_pretrained('distilgpt2').save_pretrained(save_path)
          print('✓ Downloaded distilgpt2')

          # 2. distilgpt2-mtp (base model with MTP config)
          print('Creating distilgpt2-mtp...')
          save_path = 'tests/test_models/distilgpt2-mtp'
          AutoConfig.from_pretrained('distilgpt2').save_pretrained(save_path)
          AutoTokenizer.from_pretrained('distilgpt2').save_pretrained(save_path)
          AutoModelForCausalLM.from_pretrained('distilgpt2').save_pretrained(save_path)

          # config.json에 mtp_config 추가
          config_path = Path(save_path) / 'config.json'
          with open(config_path) as f:
              config = json.load(f)
          config['mtp_config'] = {
              'n_future_tokens': 4,
              'architecture': 'DistilGPT2MTP',
              'base_model': 'distilgpt2'
          }
          config['architectures'] = ['DistilGPT2MTP']
          with open(config_path, 'w') as f:
              json.dump(config, f, indent=2)
          print('✓ Created distilgpt2-mtp with mtp_config')

          # 3. tiny-reward-model-gpt2 (reward model)
          print('Creating tiny-reward-model-gpt2...')
          save_path = 'tests/test_models/tiny-reward-model-gpt2'
          AutoConfig.from_pretrained('distilgpt2').save_pretrained(save_path)
          AutoTokenizer.from_pretrained('distilgpt2').save_pretrained(save_path)
          AutoModelForCausalLM.from_pretrained('distilgpt2').save_pretrained(save_path)
          print('✓ Created tiny-reward-model-gpt2')
          "

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
          tags: |
            type=semver,pattern={{version}}
            type=semver,pattern={{major}}.{{minor}}
            type=semver,pattern={{major}}
            type=raw,value=latest

      - name: Build and push Docker image
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./docker/Dockerfile
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          platforms: linux/amd64,linux/arm64

      - name: Generate release report
        run: |
          echo "### 🚀 Docker Image Released" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Version**: \`${{ github.ref_name }}\`" >> $GITHUB_STEP_SUMMARY
          echo "**Registry**: \`${{ env.REGISTRY }}\`" >> $GITHUB_STEP_SUMMARY
          echo "**Image**: \`${{ env.IMAGE_NAME }}\`" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Tags**:" >> $GITHUB_STEP_SUMMARY
          echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
          echo "${{ steps.meta.outputs.tags }}" >> $GITHUB_STEP_SUMMARY
          echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Pull Command**:" >> $GITHUB_STEP_SUMMARY
          echo "\`\`\`bash" >> $GITHUB_STEP_SUMMARY
          echo "docker pull ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.ref_name }}" >> $GITHUB_STEP_SUMMARY
          echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
