# VESSL Configuration for WMTP Fine-Tuning Framework
name: wmtp-finetune
description: Weighted Multi-Token Prediction Fine-Tuning

# Docker image specification
image: ghcr.io/wooshikwon/wmtp:latest  # Push via `make push`

# Resource specifications (update to your cluster/preset)
resources:
  cluster: default  # e.g., vessl-gcp-oregon
  preset: v1-a100-4-pod  # 4 GPU configuration for production

# Environment variables
env:
  # MLflow tracking (S3 recommended in production)
  MLFLOW_TRACKING_URI: s3://wmtp/mlflow
  MLFLOW_REGISTRY_URI: s3://wmtp/mlflow

  # S3 configuration (use VESSL secrets)
  AWS_ACCESS_KEY_ID: ${secret:AWS_ACCESS_KEY_ID}
  AWS_SECRET_ACCESS_KEY: ${secret:AWS_SECRET_ACCESS_KEY}
  AWS_DEFAULT_REGION: eu-north-1
  S3_BUCKET_NAME: wmtp

  # HuggingFace token for model downloads
  HF_TOKEN: ${secret:HF_TOKEN}

  # PyTorch settings
  TORCH_CUDA_ARCH_LIST: "8.0"  # A100 architecture
  PYTORCH_CUDA_ALLOC_CONF: "max_split_size_mb:512"

  # Distributed training (single-node default)
  MASTER_ADDR: localhost
  MASTER_PORT: 29500

  # Algorithm selector: baseline-mtp | critic-wmtp | rho1-weighted | rho1-tokenskip
  WMTP_ALGO: baseline-mtp

  # Environment mode: test | production
  ENV_MODE: test

# Command to run
command: |
  echo "Starting WMTP on VESSL..."
  echo "Algorithm: ${WMTP_ALGO}"
  echo "Environment: ${ENV_MODE}"

  # Set config based on environment mode
  if [ "${ENV_MODE}" = "test" ]; then
    CONFIG=tests/configs/config.local_test.yaml
    echo "Using test configuration"
  else
    CONFIG=configs/config.vessl.yaml
    echo "Using production configuration"
  fi

  # Select recipe by algorithm
  if [ "${WMTP_ALGO}" = "baseline-mtp" ]; then
    RECIPE=configs/recipe.mtp_baseline.yaml
  elif [ "${WMTP_ALGO}" = "critic-wmtp" ]; then
    RECIPE=configs/recipe.critic_wmtp.yaml
  elif [ "${WMTP_ALGO}" = "rho1-weighted" ]; then
    RECIPE=configs/recipe.rho1_wmtp_weighted.yaml
  elif [ "${WMTP_ALGO}" = "rho1-tokenskip" ]; then
    RECIPE=configs/recipe.rho1_wmtp_tokenskip.yaml
  else
    echo "Invalid WMTP_ALGO: ${WMTP_ALGO}"
    exit 1
  fi

  echo "Using config: ${CONFIG}"
  echo "Using recipe: ${RECIPE}"

  # Create ephemeral working dirs
  mkdir -p /tmp/models /tmp/datasets /tmp/outputs
  export PYTHONUNBUFFERED=1

  # Train
  uv run python -m src.cli.train \
    --config ${CONFIG} \
    --recipe ${RECIPE} \
    --run-name vessl_${WMTP_ALGO}_${ENV_MODE} \
    --tags vessl,${WMTP_ALGO},${ENV_MODE}

  # (Optional) Evaluate - expects checkpoint location managed by pipeline/MLflow
  # uv run python -m src.cli.eval \
  #   --config ${CONFIG} \
  #   --recipe ${RECIPE} \
  #   --checkpoint /tmp/models/checkpoints/final.pt

# Monitoring and alerts
monitoring:
  gpu_utilization:
    threshold: 0.8
    duration: 300
  memory_usage:
    threshold: 0.9

# Auto-restart policy
restart_policy:
  condition: on-failure
  max_attempts: 3

# Secrets to configure (add via VESSL UI)
secrets:
  - AWS_ACCESS_KEY_ID
  - AWS_SECRET_ACCESS_KEY
  - HF_TOKEN
