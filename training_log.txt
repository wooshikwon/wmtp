╭──────────── 🧪 Test Mode ────────────╮
│ WMTP Pipeline Test on MacBook M3     │
│ Testing with small model and dataset │
╰──────────────────────────────────────╯

Checking environment...
✓ PyTorch version: 2.4.1
✓ MPS (Metal Performance Shaders) is available
  MPS device: True
✓ Total memory: 36.0 GB
  Available memory: 15.5 GB
✓ Transformers version: 4.56.2

Loading configurations...
✓ Loaded config: configs/config.m3_test.yaml
✓ Loaded recipe: configs/recipe.m3_test.yaml

Using tiny model (distilgpt2) for ultra-light testing
  Model: distilgpt2 (82M parameters)
  Max sequence length: 64
  Training steps: 5

Configuring test model loader...
✓ Test model loaders registered
✓ Test data loader registered
Random seed set to 42
✓ Random seed set to 42

Pipeline Configuration:
  Algorithm: mtp-baseline
  Model: distilgpt2
  Device: mps
  Batch size: 1
  Max steps: 5
  Dry run: False

Starting actual training (auto-confirmed for testing)...

Starting WMTP pipeline test...
✓ Training pipeline imported successfully
🚀 파이프라인 실행 시작
🔍 파이프라인 단계 추적 시작...
Random seed set to 42
🔍 체크포인트 로딩 완료: epoch=0, step=0
🔍 MLflow Run ID: None
Started MLflow run: 7e991f9c17d344bbb21843b7198d4f17
🔍 MLflow 실험 추적 초기화 완료: run_name=m3_test_mtp_baseline
/Users/wesley/Desktop/wooshikwon/wmtp/src/components/loader/test_mtp_loader.py:115: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model = torch.load(cache_path, map_location=self.device)
[TestMTPLoader] Setup complete. Device: mps
[TestMTPLoader] Loading cached model from /Users/wesley/.cache/wmtp/test_models/mtp_wrapper_distilgpt2.pt
[TestMTPLoader] Successfully loaded cached model
[TestMTPLoader] Model loaded successfully
[TestMTPLoader] Total params: 197,704,704
[TestMTPLoader] Trainable params: 197,704,704
[TestMTPLoader] Estimated memory: 0.74 GB
🔍 Base 모델 로딩 완료: distilgpt2
🔍 토크나이저 생성 완료: distilgpt2
🔍 알고리즘별 추가 모델 로딩 완료: mtp-baseline
🔍 옵티마이저 설정 완료: mtp-baseline
Map:   0%|          | 0/10 [00:00<?, ? examples/s]Map: 100%|██████████| 10/10 [00:00<00:00, 1916.43 examples/s]
🔍 데이터셋 토크나이징 완료: mtp-baseline
HF호환 토크나이저로 데이터셋 토크나이징:   0%|          | 0/10 [00:00<?, ? examples/s]HF호환 토크나이저로 데이터셋 토크나이징: 100%|██████████| 10/10 [00:00<00:00, 4338.34 examples/s]
🔍 분산 훈련용 데이터 샘플러 설정 완료: mtp-baseline
🔍 분산 훈련용 데이터 샘플러 설정 완료: mtp-baseline
🔍 PyTorch DataLoader 생성 완료: mtp-baseline
🔍 Stage1 사전훈련 완료: mtp-baseline
🔍 메인 Trainer 생성 및 초기화 완료: mtp-baseline
🔍 실행 모드 분기 완료: mtp-baseline
체크포인트 저장 활성화: 매 5스텝마다 저장
체크포인트 디렉토리: checkpoints/m3_test_mtp_baseline

Pipeline test failed: Placeholder storage has not been allocated on MPS device!

This is expected for initial testing.
Common issues:
  1. Model loading issues - check paths
  2. Memory issues - use --tiny flag
  3. MPS issues - may need to use CPU

Full traceback:
Traceback (most recent call last):
  File "/Users/wesley/Desktop/wooshikwon/wmtp/test_m3_pipeline.py", line 236, in main
    outputs = run_training_pipeline(
              ^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/wesley/Desktop/wooshikwon/wmtp/src/pipelines/training_pipeline.py", line 266, in run_training_pipeline
    metrics = trainer.run({
              ^^^^^^^^^^^^^
  File "/Users/wesley/Desktop/wooshikwon/wmtp/src/components/trainer/mtp_weighted_ce_trainer.py", line 706, in run
    out = self.train_step(batch)
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/wesley/Desktop/wooshikwon/wmtp/src/components/trainer/mtp_weighted_ce_trainer.py", line 347, in train_step
    outputs: dict[str, Any] | torch.Tensor = self.model(**batch)
                                             ^^^^^^^^^^^^^^^^^^^
  File "/Users/wesley/Desktop/wooshikwon/wmtp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/wesley/Desktop/wooshikwon/wmtp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/wesley/Desktop/wooshikwon/wmtp/src/components/model/mtp_wrapper.py", line 123, in forward
    base_outputs = self.base_model(
                   ^^^^^^^^^^^^^^^^
  File "/Users/wesley/Desktop/wooshikwon/wmtp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/wesley/Desktop/wooshikwon/wmtp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/wesley/Desktop/wooshikwon/wmtp/.venv/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 1070, in forward
    transformer_outputs = self.transformer(
                          ^^^^^^^^^^^^^^^^^
  File "/Users/wesley/Desktop/wooshikwon/wmtp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/wesley/Desktop/wooshikwon/wmtp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/wesley/Desktop/wooshikwon/wmtp/.venv/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 857, in forward
    inputs_embeds = self.wte(input_ids)
                    ^^^^^^^^^^^^^^^^^^^
  File "/Users/wesley/Desktop/wooshikwon/wmtp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/wesley/Desktop/wooshikwon/wmtp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/wesley/Desktop/wooshikwon/wmtp/.venv/lib/python3.12/site-packages/torch/nn/modules/sparse.py", line 164, in forward
    return F.embedding(
           ^^^^^^^^^^^^
  File "/Users/wesley/Desktop/wooshikwon/wmtp/.venv/lib/python3.12/site-packages/torch/nn/functional.py", line 2267, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Placeholder storage has not been allocated on MPS device!
