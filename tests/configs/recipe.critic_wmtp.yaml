# M3 Test Recipe - Critic WMTP Algorithm
# Value function based dynamic weighting for Multi-Token Prediction

# Run metadata
run:
  name: "m3_test_critic_wmtp"
  tags:
    - "test"
    - "m3"
    - "critic"
    - "wmtp"

# Pretraining configuration (Stage 1)
pretrain:
  enabled: true
  num_epochs: 3
  max_steps: 30
  lr: 1e-4
  save_value_head: true

  early_stopping:
    enabled: true
    mode: "any"
    patience: 10
    min_delta: 1e-4
    monitor: "value_loss"
    grad_norm_threshold: 50.0
    grad_norm_window_size: 10
    grad_norm_threshold_ratio: 0.7
    variance_min: 0.1
    variance_max: 5.0

# Training configuration (Stage 2)
train:
  algo: "critic-wmtp"
  full_finetune: true
  max_steps: 2

  early_stopping:
    enabled: false
    patience: 100
    min_delta: 1e-5
    monitor: "wmtp_loss"

# Optimizer configuration
optim:
  optimizer: "adamw"
  lr: 5.0e-5
  weight_decay: 0.01
  betas: [0.9, 0.999]
  grad_clip: 1.0
  scheduler: "constant"
  warmup_ratio: 0.0

# Data configuration
data:
  train:
    sources: ["mbpp"]
    max_length: 128
    batch_size: 1
    pack_sequences: false
  eval:
    sources: ["mbpp"]
    max_length: 128
    batch_size: 1
    pack_sequences: false

# Loss configuration - critic settings
loss:
  weight_norm: "mean1.0_clip"
  lambda: 0.3
  weight_temperature: 0.7
  epsilon: 0.05
  max_weight: 3.0

# Critic-specific configuration
critic:
  target: "rm_sequence"
  token_spread: "gae"
  delta_mode: "td"
  normalize: "zscore"
  discount_lambda: 0.95
  gamma: 0.99
  gae_lambda: 0.95
  auxiliary_loss_coef: 0.1
  value_lr: 5e-5
  use_pseudo_rewards: true

# Evaluation configuration
eval:
  protocol: "meta-mtp"
  sampling:
    temperature: 0.7
    top_p: 0.9
    n: 1
  metrics:
    - "mbpp_exact"
