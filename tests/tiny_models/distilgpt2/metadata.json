{
  "wmtp_type": "reference_model",
  "training_algorithm": "base_training",
  "horizon": 4,
  "n_heads": 4,
  "base_architecture": "gpt2",
  "model_size": "120m",
  "storage_version": "2.0",
  "created_by": "wmtp_local_standardizer",
  "standardization_date": "2025-09-26T18:58:42.064895",
  "original_format": "huggingface_safetensors",
  "source_path": "tests/tiny_models/distilgpt2",
  "loading_strategy": {
    "loader_type": "huggingface",
    "model_class_name": null,
    "custom_module_file": null,
    "transformers_class": "AutoModelForCausalLM",
    "state_dict_mapping": {
      "remove_prefix": null,
      "add_prefix": null,
      "key_transforms": {}
    },
    "required_files": ["config.json", "model.safetensors", "metadata.json"]
  },
  "algorithm_compatibility": ["rho1-wmtp"]
}
