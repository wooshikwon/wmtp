"""
MPS í†µí•© í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
ì‹¤ì œ MPS ë””ë°”ì´ìŠ¤ì—ì„œ 4D í…ì„œ ë¸”ë¡œí‚¹ ë¬¸ì œë¥¼ ì¬í˜„í•˜ê³  ìµœì í™”ë¥¼ ê²€ì¦í•©ë‹ˆë‹¤.
"""

import time
from pathlib import Path

import torch
import yaml

from src.utils.mps_optimizer import MPSOptimizer


def test_mps_blocking_issue():
    """MPSì—ì„œ 4D í…ì„œ ë¸”ë¡œí‚¹ ë¬¸ì œ ì¬í˜„"""
    print("\n" + "=" * 60)
    print("MPS 4D Tensor Blocking Test")
    print("=" * 60)

    # ë””ë°”ì´ìŠ¤ ì •ë³´ ì¶œë ¥
    device_info = MPSOptimizer.get_device_info()
    print(f"Device Info: {device_info}")

    if not device_info["mps_available"]:
        print("âš ï¸ MPS not available. Skipping test.")
        return

    device = torch.device("mps")
    print(f"âœ… Using device: {device}")

    # WMTPì™€ ë™ì¼í•œ í¬ê¸°
    B, S, H, V = 2, 128, 4, 50257
    print(f"\nğŸ“Š Tensor dimensions: B={B}, S={S}, H={H}, V={V}")

    # 1. 4D í…ì„œ ìƒì„± (modeling.py ì‹œë®¬ë ˆì´ì…˜)
    print("\n1ï¸âƒ£ Creating 4D tensor (torch.stack)...")
    tensors = [torch.randn(B, S, V, device=device) for _ in range(H)]

    try:
        start = time.time()
        # ê¸°ì¡´ ë°©ì‹ (ë¸”ë¡œí‚¹ ê°€ëŠ¥)
        mtp_logits = torch.stack(tensors, dim=2)  # [B, S, H, V]
        torch.mps.synchronize()  # MPS ë™ê¸°í™”
        elapsed = time.time() - start
        print(f"   âœ… Stack completed: {elapsed:.4f}s")
        print(f"   Shape: {mtp_logits.shape}, Contiguous: {mtp_logits.is_contiguous()}")
    except Exception as e:
        print(f"   âŒ Stack failed: {e}")
        return

    # 2. 4Dâ†’2D flatten (base_wmtp_trainer.py ì‹œë®¬ë ˆì´ì…˜)
    print("\n2ï¸âƒ£ Flattening to 2D (view)...")
    try:
        start = time.time()
        flat = mtp_logits.view(B * S * H, V)  # âš ï¸ ì—¬ê¸°ì„œ ë¸”ë¡œí‚¹ ê°€ëŠ¥
        torch.mps.synchronize()
        elapsed = time.time() - start
        print(f"   âœ… Flatten completed: {elapsed:.4f}s")
        print(f"   Shape: {flat.shape}")
    except Exception as e:
        print(f"   âŒ Flatten failed or timed out: {e}")

    # 3. Cross-Entropy ê³„ì‚°
    print("\n3ï¸âƒ£ Computing Cross-Entropy...")
    try:
        labels = torch.randint(0, V, (B * S * H,), device=device)
        start = time.time()
        loss = torch.nn.functional.cross_entropy(flat, labels)
        torch.mps.synchronize()
        elapsed = time.time() - start
        print(f"   âœ… CE completed: {elapsed:.4f}s")
        print(f"   Loss: {loss.item():.4f}")
    except Exception as e:
        print(f"   âŒ CE failed: {e}")


def test_mps_optimized_path():
    """MPS ìµœì í™” ê²½ë¡œ í…ŒìŠ¤íŠ¸"""
    print("\n" + "=" * 60)
    print("MPS Optimized Path Test")
    print("=" * 60)

    device_info = MPSOptimizer.get_device_info()
    if not device_info["mps_available"]:
        print("âš ï¸ MPS not available. Skipping test.")
        return

    device = torch.device("mps")
    B, S, H, V = 2, 128, 4, 50257
    print(f"ğŸ“Š Tensor dimensions: B={B}, S={S}, H={H}, V={V}")

    # ì„¤ì • ì‹œë®¬ë ˆì´ì…˜
    config = {
        "launcher": {"resources": {"gpu_type": "mps"}},
        "devices": {"compute_backend": "mps"},
        "device": device,
    }

    use_mps = MPSOptimizer.should_use_mps_path(config)
    print(f"\nğŸ” MPS optimization enabled: {use_mps}")

    # 1. ìµœì í™”ëœ 4D ìŠ¤íƒ
    print("\n1ï¸âƒ£ Optimized 4D stack...")
    tensors = [torch.randn(B, S, V, device=device) for _ in range(H)]

    start = time.time()
    mtp_logits = MPSOptimizer.optimize_4d_stack(tensors, dim=2, use_mps=True)
    torch.mps.synchronize()
    elapsed = time.time() - start
    print(f"   âœ… Optimized stack: {elapsed:.4f}s")
    print(f"   Shape: {mtp_logits.shape}, Contiguous: {mtp_logits.is_contiguous()}")

    # 2. ìµœì í™”ëœ CE ê³„ì‚° (í—¤ë“œë³„ 3D)
    print("\n2ï¸âƒ£ Optimized CE computation (per-head 3D)...")
    target_labels = torch.randint(0, V, (B, S, H), device=device)

    start = time.time()
    ce_per_head = MPSOptimizer.compute_ce_per_head_mps(mtp_logits, target_labels)
    torch.mps.synchronize()
    elapsed = time.time() - start
    print(f"   âœ… CE per head: {elapsed:.4f}s")
    print(f"   Shape: {ce_per_head.shape}, Mean CE: {ce_per_head.mean().item():.4f}")

    # 3. ì„±ëŠ¥ ë¹„êµ
    print("\n3ï¸âƒ£ Performance comparison...")

    # ê¸°ì¡´ ë°©ì‹ ì‹œê°„ ì¸¡ì •
    def traditional_ce():
        logits_flat = mtp_logits.contiguous().view(B * S * H, V)
        labels_flat = target_labels.view(B * S * H)
        ce = torch.nn.functional.cross_entropy(
            logits_flat, labels_flat, reduction="none"
        )
        return ce.view(B, S, H)

    # MPS ìµœì í™” ë°©ì‹ ì‹œê°„ ì¸¡ì •
    def optimized_ce():
        return MPSOptimizer.compute_ce_per_head_mps(mtp_logits, target_labels)

    # ë²¤ì¹˜ë§ˆí¬
    traditional_time = MPSOptimizer.benchmark_operation(
        traditional_ce, num_iterations=10, warmup=3
    )
    optimized_time = MPSOptimizer.benchmark_operation(
        optimized_ce, num_iterations=10, warmup=3
    )

    speedup = traditional_time / optimized_time if optimized_time > 0 else 0
    print("\nğŸ“ˆ Performance Results:")
    print(f"   Traditional (4Dâ†’2D): {traditional_time:.6f}s")
    print(f"   Optimized (3D per-head): {optimized_time:.6f}s")
    print(f"   Speedup: {speedup:.2f}x")


def test_config_integration():
    """ì‹¤ì œ config.yamlê³¼ì˜ í†µí•© í…ŒìŠ¤íŠ¸"""
    print("\n" + "=" * 60)
    print("Config Integration Test")
    print("=" * 60)

    config_path = Path("tests/configs/config.local_test.yaml")
    if not config_path.exists():
        print(f"âš ï¸ Config file not found: {config_path}")
        return

    with open(config_path) as f:
        config = yaml.safe_load(f)

    print(f"ğŸ“„ Loaded config: {config_path}")

    # GPU íƒ€ì… í™•ì¸
    gpu_type = config.get("launcher", {}).get("resources", {}).get("gpu_type", "")
    compute_backend = config.get("devices", {}).get("compute_backend", "")

    print(f"   GPU Type: {gpu_type}")
    print(f"   Compute Backend: {compute_backend}")

    # MPS ê²½ë¡œ íŒë‹¨
    use_mps = MPSOptimizer.should_use_mps_path(config)
    print(f"\nğŸ” Should use MPS path: {use_mps}")

    if gpu_type == "mps" and not use_mps:
        print("âš ï¸ Config specifies MPS but MPS path not enabled!")
        print("   Check if MPS is available on this system.")


def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("\n" + "ğŸš€ Starting MPS Integration Tests " + "ğŸš€")

    # 1. ë¸”ë¡œí‚¹ ë¬¸ì œ ì¬í˜„
    test_mps_blocking_issue()

    # 2. ìµœì í™” ê²½ë¡œ í…ŒìŠ¤íŠ¸
    test_mps_optimized_path()

    # 3. ì„¤ì • í†µí•© í…ŒìŠ¤íŠ¸
    test_config_integration()

    print("\n" + "âœ… All tests completed! " + "âœ…")


if __name__ == "__main__":
    main()
