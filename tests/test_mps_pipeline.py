"""
Complete MPS Pipeline Test
Tests the full stack: modeling.py ‚Üí base_wmtp_trainer.py ‚Üí MPSOptimizer
"""

import sys
from pathlib import Path

import torch

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

# Import the model directly
import importlib.util

model_path = Path(__file__).parent / "test_models" / "distilgpt2-mtp" / "modeling.py"
spec = importlib.util.spec_from_file_location("modeling", model_path)
modeling_module = importlib.util.module_from_spec(spec)
spec.loader.exec_module(modeling_module)
DistilGPT2MTP = modeling_module.DistilGPT2MTP
from src.components.trainer.base_wmtp_trainer import compute_weighted_mtp_loss
from src.utils.mps_optimizer import MPSOptimizer


def test_model_with_mps_config():
    """Test that the model correctly uses MPS optimization when configured."""

    print("\n" + "=" * 60)
    print("MPS Pipeline Test: Model ‚Üí Trainer ‚Üí Optimizer")
    print("=" * 60)

    # MPS configuration
    mps_config = {
        "launcher": {"resources": {"gpu_type": "mps"}},
        "devices": {"compute_backend": "mps"},
    }

    cuda_config = {
        "launcher": {"resources": {"gpu_type": "A100"}},
        "devices": {"compute_backend": "cuda"},
    }

    device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")

    # Test 1: Model initialization with configs
    print("\n1Ô∏è‚É£ Testing Model Initialization:")

    # Create model with MPS config
    model_mps = DistilGPT2MTP(config=None, training_config=mps_config)
    print(
        f"   Model with MPS config: _use_mps_optimization = {model_mps._use_mps_optimization}"
    )

    # Create model with CUDA config
    model_cuda = DistilGPT2MTP(config=None, training_config=cuda_config)
    print(
        f"   Model with CUDA config: _use_mps_optimization = {model_cuda._use_mps_optimization}"
    )

    # Create model without config
    model_default = DistilGPT2MTP(config=None, training_config=None)
    print(
        f"   Model without config: _use_mps_optimization = {model_default._use_mps_optimization}"
    )

    # Test 2: Forward pass with MPS optimization
    print("\n2Ô∏è‚É£ Testing Forward Pass:")

    B, S = 2, 10
    input_ids = torch.randint(0, 1000, (B, S), device=device)

    # Move models to device
    model_mps = model_mps.to(device)

    # Forward pass
    with torch.no_grad():
        outputs = model_mps(input_ids=input_ids, output_hidden_states=True)

    mtp_logits = outputs.logits
    print(f"   MTP logits shape: {mtp_logits.shape}")
    print(f"   Expected shape: [{B}, {S}, 4, {model_mps.vocab_size}]")

    # Validate shape
    assert mtp_logits.shape == (
        B,
        S,
        4,
        model_mps.vocab_size,
    ), f"Shape mismatch: {mtp_logits.shape}"
    print("   ‚úÖ Shape validation passed")

    # Test 3: Complete pipeline test
    print("\n3Ô∏è‚É£ Testing Complete Pipeline (Model ‚Üí Trainer ‚Üí Optimizer):")

    # Generate target labels
    target_labels = torch.randint(0, model_mps.vocab_size, (B, S, 4), device=device)
    head_weights = torch.ones(B, S, 4, device=device)

    # Test with MPS config
    print("\n   Testing with MPS config:")
    loss_mps, mask_mps, ce_mps = compute_weighted_mtp_loss(
        logits=mtp_logits,
        target_labels=target_labels,
        head_weights=head_weights,
        ignore_index=-100,
        config=mps_config,
    )
    print(f"   Loss: {loss_mps.item():.4f}")

    # Test with CUDA config (should use default path)
    print("\n   Testing with CUDA config:")
    loss_cuda, mask_cuda, ce_cuda = compute_weighted_mtp_loss(
        logits=mtp_logits,
        target_labels=target_labels,
        head_weights=head_weights,
        ignore_index=-100,
        config=cuda_config,
    )
    print(f"   Loss: {loss_cuda.item():.4f}")

    # Test without config
    print("\n   Testing without config:")
    loss_default, mask_default, ce_default = compute_weighted_mtp_loss(
        logits=mtp_logits,
        target_labels=target_labels,
        head_weights=head_weights,
        ignore_index=-100,
        config=None,
    )
    print(f"   Loss: {loss_default.item():.4f}")

    # Verify consistency
    print("\n4Ô∏è‚É£ Consistency Check:")

    # All paths should produce similar results
    if torch.allclose(loss_mps, loss_cuda, rtol=1e-4) and torch.allclose(
        loss_mps, loss_default, rtol=1e-4
    ):
        print("   ‚úÖ All paths produce consistent loss values")
    else:
        print("   ‚ö†Ô∏è Loss values differ between paths:")
        print(f"      MPS: {loss_mps.item():.6f}")
        print(f"      CUDA: {loss_cuda.item():.6f}")
        print(f"      Default: {loss_default.item():.6f}")

    return True


def test_mps_optimization_flow():
    """Test the flow of MPS optimization through the stack."""

    print("\n" + "=" * 60)
    print("MPS Optimization Flow Test")
    print("=" * 60)

    # Check if MPS is available
    mps_available = torch.backends.mps.is_available()
    print(f"\nMPS Backend Available: {mps_available}")

    if not mps_available:
        print("‚ö†Ô∏è MPS not available on this system - testing logic only")

    # Test configuration detection
    test_configs = [
        (
            {
                "launcher": {"resources": {"gpu_type": "mps"}},
                "devices": {"compute_backend": "mps"},
            },
            True,
            "MPS",
        ),
        (
            {
                "launcher": {"resources": {"gpu_type": "m1"}},
                "devices": {"compute_backend": "mps"},
            },
            True,
            "M1",
        ),
        (
            {
                "launcher": {"resources": {"gpu_type": "m2"}},
                "devices": {"compute_backend": "mps"},
            },
            True,
            "M2",
        ),
        (
            {
                "launcher": {"resources": {"gpu_type": "m3"}},
                "devices": {"compute_backend": "mps"},
            },
            True,
            "M3",
        ),
        (
            {
                "launcher": {"resources": {"gpu_type": "A100"}},
                "devices": {"compute_backend": "cuda"},
            },
            False,
            "A100",
        ),
        (
            {
                "launcher": {"resources": {"gpu_type": ""}},
                "devices": {"compute_backend": ""},
            },
            False,
            "Empty",
        ),
        ({}, False, "No config"),
    ]

    print("\nüìã Configuration Detection Tests:")
    for config, expected, label in test_configs:
        result = MPSOptimizer.should_use_mps_path(config)
        status = "‚úÖ" if result == expected else "‚ùå"
        print(
            f"   {status} {label}: should_use_mps_path = {result} (expected {expected})"
        )

    # Test MPS optimization stack behavior
    print("\nüîß MPS Stack Optimization Test:")

    if mps_available:
        device = torch.device("mps")
    else:
        device = torch.device("cpu")
        print("   Using CPU as fallback since MPS not available")

    # Create test tensors
    tensors = [torch.randn(2, 10, 100, device=device) for _ in range(4)]

    # Test optimize_4d_stack
    print("\n   Testing optimize_4d_stack:")
    use_mps = mps_available  # Use MPS optimization if available
    stacked = MPSOptimizer.optimize_4d_stack(tensors, dim=2, use_mps=use_mps)
    print(f"   Input: 4 tensors of shape {tensors[0].shape}")
    print(f"   Output: stacked tensor of shape {stacked.shape}")

    expected_shape = (2, 10, 4, 100)
    assert (
        stacked.shape == expected_shape
    ), f"Shape mismatch: {stacked.shape} vs {expected_shape}"
    print("   ‚úÖ Shape validation passed")

    return True


def test_edge_cases():
    """Test edge cases and error handling."""

    print("\n" + "=" * 60)
    print("Edge Cases and Error Handling Test")
    print("=" * 60)

    device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")

    # Test 1: Model behavior with different configurations
    print("\n1Ô∏è‚É£ Testing model behavior with edge case configurations:")

    # Test with invalid config structure
    invalid_config = {"invalid": "config"}
    model = DistilGPT2MTP(config=None, training_config=invalid_config)
    print(
        f"   Model with invalid config: _use_mps_optimization = {model._use_mps_optimization}"
    )
    assert model._use_mps_optimization == False, "Should be False with invalid config"

    # Test with partially valid config
    partial_config = {"launcher": {"resources": {}}}  # Missing gpu_type
    model2 = DistilGPT2MTP(config=None, training_config=partial_config)
    print(
        f"   Model with partial config: _use_mps_optimization = {model2._use_mps_optimization}"
    )
    assert (
        model2._use_mps_optimization == False
    ), "Should be False with incomplete config"

    print("   ‚úÖ Edge case configurations handled correctly")

    # Test 2: Empty tensor list
    print("\n2Ô∏è‚É£ Testing optimize_4d_stack with edge cases:")

    # Single tensor
    single = [torch.randn(2, 10, 100, device=device)]
    use_mps = torch.backends.mps.is_available()
    result = MPSOptimizer.optimize_4d_stack(single, dim=2, use_mps=use_mps)
    print(
        f"   Single tensor: input shape {single[0].shape} ‚Üí output shape {result.shape}"
    )
    assert result.shape == (2, 10, 1, 100), f"Unexpected shape: {result.shape}"

    # Large number of heads
    many_heads = [torch.randn(1, 5, 50, device=device) for _ in range(16)]
    result_many = MPSOptimizer.optimize_4d_stack(many_heads, dim=2, use_mps=use_mps)
    print(f"   16 heads: output shape {result_many.shape}")
    assert result_many.shape == (1, 5, 16, 50), f"Unexpected shape: {result_many.shape}"

    print("   ‚úÖ Edge cases handled correctly")

    return True


def main():
    """Run all pipeline tests."""

    print("\nüöÄ Starting Complete MPS Pipeline Tests üöÄ")
    print("=" * 70)

    tests = [
        ("Model with MPS Config", test_model_with_mps_config),
        ("MPS Optimization Flow", test_mps_optimization_flow),
        ("Edge Cases", test_edge_cases),
    ]

    results = []
    for name, test_func in tests:
        try:
            print(f"\nüîç Running: {name}")
            result = test_func()
            results.append((name, result))
            print(f"\n   Result: {'‚úÖ PASS' if result else '‚ùå FAIL'}")
        except Exception as e:
            print(f"\n   ‚ùå Error: {e}")
            import traceback

            traceback.print_exc()
            results.append((name, False))

    # Summary
    print("\n" + "=" * 60)
    print("üìä PIPELINE TEST SUMMARY")
    print("=" * 60)

    passed = sum(1 for _, result in results if result)
    total = len(results)

    for name, result in results:
        status = "‚úÖ PASS" if result else "‚ùå FAIL"
        print(f"{status}: {name}")

    print(f"\nTotal: {passed}/{total} tests passed")

    if passed == total:
        print("\nüéâ All pipeline tests passed! MPS optimization is fully integrated.")
        print("\n‚ú® Complete stack verified:")
        print("   1. modeling.py: MPS-aware model with optimize_4d_stack")
        print("   2. base_wmtp_trainer.py: Config-based MPS path selection")
        print("   3. MPSOptimizer: Core optimization utilities")
        print("   4. All trainers: Config propagation working")
    else:
        print(f"\n‚ö†Ô∏è {total - passed} test(s) failed")
        sys.exit(1)


if __name__ == "__main__":
    main()
