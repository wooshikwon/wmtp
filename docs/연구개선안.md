# 연구 개선안

# **연구제안서 타당성 검토 및 개선 방안 보고서**

## **1. 서론: Weighted-MTP 개요 및 문제 제기**

**Weighted-MTP**(Weighted Multi-Token Prediction)는 대규모 언어모델(LLM)의 학습에서 **멀티 토큰 예측(Multi-Token Prediction, MTP)** 능력을 활용하면서 각 토큰의 중요도에 따라 가중치를 부여하여 **효율과 성능을 향상**시키고자 제안된 방법입니다. 기존 언어모델은 다음 토큰 하나만 예측하도록 훈련되는 경우가 일반적이지만, MTP는 한 번에 여러 미래 토큰을 예측하도록 함으로써 **학습 샘플 효율을 높이고 추론 속도를 향상**시킬 수 있습니다. 예컨대 Meta의 MTP 연구에서는 13억 парамет급 모델을 4-토큰 예측으로 학습한 결과, **HumanEval** 코딩 문제 정답률이 12%p, **MBPP** 정답률이 17%p 향상되었고, 다중 토큰 예측을 사용하면 **추론속도가 최대 3배까지 빨라지는** 이점을 보였습니다.

그러나 모든 토큰을 동일하게 다루는 기존 **다음-토큰 예측 손실**에는 비효율이 존재할 수 있습니다. **Rho-1** 연구는 *“학습 코퍼스의 모든 토큰이 동일하게 중요한 것은 아니다”*라고 문제를 제기하며, **유용한 토큰만 선택적으로 학습**하는 **Selective Language Modeling(SLM)**을 통해 성능과 효율을 동시에 높일 수 있음을 보였습니다[huggingface.co](https://huggingface.co/papers/2404.07965#:~:text=Unlike%20traditional%20LMs%20that%20learn,matching%20%2038%20with). 실제로 Rho-1은 **참조 모델(reference model)**로 각 **토큰의 점수를 산정**한 뒤, **“초과 손실(excess loss)”**이 큰(**더 어려운**) 토큰에만 집중적으로 손실을 적용하는 선택적 학습으로 **수학 문제에서 30%p에 달하는 정확도 향상**을 이루고, 전체 **프리트레이닝 토큰의 3%만 사용하고도 SOTA 수준 성능**을 달성하였습니다. 이처럼 **모든 토큰을 균등하게 학습하는 기존 방법의 한계**는 토큰 중요도에 기반한 가중치 조정으로 극복할 수 있다는 것이 최근 연구들의 흐름입니다.

**Weighted-MTP**는 이러한 두 방향 – **멀티 토큰 예측**과 **토큰 중요도 가중화** – 을 결합하여 **더 나은 LLM**을 만들려는 제안입니다. 한 번에 여러 토큰을 예측하면서, 그 중 **특히 중요한 토큰**에 더 큰 가중치를 두어 학습함으로써 **모델의 핵심 추론 능력을 강화**하고자 합니다. 이 아이디어는 합리적이며, MTP로 **학습 효율을 높이고** 토큰 가중화로 **학습 효과를 극대화**하려는 시도로 요약됩니다.

본 보고서에서는 해당 연구제안서의 **타당성**을 검토하고, 지도교수님의 코멘트에 따라 **우려 사항**을 분석하며, **최신 관련 연구 동향**을 폭넓게 조사하여 **개선 가능한 부분**을 제안합니다. 특히 **Weighted-MTP의 핵심 가정**(모든 토큰의 동등하지 않은 중요도)을 유지하되, **손실 함수 수식의 구성**(예: critic 역할의 수정)을 포함한 구현상의 개선점과, **유사 연구와의 비교**를 통해 **아이디어의 참신성**과 **발전 방향**을 살펴보겠습니다.

## **2. 기존 제안서 구조와 교수님 피드백 요약**

제안서에서 구상한 **Weighted-MTP 구조**는 대략 다음과 같았을 것으로 추측됩니다:

- **MTP 기반 이중 출력 구조**: 기본 LLM의 **공유 trunk** 위에 여러 개의 **병렬 출력 헤드**를 부착하여, 한 시점에 다음 **N개의 토큰을 동시에 예측**하도록 학습합니다. 예컨대 4-토큰 예측의 경우, trunk 위에 4개의 출력 레이어를 붙여 시점 t에서 t+1,…,t+4 토큰을 한 번에 예측하고, 이를 각 토큰별 손실로 계산합니다. 이때 **멀티 토큰 예측 손실** $\mathcal{L}_{MTP}$은 다음과 같이 정의될 수 있습니다:L*MTP*=−E*x*∼*Dj*=1∑*N*log*Pθ*(*xt*+*j*∣*x*<*t*),

    LMTP=−Ex∼D  ∑j=1Nlog⁡Pθ(xt+j∣x<t) ,

    각 위치 $t$에서 향후 $N$개 토큰 $x_{t+1:t+N}$의 로그확률을 예측하고 합산하는 형태입니다. 이렇게 훈련하면 **다음-토큰 예측 모델 대비 손실 감소와 성능 향상**을 가져올 수 있으며, 모델 규모가 클수록 이러한 **MTP 훈련의 이득이 커지는 경향**이 보고되었습니다.

- **토큰 중요도 가중치 부여**: **Weighted-MTP**에서는 위 손실에 토큰별 가중치 $w_{t+j}$를 곱하여 **중요한 토큰의 예측 오류에 더 크게 패널티를 주는** 방식을 취합니다. 이상적인 Weighted-MTP 손실은:L*WMTP*=−E*x*∼*Dj*=1∑*Nwt*+*j*⋅log*Pθ*(*xt*+*j*∣*x*<*t*),

    LW-MTP=−Ex∼D  ∑j=1Nwt+j⋅log⁡Pθ(xt+j∣x<t) ,

    형태가 되며, **가중치 $w_{t+j}$는 해당 토큰의 중요도를 반영**합니다. 가중치 산정 방법으로 제안서에서는 **별도의 Critic 함수**(예: 가치함수 또는 보상모델 등)를 활용하는 방안을 언급한 것으로 보입니다. 즉, **Critic**이 문맥 또는 partial output에 대해 **value/reward를 예측**하고, 각 토큰이 **미래 보상에 기여하는 정도**를 평가하여 $w$를 산출하거나, 혹은 **Advantage** 신호로써 $w$를 부여하는 구조를 고려한 것으로 추정됩니다. 이는 **강화학습(RL)**의 **정책 경사(Policy Gradient)** 방법과 유사하게, **토큰별 Advantage로 로그확률을 가중**하는 접근으로 볼 수 있습니다.

    예를 들어, 에피소드 전체 보상 $R$에 대해 각 시점 $t$의 advantage $A_t = R - b$ (baseline 뺀 보상)을 산출하고 이를 $w_t$로 사용하여 **로그확률 경사에 곱하는** 방식은 REINFORCE와 유사합니다. PPO에서는 이때 **Critic(가치망)**을 학습시켜 baseline $V(s_t)\approx \mathbb{E}[R|s_t]$를 추정하지만, LLM 영역에서는 이러한 **value 학습이 어렵고 불안정**하다는 지적이 있습니다.


**지도교수님의 피드백**을 요약하면 다음과 같습니다:

- **Critic 도입에 대한 우려**: 제안한 Weighted-MTP에서 **Critic 함수**(가치 예측 모듈)를 활용하여 토큰 가중치를 부여하는 아이디어에 대해, 교수님은 **LLM에서 value estimation의 난이도**를 지적하셨습니다. 실제 사례로 **GRPO**(Group Relative Policy Optimization) 알고리즘을 언급하며, **PPO 대비 개선된 이유 중 하나가 복잡한 value 추정을 아예 제거**한 데 있음을 상기시켰습니다. **DeepSeek의 GRPO** 연구에서는 **별도 critic 네트워크 없이**, **하나의 프롬프트에 대해 다수의 응답을 생성하여 그룹 내 상대적인 보상으로 advantage를 계산**함으로써, **critic 학습을 제거**하고도 안정적인 RLHF 미세조정을 구현했습니다. 이 접근은 **대규모 언어모델의 RL 튜닝에서 가치망 학습이 갖는 불안정성**을 피하면서, **메모리와 계산비용을 약 50% 절감**하는 효과도 있었습니다. 따라서 교수님은 Weighted-MTP에서 제안된 critic 기반 토큰 가중화가 **비슷한 난항에 봉착할 우려**가 있다고 평가하신 것입니다.
- **대안 접근 권고**: Critic에 의존하지 않으면서 **토큰별 중요도를 반영**하는 다른 형태의 방법을 검토해보길 권하셨습니다. 특히 학생이 레퍼런스로 언급한 **NAACL 2025 논문**을 비롯하여, 교수님이 추가로 제시하신 **Rho-1: Not All Tokens Are What You Need** 논문에 주목하라고 하셨습니다. 이 연구들은 **토큰 수준의 selection/weighting**을 **value 네트워크 없이** 수행한 예시로 볼 수 있습니다. Rho-1의 **Selective LM**에서는 **reference LLM**으로 각 토큰의 **유용성을 점수화**하고, **Loss에 선택적으로 반영**하는 방법을 사용하였는데, 이는 critic 없이도 **토큰 중요도를 수식에 녹여낸 사례**입니다[huggingface.co](https://huggingface.co/papers/2404.07965#:~:text=Unlike%20traditional%20LMs%20that%20learn,matching%20%2038%20with). NAACL 2025에 발표될 유사 연구 역시 Critic 대신 다른 손실 구성으로 토큰 중요도를 고려했을 가능성이 높습니다 (예를 들어, **DPO 변형**이나 **토큰-level contrastive 학습** 등). 교수님은 이러한 **동향을 고려한 대안 설계**가 **더 성공 가능성이 높지 않겠느냐**는 견해를 밝히셨습니다.
- **연구 범위 및 리소스**: 제안된 연구 주제가 **현실적으로 우리 연구실에서 감당 가능한지**에 대해서는, *“7~8B 규모 모델의 full fine-tuning에 GPU 4장 (A6000급) 정도가 필요하며, 이것이 상시 가용하다면 실현 가능”*하다고 평가하셨습니다. 즉, **연구실 수준에서 충분히 도전 가능한 주제**로 보았습니다. 다만 이는 **효율적인 구현**을 전제로 하며, 불필요한 복잡도를 줄이는 것도 중요할 것입니다 (예: Critic 제거는 메모리절감에도 도움).
- **사전 준비 및 실험 전략**: 실험에 들어가기 전에는 **관련 선행 연구의 코드 베이스**를 신속히 구현하여 **기본 동작을 확인**하고, **예상대로 작동하는 부분과 그렇지 않은 부분**을 파악해 **유동적으로 다음 액션을 결정**하는 것이 좋겠다고 조언하였습니다. 또한 **초기 실험**은 제안한 방법의 효과를 검증하기 위해 **기존 연구 결과를 재현**하는 것부터 시작하길 권했습니다. 특히 언급하신 NAACL 2025 논문 (또는 Meta의 MTP 논문)이 **본 연구의 베이스라인**이 될 수 있으므로, **해당 논문의 결과와 평가 지표, 데이터셋을 재현**하며 시작하는 것이 타당하다고 하셨습니다. 이는 곧 아래 개선 방안에서 더 자세히 다루겠지만, **평가지표(metric)와 데이터셋 선정**도 선행연구를 참고해 결정하라는 의미입니다.

정리하면, **교수님의 피드백**은 Weighted-MTP 아이디어 자체의 **흥미로움**을 인정하면서도, **Critic 기반 설계의 위험성**을 지적하고 **보다 단순하고 검증된 대안 기법**을 고려하도록 유도하는 것입니다. 이제 이러한 조언을 염두에 두고, 현재까지의 **관련 연구 동향**과 **유사 아이디어들의 사례**를 조사한 결과를 토대로, 본 제안서의 **타당성 분석 및 개선 방향**을 모색하겠습니다.

## **3. 관련 연구 동향 및 유사 연구 사례 (표절 가능성 검토)**

2024년부터 2025년 중반에 이르기까지, **토큰 단위 최적화**와 **멀티 토큰 예측** 주제에서는 여러 흥미로운 연구들이 등장하여 본 제안과 부분적으로 맥을 같이하고 있습니다. 이러한 연구들의 접근법과 본 연구아이디어의 **중복 여부(참신성)**를 검토합니다.

### **3.1 멀티 토큰 예측 (Multi-Token Prediction, MTP)**

- **Meta의 MTP (Better & Faster LLM via MTP)**: 2024년 Meta(Facebook) 연구진은 **MTP 훈련 패러다임**을 소개하며, **동일한 모델 용량과 학습량으로도 성능을 향상**시킬 수 있음을 보였습니다. 이 방식은 **각 위치에서 다음 n개의 토큰을 한꺼번에 예측**하도록 **모델에 n개의 병렬 출력 헤드**를 두고 **보조적(auxiliary) 과제**로 학습시키는 것입니다. 놀랍게도, 이렇게 멀티토큰 예측으로 보조 학습하면 **추가적인 계산 비용 없이**(동일한 스텝당 forward 계산을 활용) **다운스트림 성능이 향상**되었습니다. 특히 모델 크기가 클수록 효과가 컸는데, **코드 생성**과 같은 생성 작업의 경우 MTP로 훈련한 모델이 기존 Next-Token 모델보다 **몇 퍼센트 포인트 높은 정확도**를 보였습니다. 또한 MTP 헤드 중 하나만으로 평소처럼 다음 토큰을 생성하면 되므로, **추론 시에도 추가 지연이 없고**, 원한다면 **나머지 헤드들을 활용한 병렬 토큰 생성(자기추측 디코딩)**으로 **실질적 속도 향상**도 얻을 수 있었습니다. 다만 자연어 일반 도메인에 대한 MTP의 효과는 mixed한 결과도 보고되었는데, 한 커뮤니티 분석에서는 **소설 등 자연언어에 대해서는 MTP가 perplexity 개선이 미미하거나 오히려 어려움이 있었다**는 논의도 있었습니다. 이는 MTP가 **텍스트 도메인에 따라 상이한 영향**을 미칠 수 있음을 시사하며, 따라서 **Weighted-MTP 평가 시 다양한 과제**에서 검증이 필요함을 의미합니다.
- **Leap MTP (L-MTP)**: 2025년 5월에는 **L-MTP: Leap Multi-Token Prediction**이라는 후속 연구가 등장했습니다. L-MTP는 기존 MTP가 항상 **인접한 다음 토큰들을 예측**하는 데 반해, **보다 긴 거리를 도약(leap)한 토큰 예측**으로 확장한 방법입니다. 예컨대, 인접 토큰뿐 아니라 몇 단어 건너뛰어 **문맥상의 다음 중요한 단어**를 예측하도록 하여 **병렬 예측 폭을 늘리는** 아이디어입니다. 이는 **MTP의 범위를 넓혀** 보다 큰 **병렬성**을 확보하려는 시도로, 추론속도를 추가로 개선하거나 학습시 **멀리 떨어진 의존성도 한 번에 학습**하는 장점이 있을 것으로 기대됩니다. L-MTP 연구는 *“보다 넓고 빠르게 (broader & faster)”*라는 철학으로 MTP를 확장했고, **멀티토큰 병렬 생성의 한계를 극복**하기 위한 방향을 제시하였습니다. 이러한 MTP 계열 연구들은 Weighted-MTP의 **멀티토큰 기반**이 시의적절하고 유용한 아이디어임을 뒷받침합니다. 본 연구는 이러한 **MTP 구조 위에 토큰 가중화**를 더하려는 것이므로, **MTP 자체는 복수의 선행연구로 타당성이 검증**되고 있다고 볼 수 있습니다.

### **3.2 토큰 중요도 기반 학습 (Token-Level Weighting & Selection)**

- **Rho-1 (Selective Language Modeling)**: 앞서 소개한 Microsoft Research의 **Rho-1**은 **모든 토큰에 균일 손실을 적용하는 표준 언어모델 학습을 재고**한 선구적 연구입니다. 저자들은 **사전학습 시 각 토큰의 학습 dynamics**를 분석한 결과, **토큰마다 손실 기여 양상이 다르다**는 점에 착안했습니다. 그리고 **“학습에 진짜 필요한 토큰만 골라 훈련하자”**는 파격적인 접근을 제안하였죠. **구현적으로**, 먼저 기존 모델(레퍼런스 모델)을 이용해 대량 코퍼스의 각 위치별 **토큰 손실값을 계산**하고, 해당 **토큰이 현재 모델 분포에 비해 얼마나 어려운지**를 **점수화(score)**합니다[huggingface.co](https://huggingface.co/papers/2404.07965#:~:text=Unlike%20traditional%20LMs%20that%20learn,matching%20%2038%20with). 이 점수가 일정 임계 이상인 토큰들, 즉 **“유용한”(또는 중요한) 토큰들만 선택**하여 **언어모델을 계속 학습**시켰습니다. 이 방법을 **Selective Language Modeling (SLM)**이라 명명하며, 결국 손실 함수를 다음과 같이 **선택적 형태**로 바꾼 셈입니다:L*SLM*=−E*x*∼*Dt*∈*S*(*x*)∑log*Pθ*(*xt*∣*x*<*t*),

    LSLM=−Ex∼D  ∑t∈S(x)log⁡Pθ(xt∣x<t) ,

    여기서 $S(x)$는 **선택된 유용 토큰들의 인덱스 집합**입니다 (나머지 토큰은 손실 계산에서 제외)[arxiv.org](https://arxiv.org/abs/2404.07965#:~:text=corpus%2C%20Rho,on%20tokens%20with%20higher%20scores). Rho-1의 결과는 놀라웠는데, **OpenWebMath** 대형 수학 코퍼스로 150억 토큰 규모의 **continual pretraining**을 수행한 결과, **단 3%의 토큰만 학습하고도** **DeepSeekMath 수준의 수학 성능**을 달성했습니다. 또 **일반 도메인 800억 토큰**을 선택적 학습한 경우에도, **15가지 과제에서 평균 6.8% 성능 향상**을 이루어 **효율과 성능을 동시에 잡는** 잠재력을 보였습니다. Rho-1은 Weighted-MTP와 **아이디어의 결이 상당히 유사**합니다. 둘 다 *“Not All Tokens Are What You Need”*라는 기본 철학 아래, **모든 토큰에 동일한 학습비용을 쓰지 말자**는 것이죠. 차이는 Rho-1은 **다음-토큰 예측** 기준에서 **토큰 서브셋을 아예 드롭**해버렸다면, Weighted-MTP는 **멀티토큰 예측** 상황에서 **토큰별 가중치를 연속적으로 부여**한다는 점입니다. **중복성 측면**에서 보면, Weighted-MTP의 핵심 아이디어인 *“토큰별 중요도 차등 부여”*는 Rho-1과 맥을 같이하지만 **동일한 접근은 아니며**, Rho-1처럼 완전히 선택/탈락 이분법이 아닌 **연속 가중치 방식**이라는 차별점이 있습니다. 또한 Rho-1은 **Critic 없이** **레퍼런스 모델 기반 점수**로 이루어졌기에, Weighted-MTP에서도 이같은 **참조모델 활용** 아이디어를 차용할 여지가 있습니다 (개선방안에서 후술).

- **Token Weighting for Long-Range LM**: 2025년 초, 독일 Darmstadt 대학의 연구[arxiv.org](https://arxiv.org/html/2503.09202v1#:~:text=Many%20applications%20of%20large%20language,the%20confidences%20of%20a%20long)[arxiv.org](https://arxiv.org/html/2503.09202v1#:~:text=multiple%20long,can%20be%20found%20on%20Github)에서는 **긴 컨텍스트(long context)**에서 LLM이 성능을 내지 못하는 문제를 **학습 시 토큰 가중치 조정**으로 개선할 수 있다는 결과를 보고했습니다. 저자들은 **긴 문맥 이해에 어려움**을 겪는 원인 중 하나로, **기존 next-token 학습이 모든 토큰을 동일하게 취급**하기 때문에 **장기 의존(token 간 거리가 김)에 해당하는 토큰도 쉽게 간과**된다고 보았습니다[arxiv.org](https://arxiv.org/html/2503.09202v1#:~:text=Many%20applications%20of%20large%20language,the%20confidences%20of%20a%20long). 예를 들어, 소설에서 5000토큰 전에 등장한 인물이 재등장하는 경우, 현행 모델은 해당 이름 토큰을 예측하기 어려운데도 그 **오류에 특별한 패널티를 부여하지 않으므로** 학습이 잘 안된다는 것입니다[arxiv.org](https://arxiv.org/html/2503.09202v1#:~:text=qu%20%C3%AD%20ades%20%E2%80%99%20books). 이를 해결하기 위해, **token-weighting scheme**이라는 방법을 고안했는데, **동일 입력을 짧은 문맥으로 봤을 때와 긴 문맥으로 봤을 때의 모델 확신도 차이**로 각 토큰의 **장기 의존 중요도**를 계산합니다[arxiv.org](https://arxiv.org/html/2503.09202v1#:~:text=that%20conventional%20next,uniform%20loss%20weights)[arxiv.org](https://arxiv.org/html/2503.09202v1#:~:text=methods%20using%20a%20two,code%20can%20be%20found%20on). 간단히 말해, **짧은 문맥 모델과 긴 문맥 모델의 loss 차이**가 크다면, 해당 토큰은 **긴 문맥이 특히 필요한 토큰**이므로 학습 시 **가중치를 크게 주고**(더 집중 학습)[arxiv.org](https://arxiv.org/html/2503.09202v1#:~:text=context%20needed%20to%20predict%20the,token%20scoring%2C%20including%20models%20that)[arxiv.org](https://arxiv.org/html/2503.09202v1#:~:text=multiple%20long,can%20be%20found%20on%20Github), 반대로 차이가 적거나 trivial한 토큰(예: 자주 나오는 불용어 등)은 가중치를 낮춰 덜 학습하는 방식입니다. 이 기법을 통해 **긴 문맥 의존 과제들의 성능이 전반적으로 향상**되었고, 심지어 **참조용 짧은 문맥 모델은 훨씬 작은 모델을 써도** 효과적으로 토큰 스코어링을 할 수 있음을 보여주었습니다[arxiv.org](https://arxiv.org/html/2503.09202v1#:~:text=multiple%20long,can%20be%20found%20on%20Github). 이는 Weighted-MTP에서 고려하는 **“참조 모델로 토큰 중요도 산정”** 아이디어와도 통하며, **토큰별 loss weight 조정이 특정 능력 향상에 직접적 효과**를 준 사례입니다. 표절 관점에서 보면 이 연구는 **Weighted-MTP와 문제의식은 유사**하나, **멀티토큰 예측이나 RL 요소는 없고**, 특정 **긴 문맥 시나리오에 특화**되었다는 점에서 다릅니다. 다만 *“모델 두 개를 활용해 토큰 중요도 평가”*라는 테크닉은 Rho-1, 본 연구 모두 접목 가능한 아이디어입니다.
- **Critical Tokens Matter (Token-Level Contrastive Estimation)**: 2024년 말에는 **추론 능력(논리/수학적 reasoning) 향상**을 위해 **중요 토큰 식별**을 접목한 연구들도 등장했습니다. Lin 등의 *“Critical Tokens Matter”* 논문은, **Chain-of-Thought** 스타일의 복잡한 추론 문제에서 **정답의 옳고 그름이 일부 핵심 토큰 결정에 달려있다**는 가정 하에, **대조 학습(contrastive estimation)**을 통해 **중요 토큰을 판별하고 학습을 강조**하는 방법을 제안했습니다. 구체적으로는, **모델의 정답 출력과 오답 출력을 비교**하여 **양측 간 가장 예측 분포 차이가 큰 토큰**들을 **“결정적 토큰”**으로 간주하고, 모델이 **이 토큰들을 더 확실하게 맞히도록 대조적 손실**을 부여합니다. 이는 DPO(Direct Preference Optimization)의 변형으로 볼 수도 있는데, 기본 DPO가 응답 전체의 좋고 나쁨만 비교하는 데 비해, **cDPO**라 불리는 *contrastive DPO* 기법으로 **토큰별로 정답/오답 간 차이를 극대화**하도록 한 것입니다. 이러한 방법은 **추론 단계별로 모델이 중요한 결정**(예: 수학문제 풀이의 중간 계산 결과 등)을 틀리지 않게 유도하여 **전체 정답률을 높이는 효과**를 보였습니다. **Weighted-MTP**의 맥락에서 보면, Critical Tokens Matter 역시 *“중요 토큰에 집중”*한다는 사상을 공유하지만, **contrastive한 토큰 식별** 및 **응답쌍 비교** 등 RL보다는 **self-supervised 대조 학습** 방식에 가깝습니다. 또한 이 연구는 **멀티토큰 예측 개념은 포함하지 않으므로**, 본 연구 아이디어와 직접적 중복은 아니나, **토큰별 중요도 부여의 효과**를 입증한 또 하나의 예로 참고할 만합니다.

### **3.3 RLHF 및 Preference Optimization에서의 토큰 수준 기법**

Weighted-MTP 제안에는 인간 피드백에 의한 보상 최적화(RLHF) 맥락이 직접 언급되지는 않았지만, **Critic을 활용**하여 어떤 **보상 함수**를 최적화하려는 구상이 있었다면 RLHF와 유사한 설정일 수 있습니다. 최근 **LLM 정렬(alignment)** 연구에서 **PPO 대안 기법**과 **토큰별 보상 분해**에 관련된 동향을 정리합니다:

- **DPO (Direct Preference Optimization)**: RLHF의 대안으로 2023년에 등장한 DPO는 **학습된 보상모델을 사용하지 않고** 곧바로 **인간 선호 비교 데이터**로부터 **정책을 업데이트**하는 방법입니다. 요컨대, 인간이 선택한 **우수 응답** $x^+$과 열등 응답 $x^-$ 간의 **랭킹 정보**를 이용하여, 정책 $\pi_\theta$가 $x^+$를 $x^-$보다 선호하도록 **로지스틱 손실**을 최적화합니다. 식으로는 다음과 같은 **implicit reward** 최적화로 나타낼 수 있습니다:L*DPO*(*θ*)=−log*σ*(log*πθ*(*x*+∣*p*)−log*πθ*(*x*−∣*p*)),

    LDPO(θ)=−log⁡σ ⁣(log⁡πθ(x+∣p)−log⁡πθ(x−∣p)) ,

    여기서 $p$는 프롬프트, $\sigma$는 sigmoid 함수입니다. DPO는 **별도 가치망이나 PPO 훈련 없이도** RLHF와 유사한 효과를 낼 수 있음을 보여주었습니다. 그러나 **교수님이 지적하신 부분**은, **DPO의 기본 손실은 응답 전체의 우열만 반영하고 토큰별 중요도 차이를 고려하지 않는다**는 점입니다. 실제로 DPO 최적화 과정에서 모델은 **좋은 답변 쪽의 모든 토큰 확률을 올리고, 나쁜 쪽의 토큰 확률을 내리려는 경향**을 갖지만, 이는 앞서 예시한 “safe” vs “risk” 같은 **치명적 토큰 하나의 잘못**과 **나머지 부분**을 구별하지 못합니다. 결과적으로 DPO는 **sampling 분포 변화 문제** 등을 겪으며, 중요한 토큰 하나가 틀려 전체 응답 품질이 달라지는 경우를 충분히 학습에 반영 못할 수 있습니다.

- **Token-level DPO 변형들**: 이러한 한계를 인식하여, 2024년경부터 **DPO를 토큰 단위로 세분화**하려는 다양한 시도가 나오고 있습니다. 대표적으로 **TIS-DPO**(Token-level Importance Sampling DPO)에서는 **좋은 응답과 나쁜 응답의 토큰별 확률차**를 이용해 **토큰 중요도 weight**를 추정하고, 이것으로 DPO 손실을 가중화하였습니다. 확률차가 큰 토큰일수록 사람 선호에 큰 영향이라 보고 크게 가중치 주는 접근이지만, **contrastive 모델 출력이 일관되지 않을 때 편향 발생** 등 한계가 있다는 평가를 받았습니다. **TDPO**, **RTO**, **LogicRL** 등도 토큰 수준으로 선호정렬 문제를 다루려 한 연구들인데, 이들 중 일부는 **고정적인/단순 가중치 스킴**을 적용하여 충분히 정교하지 못하다는 지적이 있습니다. 예컨대 **일정 위치 이후 토큰에 가중치 주기** 등의 단순 규칙은 semantic한 중요도를 반영하지 못한다는 것입니다.
- **TI-DPO (Token-Importance Guided DPO)**: 2025년 등장한 최신 연구로, **중국과 MS연구팀의 합작**인 TI-DPO는 위의 토큰-level DPO 시도를 **한층 발전**시켰습니다. 두 가지 핵심 아이디어가 있는데, 하나는 **gradient-based token importance**입니다. **보상모델 출력에 대한 각 토큰 임베딩의 기여도(그래디언트 크기)**를 계산하여, **실제로 최종 선호 점수에 영향력이 큰 토큰들에 높은 가중치**를 주는 방식입니다. 이는 **모델 내재적으로 어떤 토큰이 판정을 가르는지**를 계산하는 보다 **동적인 방법**으로, 확률 등 heuristic보다 신뢰도를 높였습니다. 다른 하나는 **triplet loss 도입**인데, 기존 DPO는 $x^+$ vs $x^-$의 **이분 비교**만 하는 반면, TI-DPO는 **“좋음-중간-나쁨”**의 3자 관계로 확장하여, **중간 수준 샘플**을 명시적으로 좋은 쪽으로 끌어올리고 나쁜 쪽에서 멀어지도록 학습시켰습니다. 이렇게 하면 **이산적인 선호 비교 대신 연속적인 향상 신호**를 주어 **안정적이고 세밀한 학습**이 가능합니다. TI-DPO는 이론적으로도 **DPO 대비 수렴 안정성이 향상됨을 증명**하였는데, **DPO보다 더 낮은 bound의 손실을 보장**하여 최적화가 용이함을 보여주었습니다. 실험적으로도 **TruthfulQA, IFEval 등의 선호 비교 벤치마크에서 최고 정확도**를 기록하며, **기존 RLHF 방법들보다 안정적이고 효율적**임을 보고했습니다. TI-DPO는 Weighted-MTP에 시사하는 바가 큽니다. **Critic(가치망) 없이도** Preference 최적화를 할 수 있을 뿐 아니라, **토큰별 gradient 신호로 중요도를 계산**해 **가중치를 동적으로 조정**했다는 점에서, Weighted-MTP의 **“토큰 중요도 가중” 개념을 정교하게 실현**한 예라 할 수 있습니다. 비록 TI-DPO는 인간 선호 정렬 문제에 집중했지만, **그 개념과 수식(gradient weighting, triplet loss)**은 일반적인 보상 신호나 과제 성능 최적화에도 응용 가능할 것입니다.
- **GRPO 및 기타**: 앞서 교수님 코멘트에서도 나온 **GRPO**는 **Proximal Policy Optimization (PPO)**을 대체하기 위해 고안된 기법으로, **Advantage 추정을 value함수 대신 여러 샘플의 그룹 내 상대값으로** 수행합니다. PPO가 각 시점 step별로 $(R_t - V(s_t))$ 형태의 advantage로 정책을 업데이트한다면, **GRPO는 하나의 프롬프트에서 생성된 K개의 응답에 대해 얻은 K개의 보상 점수를 정규화하여** 각 응답의 advantage로 사용합니다. 이렇게 하면 **학습해야 할 critic 네트워크가 사라지며**, PPO의 clipping, KL-penalty 등 정책 업데이트 룰만 남게 됩니다. GRPO는 **수식적으로 REINFORCE의 baseline을 샘플 평균으로 둔 것과 유사**하며, 구현은 간단하지만 **충분히 많은 샘플(K≈1000)을 필요**로 하기에 오픈소스 맥락에서는 다소 부담될 수 있습니다. 그럼에도 **DeepSeek 사례**처럼 강력한 사전학습 모델을 바로 RLHF로 후처리할 때 효과적인 것으로 나타났습니다. Weighted-MTP에서 만약 RLHF류의 보상 최적화를 염두에 두고 있다면, **GRPO 스타일로 critic 없이 그룹 기반 학습**을 고려해볼 수 있습니다. 이때도 **토큰별로 reward를 쪼개는 대신 전체 응답별로 비교**하게 되지만, Weighted-MTP의 MTP출력 구조와 결합하여 **한 프롬프트에 대한 다중 토큰 시퀀스들의 그룹**으로 확장할 여지가 있을 것입니다.
- **T-REG (Token-Level Reward Regularization)**: 2024년 Zoom AI 팀의 T-REG 연구는 또 다른 독특한 접근으로, **LLM 스스로 생성한 토큰별 보상**을 활용하여 RLHF를 개선했습니다. 인간이 일일이 토큰 단위로 피드백 주는 것은 어렵기 때문에, **모델이 자기 응답을 두 가지 방향(더 좋은 쪽, 더 나쁜 쪽)으로 살짝 수정해보는 contrastive prompting**을 합니다. 그리고 **두 버전의 출력에서 각 토큰의 확률 변화**를 측정하여, **해당 토큰이 더 좋은 응답에 기여하는지**를 추론, 이를 **토큰별 reward 신호**로 삼는 것입니다. 이렇게 **AI가 자동 생성한 토큰-level 보상**을 **정규화 항**으로 DPO 학습에 추가하여 (sequence-level 보상 최적화는 그대로 두고) **모델이 자연스럽게 token credit assignment를 하도록 유도**했습니다. T-REG은 **별도 critic이나 human 라벨링 없이**도 token-level 신호를 줄 수 있는 흥미로운 방식이며, Weighted-MTP의 맥락에서도 **Critic 없이 토큰 중요도를 얻는 창의적 방법**으로 참고될 수 있습니다.

이상 살펴본 관련 연구들을 통해 **표절 가능성**을 검토해보면, **Weighted-MTP와 완전히 동일한 연구는 현재까지 없는 것**으로 판단됩니다. 유사한 요소들은 각기 존재하지만, 본 연구제안서의 **특정 조합** – *멀티토큰 예측 + 토큰별 가중 학습 + (잠재적으로 RL critic 활용)* – 은 여전히 독창적인 부분이 있습니다:

- **멀티토큰 예측 + 토큰 weighting**: MTP 자체는 Meta, L-MTP 등이 다루었고, 토큰 weighting은 Rho-1, Token Weighting for Long-Range 등 다루었지만, **두 가지를 결합**한 예시는 찾아보기 어렵습니다. 예컨대 Rho-1은 next-token 예측에서 selective training을 했지 MTP 환경은 아닙니다. Meta MTP는 모든 토큰 동등하게 처리했습니다. **따라서 Weighted-MTP의 조합은 새로운 탐구 영역**입니다.
- **Critic 활용 토큰 가중 RL**: RLHF 분야에서 token-level 접근(TI-DPO 등)은 등장했으나, 이는 **human preference** 정렬 맥락입니다. Weighted-MTP가 꼭 인간 피드백이 아니더라도, **어떤 외부 보상**(예: 정답 정확도 등)을 토큰별로 전파하려는 것이라면, 위 연구들과 개념은 통하지만 구현 디테일은 다를 것입니다. 특히 **MTP 구조를 RLHF에 쓴 경우**는 아직 많지 않아, Weighted-MTP의 시도는 **선행 연구의 조각들을 새롭게 결합**하는 측면이 강합니다. 최근 GRPO, DPO 등으로 **critic 없는 RL**이 주목받는 와중에, Weighted-MTP도 그런 흐름을 반영하면 참신성을 유지하며 개선될 수 있습니다.
- **핵심 아이디어의 독창성**: “**중요한 토큰에 더 집중하여 더 좋은 LLM을 만들자**”는 큰 방향은 여러 연구에서 부분적으로 암시되어 왔지만, **이를 한데 묶어 체계화**한 연구는 아직 없습니다. 예컨대 Reddit 토론에서도 *“모델도 인간처럼 쉬운 토큰은 빠르게 넘기고 어려운 부분에 더 노력하면 좋겠다”*[reddit.com](https://www.reddit.com/r/mlscaling/comments/1chia40/better_faster_large_language_models_via/#:~:text=I%20think%20we%20need%20to,token%20compute)는 아이디어가 있었는데, Weighted-MTP는 바로 그러한 **adaptive per-token computation**의 구현을 목표로 합니다. 이는 여전히 발전 중인 개념이기에, **표절보다는 시의적절한 문제 설정**으로 보는 것이 맞습니다.

결론적으로, 관련 연구 조사는 **본 연구제안서의 방향이 최신 트렌드와 맥을 같이하여 유망**함을 보여줌과 동시에, **구현상의 교훈과 개선점**도 시사합니다. 다음 장에서는 이러한 인사이트를 바탕으로 **연구제안서의 개선 방안**을 구체적으로 제안합니다.

## **4. 개선 방안 제안 (기술적 구조 및 손실함수 보완)**

위에서 살핀 교수님 코멘트와 관련 연구를 토대로, **Weighted-MTP의 핵심 뼈대는 유지**하되 **비현실적이거나 위험 요소를 개선**할 수 있는 방안을 제시합니다. 주요 개선 목표는 **(1) Critic 기능의 재설계 또는 대체**, **(2) 손실 함수 수식의 엄밀하고 효과적인 구성**, **(3) 실험 검증 전략 구체화**입니다.

### **4.1 Critic 사용에 대한 대안: 참조 모델 및 직관적 보상 함수**

**교수님의 최대 우려는 Critic(가치망) 학습의 불안정성**이었습니다. 이를 해소하기 위해, **가능하면 별도의 Critic 네트워크 학습을 배제**하고도 **토큰 가중치 신호를 얻는 방법**으로 전환할 것을 권장합니다:

- **(A) 참조 모델 기반 토큰 스코어링**: Critic 대신, **고정된 Reference 모델**을 활용하여 **토큰별 중요도**를 계산하는 방법입니다. 이는 Rho-1[huggingface.co](https://huggingface.co/papers/2404.07965#:~:text=Unlike%20traditional%20LMs%20that%20learn,matching%20%2038%20with)이나 Long-Range Token Weighting 연구[arxiv.org](https://arxiv.org/html/2503.09202v1#:~:text=schemes%20that%20assign%20different%20weights,context)에서 효과가 입증되었습니다. 예컨대, 우리 **기본 LLM (예: 7B 모델)**과 같은 구조의 **소형 LLM (예: 1.3B)**을 준비해 두고, 학습 데이터의 각 토큰에 대해 **소형 모델의 예측 손실이나 perplexity**를 측정합니다. **예상 손실이 큰 토큰**은 **현재 모델에게 어려운 사례**이므로 더 배우는 게 이득입니다[huggingface.co](https://huggingface.co/papers/2404.07965#:~:text=Unlike%20traditional%20LMs%20that%20learn,matching%20%2038%20with). 이를 점수 $s_t$로 두고, **가중치 $w_t = f(s_t)$** (예: $s_t$가 높으면 $w_t=1$ 아니면 0, 혹은 $w_t = \text{min}(s_t / \tau, 1)$ 같은 스케일링)로 정합니다. 이렇게 하면 **Critic 없이 사전 계산된 점수로 손실에 가중치**를 곱할 수 있습니다:L*WMTP*′=−*j*=1∑*Nwt*+*j*(*x*)log*Pθ*(*xt*+*j*∣*x*<*t*),

    LW-MTP′=−∑j=1Nwt+j(x)log⁡Pθ(xt+j∣x<t) ,

    여기서 $w_{t+j}(x) = f\big(\text{Score}*{ref}(x*{t+j}, x_{<t})\big)$ 입니다. 이 방식은 **학습 도중 $w$를 동적으로 바꾸지 않기에 RL은 아니지만**, **여러 epoch에 걸쳐 정적 참조모델로 중요토큰 식별→모델 훈련→참조모델 업데이트**의 iterative 방법도 고안할 수 있습니다. **장점**은 구현이 단순하고, Critic처럼 훈련되지 않는 모듈이므로 **불안정성이 낮다**는 것입니다. Rho-1 사례에서 보듯, **이렇게 토큰을 취사선택/가중**해도 **성능 향상**이 컸던 만큼, Weighted-MTP에도 충분히 적용 가치가 있습니다. 특히 **교수님이 첨부한 Rho-1 논문**이 이 접근을 대표하므로, 제안서에 해당 방법론을 적극 반영하면 피드백을 수용한 개선이 될 것입니다.

- **(B) 명시적 보상 함수로 가중치 부여**: 만약 Weighted-MTP에서 의도한 바가 **“모델 출력이 특정 기준에 맞도록”(예: 사실 여부, 정답률 등)** 하는 것이라면, Critic 대신 **그 기준 자체를 수식으로 활용**할 수 있습니다. 예를 들어, **정답을 아는 환경**(수식 계산, 퍼즐 등)에서는 **정답과 모델 출력을 비교**하여 각 토큰에 **정/오표시**를 할 수 있습니다. 최종 보상 $R$을 “**전체 정답 여부**”로 정의하되, **토큰별로 partial credit**을 부여하는 규칙을 만들어볼 수 있습니다. 간단히는, *정답 토큰 위치에 점수, 오답 토큰에 페널티*를 주는 함수를 만들고 이를 $w_t$로 삼아 supervised loss에 곱하는 식입니다. 이는 RL을 거치지 않고 **task-specific heuristic reward**를 바로 활용하는 것으로, **Critic 학습 단계를 생략**합니다. 다만 이 접근은 범용성이 떨어지고, 자칫 **manual reward design**이 되어버리므로, 가능하다면 **일반적 레퍼런스 모델 차이**나 **확률 기반** 방법이 더 낫습니다. 그럼에도, 만일 Weighted-MTP의 실험을 특정 벤치마크(예: MATH, 알고리즘 문제 등)로 시작한다면, **평가 척도를 미리 녹여 손실에 활용**하는 것도 초반 튜닝에는 도움될 수 있습니다.
- **(C) Self-Critic 또는 AI Feedback 활용**: Critic을 인간이 아니면서 학습시키는 대신, **LLM 자기 자신 또는 별도 LLM**이 **피드백을 제공**하도록 할 수 있습니다. 최근 **T-REG**처럼 모델에게 **“어떤 토큰이 더 나은 답을 만드는 데 중요했는지”** 자체 점검을 시키는 방법이 대표적입니다. 예컨대, **모델 출력에 대해** *“이 답변에서 핵심적으로 잘해야 할 부분은 어디인가?”*를 묻거나, 답을 조금 바꾸어 **토큰 중요도를 자기평가**하도록 프롬프트를 넣어볼 수 있습니다. 아직 확립된 기법은 아니지만, **AI 안전성 영역**에서 ChatGPT 등을 활용해 **토큰 라벨링**을 시도한 사례도 있는 만큼, 본 연구에도 접목 가능한 보조 수단입니다. 이 역시 **Critic 네트워크 훈련**을 피하는 전략이므로, 보상모델 없이 **토큰-level 신호**를 얻는 방식으로 고려될 수 있습니다.

요약하면, **Critic에 대한 대안**은 **reference LLM**이나 **직접적 신호**로 토큰 중요도를 구하는 것입니다. 이러한 접근들은 **교수님이 언급한 GRPO나 NAACL25 대안들과 궤를 같이하며**, Weighted-MTP의 **복잡도를 낮추고 안정성을 높이는** 결과를 가져올 것입니다.

### **4.2 손실 함수 및 학습 수식 개선**

**Weighted-MTP 손실 함수**를 보다 **엄밀하게 구성**하기 위해, **기존 제안의 수식**을 재검토하고 앞서 언급된 기법들의 **이론적 장점**을 통합합니다:

- **(A) 토큰 중요도 가중치 산출의 이론적 기반**: 단순히 $w_t$를 heuristic하게 정하는 대신, **정보 이론적 또는 통계적 근거**를 사용할 수 있습니다. 예를 들어 **정보 이득(information gain)** 개념을 적용해, 한 토큰을 정확히 맞히는 것이 전체 시퀀스 확률에 주는 영향도를 정량화할 수 있습니다. TI-DPO는 **보상모델 $R_\phi$에 대한 각 토큰 임베딩의 기여도 $\nabla h_\phi$**로 중요도를 구했는데, 우리 경우 보상모델 대신 **모델의 로짓 또는 손실 함수 자체에 대한 기여도**를 볼 수 있습니다. 만약 어떤 토큰이 모델에게 **예측 난이도가 높아 손실 기여도가 크다면**, 그것은 gradient $\frac{\partial \mathcal{L}}{\partial z_t}$ (여기서 $z_t$는 토큰 $t$의 로짓)에 반영됩니다. 이러한 **그래디언트 크기**를 **토큰 weight**로 사용하거나, 적어도 **weight 산출의 참고치**로 활용할 수 있습니다. 이 접근의 장점은, **모델 내부의 미분 가능 신호를 활용**하므로 **편향이 적고 동적으로 변화**한다는 것입니다. 다만 단점은 gradient 계산 비용이 추가되고 노이즈가 있을 수 있다는 점인데, TI-DPO 연구에서는 이 기법으로 **학습 안정성 향상** 및 **이론적 bound 개선**까지 이뤄졌으므로, 충분히 시도 가치가 있습니다. Weighted-MTP에서도, 예를 들어 **Critic 없이 레퍼런스 모델로 1차 weight 결정 → 그 weight로 학습하면서, 미세하게 gradient로 보정**하는 2단계 접근도 가능합니다.
- **(B) 손실 함수 구조: 혼합 정책/행위자-비참조자**: Weighted-MTP가 RL 요소를 도입하려 했다면, **DPO나 SimPO** 같은 **Reference-free** objective를 사용할 것을 제안합니다. PPO 대신 DPO류 방법을 쓰면 **explicit한 KL 페널티나 참조 모델 필요 없이** 바로 현재 정책을 업데이트할 수 있습니다. 특히 **SimPO (Simple Preference Optimization)**는 **참조 모델 없이** DPO와 유사한 역할을 하도록 정식화되어, **별도 SFT 모델 없이도** 선호 최적화가 가능함을 보였습니다. SimPO의 아이디어를 본 과제에 적용한다면, **Critic/reference 없이** Weighted-MTP 정책 자체를 미세조정하게 할 수 있습니다. 구체적 수식은 복잡할 수 있으나, DPO 기반으로 **“좋은 출력일수록 토큰 확률 높이고, 나쁜 출력일수록 낮추는”** 방향의 손실에, 앞서 말한 **토큰 중요도 weight** $w_t$를 곱해주면 될 것입니다. 이때 **손실 함수는 이중 합**(응답 차원과 토큰 차원)으로 전개되겠지만, 이는 TI-DPO 논문 등에서 충분히 유도된 바 있습니다. 이러한 혼합 손실은:L(*θ*)=−E(*x*+,*x*−)[*t*=1∑*Twt*⋅log*σ*(Δ*t*log*πθ*(*xt*+)−log*πθ*(*xt*−))],

    L(θ)=−E(x+,x−)[∑t=1Twt⋅log⁡σ(log⁡πθ(xt+)−log⁡πθ(xt−)⏟Δt)] ,

    와 같이 상정할 수 있습니다 (여기서 $x^+, x^-$는 동일 프롬프트에 대한 선호/비선호 응답). $\Delta_t$는 **같은 위치 토큰에 대한 두 응답의 대조 신호**이고, $w_t$는 **해당 토큰 위치가 전체 응답 품질에 중요한지**를 나타내는 가중치입니다. 이 수식은 한 가지 아이디어 뼈대이며, 실제 구현에서는 $\Delta_t$ 대신 **응답 전체 $\Delta$에 토큰 기여도를 곱한 형태**가 될 수도 있습니다. 핵심은, **토큰별로 차등 업데이트**를 한다는 점입니다. 이러한 설계는 **한쪽 응답에서만 발생하는 토큰** (예: 좋은 답에만 있는 핵심 문구, 나쁜 답에만 있는 오류 표현)을 **집중적으로 학습/벌점** 줄 수 있게 합니다. 이는 **Critic이 모든 토큰에 균일 영향 주는 PPO와 대조적**인, 보다 fine-grained한 최적화입니다. 이같은 **수식적 개선**은 연구제안서를 **보다 엄밀하고 학술적으로 탄탄하게** 만들어 줄 것입니다.

- **(C) 수식 검증과 이론적 분석**: 개선된 손실 함수를 제안할 때, **이론적 근거**를 함께 제시하면 연구 타당성이 높아집니다. 예를 들어, *“우리의 가중치 부여 손실은 기존 손실 대비 variance를 감소시킨다”*거나 *“모델이 중요 토큰에서 발생시키는 cross-entropy를 상계함으로써 더 낮은 일반화 오류를 보인다”* 같은 주장을 검증하는 것입니다. TI-DPO의 경우 **토큰 중요도 가중이 DPO의 불안정성 문제를 완화하여 수렴 보장**을 향상시켰음을 증명했습니다. 우리도 간단히, **toy 문제**에서 Weighted-MTP가 수렴을 개선함을 보이거나, **MTP로 얻는 이득**을 수식으로 확인하는 등의 작업을 생각해볼 수 있습니다. MTP 자체는 **동적 프로그래밍 해석**을 통해, **k-토큰 예측이 1-토큰 예측보다 더 많은 정보를 한 번에 획득**함을 보일 수 있습니다. 토큰 가중화는 **중요도 높은 샘플을 oversample하는 효과**라 볼 수 있어 **importance sampling의 분산 감소 이론**을 인용할 수 있습니다. 이러한 **이론적 분석 추가**는 연구제안서의 깊이를 더해줄 것입니다.

### **4.3 실험 계획 및 평가 지표**

마지막으로, 개선된 Weighted-MTP 아이디어를 **어떻게 검증하고 평가할 것인지** 구체적인 전략을 수립하는 것이 중요합니다. 교수님 코멘트에서도 강조되었듯, **선행 연구 재현 -> 개선안 적용**의 단계로 접근합니다:

- **베이스라인 재현**: 첫 단계로, **NAACL 2025 논문이나 관련 대표 논문**의 결과를 재현합니다. 만약 NAACL25에서 학생이 참고한 논문이 **Rho-1**이었다면, **Rho-1의 selective pretraining 설정**을 작은 규모로 따라해볼 수 있습니다. 예를 들어 **OpenWebMath** 일부 데이터로 1.3B 모델을 **Selective LM**훈련하여, **MATH 벤치마크 성능 향상폭**을 검증해보는 것입니다. 혹은 NAACL25 논문이 **DPO 변형**이었다면, 공개된 **인간 선호 데이터셋**(예: OpenAI Summary, HH RLHF 등)으로 **DPO vs PPO** 성능을 비교 재현해볼 수 있습니다. 또 다른 베이스라인은 **Meta의 MTP 모델**입니다. Meta 논문에서는 **코드 모델 (GPT 계열)**을 MTP로 학습하여 HumanEval, MBPP 등을 평가했으므로, 공개된 **HumanEval** 문제들을 사용해 **MTP 학습 vs 표준학습** 성능차를 우리 구현으로 확인할 수 있습니다. 이러한 베이스라인 실험을 통해 **코드 구현의 정확성**을 담보하고, **평가에 사용할 지표와 데이터셋 선정에 감을 잡게** 됩니다.
- **데이터셋 및 Metric 선정**: 연구목표에 맞는 **벤치마크**를 선택하는 것이 중요합니다. **Weighted-MTP가 지향하는 바**가 예컨대 *“모델의 추론 정확도 향상”*이라면, **수학 문제(MATH), 논리 퍼즐, 코드 생성(HumanEval) 등**이 적절한 평가세트가 됩니다. *“모델의 인간 선호 순응도 향상”*이라면 **헌신적으로 RLHF 벤치마크**(TruthfulQA, HHH Alignment, Preference vs Toxicity trade-off 등)를 써야 할 것입니다. 또 *“모델의 언어모델 일반 능력 향상”*이라면 **GPT-4 성능 평가에 쓰이는 다양한 태스크**(MMLU, BIG-Bench 등)에서 향상 여부를 측정할 수 있습니다. 교수님께서는 **가능하면 기존 연구의 metric/dataset을 따르는 것**을 권하셨는데, 이는 **직접 비교 가능성**을 높여주고 향후 **연구 출판 시 독자 이해를 돕기 때문**입니다. 예컨대 Rho-1식 접근이라면 **few-shot 정확도 향상**(MATH 데이터셋 few-shot)으로 결과를 제시할 수 있고, DPO식 접근이라면 **승자 응답 비율** 또는 **상대정확도**를 쓸 것입니다. 또한 MTP의 효과는 **모델 크기별로 달라지므로**, 가능하면 **여러 스케일**(1B, 7B 등)에서 실험하여 **scaling 추세**를 관찰하는 것도 의미 있습니다.
- **실험 설계**: 개선안을 검증하기 위한 구체적 비교실험을 설계합니다. 예를 들어:
    - **실험1**: *“Critic 사용 유무에 따른 성능 차이”* – PPO vs GRPO vs DPO vs TI-DPO 등의 세팅으로 동일 데이터에서 미세조정하여, **학습 안정성(수렴 곡선)**과 **최종 성능**을 비교합니다. 기대는 Critic 제거/대체한 방법들이 더 안정/우수한지 확인하는 것입니다.
    - **실험2**: *“토큰 가중화 효과”* – 동일 MTP 구조에서, $w_t=1$ (균일)인 모델과, $w_t$에 우리 방법(예: reference scoring, gradient 등) 적용한 모델을 비교합니다. **평가지표**로는 **토큰 난이도별 오류율, 전체 perplexity, 다운스트림 정확도** 등을 볼 수 있습니다. 만약 Weighted-MTP 아이디어가 맞다면, **희소하게 발생하지만 중요한 오류**(예: 특정 중요한 토큰을 놓쳐 틀리는 케이스)가 감소하고, **전반적 성능**이 향상되는 양상을 보여줄 것입니다.
    - **실험3**: *“멀티토큰 예측의 기여”* – 멀티토큰 예측 (예: 4-token)으로 학습한 모델과 1-token 예측 모델을 비교하여, **학습 곡선**(같은 데이터에서 동일 epoch에 성능) 및 **추론 속도**를 평가합니다. MTP가 sample efficiency 개선을 가져오는지 확인하고, Weighted-MTP라면 그 효과가 더 뚜렷한지 관찰합니다.
    - **실험4**: *“조합 기법의 상승효과”* – 최종적으로, **토큰 가중 + MTP + (가능 시 RLHF)**를 한꺼번에 적용한 우리의 **완성 모델**을, 기존 방법들(Rho-1만 적용, MTP만 적용, RLHF만 적용 등)과 비교합니다. 여기서 긍정적인 **상승효과(synergy)**가 보인다면 연구의 가치를 입증할 수 있습니다. 예를 들어, MTP만 했을 때 5% 오르던 정확도가 거기에 토큰 가중까지 하니 8% 올랐다든지, RLHF만 했을 때보다 토큰 가중 RLHF가 더 안정적이었다든지 하는 결과가 나올 수 있습니다.
- **평가 측면**: 단순 성능 수치 외에도, **출력의 질적 변화**를 분석하는 것이 필요합니다. 토큰 가중화 기법은 특히 **출력의 특정 부분**에 영향줄 가능성이 높으므로, 예컨대 **모델이 어떤 종류의 토큰(개념, rare word, 위험단어 등)을 이전보다 더 잘 다루게 되었는지**를 살펴볼 수 있습니다. Rho-1의 경우 **모델이 어려워하던 수학 기호, 희귀 단어에 강해졌다**는 식의 분석이 있었는데, 우리도 **중요 토큰 리스트**를 뽑아 학습 전후 **Per-token loss 감소량**을 측정하면, **가장 개선된 토큰들과 덜 개선된 토큰들**을 알아낼 수 있습니다. 이는 향후 **어떤 토큰이 “중요”한지 정의**를 내리는 데도 도움을 주고, 혹시 **부작용**(특정 유형 토큰 무시 등)이 없는지도 점검할 수 있습니다.

### **4.4 기대 효과 및 위험 요소**

- **기대 효과**: 개선된 Weighted-MTP 접근을 통해, **학습 안정성**이 높아지고 **모델 성능 향상폭**이 극대화될 것으로 예상됩니다. Critic을 제거하거나 단순화함으로써 학습 **난조나 발산 위험 감소**가 기대되며, 토큰 중요도 기법을 정교화함으로써 **동일 자원으로 더 높은 성능**을 낼 가능성이 있습니다. 특히 **수식적으로 입증된 기법**들을 채택하였기에, **재현성과 이론적 신뢰성**이 올라갈 것입니다. 또한 다양한 선행 연구와의 비교실험을 통해 본 방법의 **위치**를 명확히 하면, 연구의 기여도가 설득력 있게 부각될 것입니다.
- **고려할 위험 요소**: 남은 도전으로는, **멀티토큰 예측과 토큰 가중화의 상호작용**에 대한 연구가 적었다는 점입니다. 둘을 동시에 적용했을 때 예기치 못한 학습 난이도가 있을 수 있습니다. 예를 들어, MTP 자체로도 최적화가 복잡한데 거기에 weight까지 곱하면 **gradient variance**가 커질 가능성도 있습니다. 이를 보완하려면, 초기에는 **작은 weight scaling부터 점진적으로** 적용하거나, **MTP 헤드 중 일부에만 weighting**을 주는 식의 실험도 고려해야 합니다. 또한 **hyperparameter (예: 선택할 토큰 비율, weight 강도 등)**가 추가로 늘어나므로, 이를 적절히 튜닝해야 할 것입니다. 마지막으로, **평가를 다양한 기준에서** 해야 합니다. 토큰 가중이 특정 벤치마크엔 좋지만 다른 한편 손실을 야기하지 않는지 (예: 문장 유창성 저하 등) 살펴봐야 합니다. 이러한 부분까지 면밀히 검토한다면, Weighted-MTP의 개선된 구조는 충분히 **실용적이고 혁신적인 LLM 훈련 기법**으로 자리매김할 수 있을 것입니다.

## **5. 결론**

본 보고서에서는 **Weighted-MTP 기반 연구제안서의 타당성**을 재검토하고, **지도교수님의 조언을 반영한 개선점**들을 도출하였습니다. **멀티 토큰 예측**과 **토큰 중요도 가중화**라는 두 축은 최근 LLM 연구에서 각각 **효율 향상**과 **성능 제고**의 열쇠로 부상하고 있으며, 이를 결합하려는 Weighted-MTP의 발상은 충분히 의미 있고 참신합니다. 다만 초기 제안서의 **Critic 활용 설계**는 **value 학습의 난해함**으로 인해 위험이 있었고, 이를 대신할 방안으로 **reference 모델 기반 방법**[huggingface.co](https://huggingface.co/papers/2404.07965#:~:text=Unlike%20traditional%20LMs%20that%20learn,matching%20%2038%20with), **직접적인 선호 최적화 기법(DPO/SimPO 등)**, **gradient 활용 중요도 산출** 등을 제안하였습니다. 이러한 개선을 통해 **손실 함수 수식**은 더욱 엄밀해지고, **학습 안정성**은 높아지며, **계산 자원도 효율적으로 사용**될 것으로 기대됩니다.

조사 결과, Weighted-MTP와 **유사한 연구들이 다수 진행 중**이었으나 완전히 동일한 개념은 나타나지 않았습니다. 이는 한편으로 본 연구의 **차별성**과 **혁신성**을 뒷받침하는 동시에, 다른 한편으로 **아이디어의 실현 가능성**을 뒷받침하는 **증거**가 되기도 합니다. 예컨대 Rho-1의 성공은 *“토큰 선택적 학습”*의 힘을 보여주었고, Meta의 MTP 결과는 *“멀티토큰 학습”*의 유용성을 검증했습니다. 또 TI-DPO 등의 최신 연구는 *“토큰별 중요도 반영”*이 RLHF에서 성과를 낼 수 있음을 시사했습니다. Weighted-MTP는 이러한 퍼즐 조각들을 통합하여, **보다 강력하고 효율적인 LLM 훈련 기법**을 제시하려는 것입니다.

향후 연구에서는 제안된 개선 방안들을 실제 구현하고, 다양한 벤치마크로 실험하여 얻은 **정량적/정성적 성과**를 통해 본 접근의 가치를 입증해야 할 것입니다. 성공한다면, Weighted-MTP는 **“모든 토큰을 동일하게 보지 않는다”**는 모토 아래, **LLM 훈련 패러다임의 새로운 지평**을 열 수 있을 것입니다. 이는 곧 **더 적은 데이터와 자원으로도 더 똑똑한 LLM**을 만드는 길이며, 동시에 **인간이 중요시하는 요소에 모델이 집중하도록** 유도함으로써 모델의 **합리성과 신뢰도**를 높이는 방향이기도 합니다. 이상으로, **연구제안서의 개선된 방향과 그 타당성**을 검토하였으며, 이 보고서가 추후 연구 진행에 유용한 로드맵이 되기를 기대합니다.
