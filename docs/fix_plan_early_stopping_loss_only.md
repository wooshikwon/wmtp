# Early Stopping loss_only ëª¨ë“œ ìˆ˜ì • ê³„íš

## 1. ë¬¸ì œ ì •ì˜

### 1.1 ë°œìƒí•œ ì—ëŸ¬
```
TypeError: '<=' not supported between instances of 'NoneType' and 'float'
Location: src/utils/early_stopping.py:390 in _check_variance_invalid
```

### 1.2 ì‚¬ìš©ì ì˜ë„
- **Stage 1 (Value Head Pretraining)**ì—ì„œ `mode="loss_only"` ì‚¬ìš©
- Variance/Gradient ì²´í¬ë¥¼ recipeì—ì„œ ì œê±°
- **ì´ìœ **:
  - Variance: ì´ˆê¸°í™” ì‹œ ë§¤ìš° ë‚®ì•„ ì¦‰ì‹œ ìˆ˜ë ´ìœ¼ë¡œ ì˜¤íŒ
  - Gradient: Value HeadëŠ” ì‘ì€ ë„¤íŠ¸ì›Œí¬ë¼ í­ì£¼ ê¸°ì¤€ì´ ë¶€ì ì ˆ

### 1.3 í˜„ì¬ Recipe ì„¤ì •
```yaml
# tests/configs/recipe.critic_wmtp.yaml
pretrain:
  early_stopping:
    enabled: true
    mode: "loss_only"
    patience: 10
    min_delta: 1e-4
    monitor: "value_loss"
    # variance_min/max, grad_norm_* ì„¤ì • ì—†ìŒ!
```

**Productionë„ ë™ì¼:**
```yaml
# configs/recipe.critic_wmtp.yaml
pretrain:
  early_stopping:
    mode: "loss_only"  # Most reliable: loss convergence only
```

---

## 2. ê·¼ë³¸ ì›ì¸

### 2.1 ì„¤ê³„ ê²°í•¨ ë°œê²¬

í˜„ì¬ `ValueHeadEarlyStopping.should_stop()` ë©”ì„œë“œëŠ” **modeì™€ ë¬´ê´€í•˜ê²Œ ëª¨ë“  ì²´í¬ í•¨ìˆ˜ë¥¼ í˜¸ì¶œ**í•©ë‹ˆë‹¤:

```python
# src/utils/early_stopping.py:266-269
def should_stop(self, metrics: dict[str, float]) -> bool:
    # ... ê¸°ë³¸ ì²´í¬ ...

    # ğŸ”¥ mode="loss_only"ì—¬ë„ í•­ìƒ ì‹¤í–‰!
    loss_converged = self._check_loss_convergence(value_loss)
    grad_unstable = self._check_gradient_instability(grad_norm)      # â† í˜¸ì¶œë¨
    variance_invalid = self._check_variance_invalid(value_variance)  # â† None ì—ëŸ¬!

    # ... modeë³„ ë¶„ê¸° ...
    if self.mode == "loss_only":
        if loss_converged:  # â† ì´ê²ƒë§Œ ì‚¬ìš©í•˜ëŠ”ë° ìœ„ì—ì„œ ì´ë¯¸ ë‹¤ í˜¸ì¶œí•¨!
            ...
```

### 2.2 ì™œ ì—ëŸ¬ê°€ ë°œìƒí–ˆë‚˜

`_check_variance_invalid()` ë‚´ë¶€:
```python
def _check_variance_invalid(self, variance: float | None) -> bool:
    if variance is None:
        return False

    # ğŸ”¥ variance_min/maxê°€ Noneì´ë©´ TypeError!
    return not (self.variance_min <= variance <= self.variance_max)
```

**ë¬¸ì œ íë¦„:**
1. Recipeì— `variance_min`/`variance_max` ì„¤ì • ì—†ìŒ
2. `__init__()`ì—ì„œ ê¸°ë³¸ê°’ í• ë‹¹ ì‹œë„í•˜ì§€ë§Œ ì–´ë–¤ ì´ìœ ë¡œ Noneì´ ë¨
3. `mode="loss_only"`ì¸ë°ë„ `_check_variance_invalid()` í˜¸ì¶œë¨
4. `self.variance_min`ì´ Noneì¸ ì±„ë¡œ ë¹„êµ ì—°ì‚°(`<=`) ì‹œë„ â†’ TypeError

---

## 3. í•´ê²° ì² í•™

### 3.1 ì„¤ê³„ ì›ì¹™

**"Modeë³„ë¡œ í•„ìš”í•œ ì²´í¬ë§Œ ìˆ˜í–‰í•œë‹¤"**

- `loss_only`: Loss convergenceë§Œ ì²´í¬
- `any`/`all`: Loss + Gradient + Variance ëª¨ë‘ ì²´í¬

### 3.2 2ë‹¨ê³„ ë°©ì–´ ì „ëµ

**Primary Defense (ì£¼ ë°©ì–´ì„ )**: Mode ê¸°ë°˜ ì¡°ê¸° ë¶„ê¸°
- `loss_only` ëª¨ë“œë©´ gradient/variance í•¨ìˆ˜ë¥¼ **ì•„ì˜ˆ í˜¸ì¶œí•˜ì§€ ì•ŠìŒ**
- ë¶ˆí•„ìš”í•œ ì—°ì‚° ì œê±° + None ì—ëŸ¬ ê·¼ë³¸ ì°¨ë‹¨

**Secondary Defense (ë³´ì¡° ë°©ì–´ì„ )**: None ì²´í¬ ë°©ì–´ ì½”ë“œ
- `any`/`all` ëª¨ë“œì—ì„œ ì„¤ì •ì´ ëˆ„ë½ëœ ê²½ìš°ë¥¼ ëŒ€ë¹„
- `_check_variance_invalid()`ì— variance_min/max None ì²´í¬ ì¶”ê°€
- `_check_gradient_instability()`ì™€ ë™ì¼í•œ íŒ¨í„´ ì ìš©

---

## 4. Phaseë³„ êµ¬í˜„ ê³„íš

### Phase 1: should_stop() ë©”ì„œë“œ ë¦¬íŒ©í† ë§

#### ëª©í‘œ
`mode="loss_only"`ì¼ ë•Œ lossë§Œ ì²´í¬í•˜ê³  ì¦‰ì‹œ ë°˜í™˜í•˜ë„ë¡ ìµœì í™”

#### ë³€ê²½ íŒŒì¼
`src/utils/early_stopping.py`

#### ë³€ê²½ ëŒ€ìƒ ë©”ì„œë“œ
`ValueHeadEarlyStopping.should_stop()` (line 242-316)

#### í˜„ì¬ êµ¬ì¡°
```python
def should_stop(self, metrics: dict[str, float]) -> bool:
    if not self.enabled:
        return False

    # í•„ìˆ˜ ë©”íŠ¸ë¦­ í™•ì¸
    value_loss = metrics.get(self.monitor)
    grad_norm = metrics.get("grad_norm")
    value_variance = metrics.get("value_variance")

    if value_loss is None:
        return False

    # ğŸ”¥ modeì™€ ë¬´ê´€í•˜ê²Œ ëª¨ë“  ì²´í¬ í•¨ìˆ˜ í˜¸ì¶œ
    loss_converged = self._check_loss_convergence(value_loss)
    grad_unstable = self._check_gradient_instability(grad_norm)
    variance_invalid = self._check_variance_invalid(value_variance)

    # Modeë³„ ì¤‘ë‹¨ ê²°ì •
    should_stop = False
    reasons = []

    if self.mode == "any":
        # í•˜ë‚˜ë¼ë„ ë§Œì¡±í•˜ë©´ ì¤‘ë‹¨
        if loss_converged:
            reasons.append(...)
        if grad_unstable:
            reasons.append(...)
        if variance_invalid:
            reasons.append(...)
        should_stop = len(reasons) > 0

    elif self.mode == "all":
        # ëª¨ë‘ ë§Œì¡±í•´ì•¼ ì¤‘ë‹¨
        if loss_converged and not grad_unstable and not variance_invalid:
            reasons.append(...)
            should_stop = True

    else:  # "loss_only"
        # Loss convergenceë§Œ ì²´í¬
        if loss_converged:
            reasons.append(...)
            should_stop = True

    # ì¤‘ë‹¨ ê²°ì •
    if should_stop:
        self.should_stop_flag = True
        self.stop_reason = f"Stage 1 early stop ({self.mode} mode): {'; '.join(reasons)}"
        return True

    return False
```

#### ë³€ê²½ í›„ êµ¬ì¡°
```python
def should_stop(self, metrics: dict[str, float]) -> bool:
    if not self.enabled:
        return False

    # í•„ìˆ˜ ë©”íŠ¸ë¦­ í™•ì¸
    value_loss = metrics.get(self.monitor)
    if value_loss is None:
        return False

    # ğŸ¯ loss_only ëª¨ë“œëŠ” ì—¬ê¸°ì„œ ì¡°ê¸° ì²˜ë¦¬
    if self.mode == "loss_only":
        if self._check_loss_convergence(value_loss):
            self.should_stop_flag = True
            self.stop_reason = (
                f"Stage 1 early stop (loss_only mode): "
                f"loss converged ({value_loss:.6f}, patience={self.patience})"
            )
            return True
        return False

    # any/all ëª¨ë“œë§Œ ì—¬ê¸° ë„ë‹¬ â†’ ëª¨ë“  ë©”íŠ¸ë¦­ ê°€ì ¸ì˜¤ê¸°
    grad_norm = metrics.get("grad_norm")
    value_variance = metrics.get("value_variance")

    # ëª¨ë“  ì²´í¬ í•¨ìˆ˜ í˜¸ì¶œ
    loss_converged = self._check_loss_convergence(value_loss)
    grad_unstable = self._check_gradient_instability(grad_norm)
    variance_invalid = self._check_variance_invalid(value_variance)

    # Modeë³„ ì¤‘ë‹¨ ê²°ì •
    should_stop = False
    reasons = []

    if self.mode == "any":
        # í•˜ë‚˜ë¼ë„ ë§Œì¡±í•˜ë©´ ì¤‘ë‹¨
        if loss_converged:
            reasons.append(
                f"loss converged ({value_loss:.6f}, patience={self.patience})"
            )
        if grad_unstable:
            reasons.append(
                f"gradient unstable (threshold={self.grad_norm_threshold}, "
                f"ratio={self.grad_norm_threshold_ratio})"
            )
        if variance_invalid:
            reasons.append(
                f"variance out of range ({value_variance:.4f}, "
                f"range=[{self.variance_min}, {self.variance_max}])"
            )
        should_stop = len(reasons) > 0

    elif self.mode == "all":
        # ëª¨ë‘ ë§Œì¡±í•´ì•¼ ì¤‘ë‹¨
        if loss_converged and not grad_unstable and not variance_invalid:
            reasons.append(
                f"all conditions met: loss={value_loss:.6f}, grad stable, variance valid"
            )
            should_stop = True

    # ì¤‘ë‹¨ ê²°ì •
    if should_stop:
        self.should_stop_flag = True
        self.stop_reason = (
            f"Stage 1 early stop ({self.mode} mode): {'; '.join(reasons)}"
        )
        return True

    return False
```

#### í•µì‹¬ ë³€ê²½ í¬ì¸íŠ¸

1. **Line 257-260 ì œê±°**: grad_norm, value_varianceë¥¼ ë¨¼ì € ê°€ì ¸ì˜¤ëŠ” ë¶€ë¶„ ì‚­ì œ
2. **Line 262-264 ì´í›„ì— loss_only ë¶„ê¸° ì¶”ê°€**:
   ```python
   if self.mode == "loss_only":
       # lossë§Œ ì²´í¬í•˜ê³  ì¦‰ì‹œ ë°˜í™˜
       ...
       return True/False
   ```
3. **any/all ëª¨ë“œì—ì„œë§Œ ë©”íŠ¸ë¦­ ê°€ì ¸ì˜¤ê¸°**:
   ```python
   grad_norm = metrics.get("grad_norm")
   value_variance = metrics.get("value_variance")
   ```
4. **ê¸°ì¡´ loss_only ë¸”ë¡ (line 300-306) ì œê±°**: ìœ„ì—ì„œ ì²˜ë¦¬ë˜ë¯€ë¡œ ë¶ˆí•„ìš”

#### ë³´ì¡´ ì‚¬í•­
- âœ… ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜ (`def should_stop(self, metrics: dict[str, float]) -> bool`)
- âœ… ë°˜í™˜ê°’ íƒ€ì… (`bool`)
- âœ… `any` ëª¨ë“œ ë¡œì§ (line 275-290)
- âœ… `all` ëª¨ë“œ ë¡œì§ (line 292-298)
- âœ… `should_stop_flag`, `stop_reason` ì„¤ì • ë°©ì‹
- âœ… ì—ëŸ¬ ë©”ì‹œì§€ í¬ë§·

#### ê°œì„  íš¨ê³¼
- âœ… `loss_only` ëª¨ë“œì¼ ë•Œ ë¶ˆí•„ìš”í•œ í•¨ìˆ˜ í˜¸ì¶œ ì œê±° (ì„±ëŠ¥ í–¥ìƒ)
- âœ… None ì—ëŸ¬ ê·¼ë³¸ì ìœ¼ë¡œ ì°¨ë‹¨
- âœ… ì½”ë“œ ì˜ë„ì™€ ì‹¤ì œ ë™ì‘ ì¼ì¹˜
- âœ… ê° ëª¨ë“œì˜ ì—­í• ì´ ëª…í™•í•´ì§

#### ìœ„í—˜ë„
**ë‚®ìŒ** - `any`/`all` ëª¨ë“œ ë¡œì§ì€ ê·¸ëŒ€ë¡œ ìœ ì§€, `loss_only`ë§Œ ìµœì í™”

---

### Phase 2: _check_variance_invalid() ë°©ì–´ ì½”ë“œ ì¶”ê°€

#### ëª©í‘œ
`variance_min`/`variance_max`ê°€ Noneì¼ ë•Œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬ (Secondary Defense)

#### ë³€ê²½ íŒŒì¼
`src/utils/early_stopping.py`

#### ë³€ê²½ ëŒ€ìƒ ë©”ì„œë“œ
`ValueHeadEarlyStopping._check_variance_invalid()` (line 376-390)

#### í˜„ì¬ ì½”ë“œ
```python
def _check_variance_invalid(self, variance: float | None) -> bool:
    """Variance ë²”ìœ„ ì´íƒˆ ì²´í¬.

    Args:
        variance: Value ì˜ˆì¸¡ ë¶„ì‚°

    Returns:
        ë¶„ì‚°ì´ ë²”ìœ„ë¥¼ ë²—ì–´ë‚¬ëŠ”ì§€ ì—¬ë¶€ (Trueë©´ ì¡°ê¸° ì¢…ë£Œ ì‚¬ìœ )
    """
    if variance is None:
        # Varianceê°€ ì—†ìœ¼ë©´ ì²´í¬í•˜ì§€ ì•ŠìŒ (ìœ íš¨í•¨)
        return False

    # ë²”ìœ„ë¥¼ ë²—ì–´ë‚¬ìœ¼ë©´ True (invalid)
    return not (self.variance_min <= variance <= self.variance_max)
```

#### ë³€ê²½ í›„ ì½”ë“œ
```python
def _check_variance_invalid(self, variance: float | None) -> bool:
    """Variance ë²”ìœ„ ì´íƒˆ ì²´í¬.

    Args:
        variance: Value ì˜ˆì¸¡ ë¶„ì‚°

    Returns:
        ë¶„ì‚°ì´ ë²”ìœ„ë¥¼ ë²—ì–´ë‚¬ëŠ”ì§€ ì—¬ë¶€ (Trueë©´ ì¡°ê¸° ì¢…ë£Œ ì‚¬ìœ )
    """
    # ì„¤ì •ì´ ì—†ìœ¼ë©´ ì²´í¬í•˜ì§€ ì•ŠìŒ
    if self.variance_min is None or self.variance_max is None:
        return False

    # Varianceê°€ ì—†ìœ¼ë©´ ì²´í¬í•˜ì§€ ì•ŠìŒ
    if variance is None:
        return False

    # ë²”ìœ„ë¥¼ ë²—ì–´ë‚¬ìœ¼ë©´ True (invalid)
    return not (self.variance_min <= variance <= self.variance_max)
```

#### í•µì‹¬ ë³€ê²½ í¬ì¸íŠ¸

**Line 385ì™€ 386 ì‚¬ì´ì— 3ì¤„ ì¶”ê°€:**
```python
# ì„¤ì •ì´ ì—†ìœ¼ë©´ ì²´í¬í•˜ì§€ ì•ŠìŒ
if self.variance_min is None or self.variance_max is None:
    return False
```

#### ì°¸ê³  íŒ¨í„´
`_check_gradient_instability()` (line 347-360)ì—ì„œ ì´ë¯¸ ë™ì¼í•œ íŒ¨í„´ ì‚¬ìš©:
```python
def _check_gradient_instability(self, grad_norm: float | None) -> bool:
    # ğŸ¯ ì´ë¯¸ ë°©ì–´ ì½”ë“œê°€ ìˆìŒ!
    if grad_norm is None or self.grad_norm_threshold is None:
        return False
    ...
```

#### ê°œì„  íš¨ê³¼
- âœ… `any`/`all` ëª¨ë“œì—ì„œ variance ì„¤ì • ëˆ„ë½ ì‹œì—ë„ ì•ˆì „
- âœ… `_check_gradient_instability()`ì™€ ì¼ê´€ëœ íŒ¨í„´
- âœ… Fail-safe ë©”ì»¤ë‹ˆì¦˜ ì™„ì„±

#### ìœ„í—˜ë„
**ë§¤ìš° ë‚®ìŒ** - ë°©ì–´ ì½”ë“œë§Œ ì¶”ê°€, ê¸°ì¡´ ë¡œì§ ë¬´ë³€ê²½

---

### Phase 3: í…ŒìŠ¤íŠ¸ ë° ê²€ì¦

#### 3.1 ë‹¨ìœ„ í…ŒìŠ¤íŠ¸

**í…ŒìŠ¤íŠ¸ íŒŒì¼:** `tests/test_early_stopping.py`

**í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤:**

**Scenario 1: loss_only ëª¨ë“œ (ì •ìƒ ë™ì‘)**
```python
config = {
    "enabled": True,
    "mode": "loss_only",
    "patience": 10,
    "min_delta": 1e-4,
    "monitor": "value_loss",
    # variance_min/max ì—†ìŒ!
}
early_stop = ValueHeadEarlyStopping(config)

# Variance/gradientëŠ” ì „ë‹¬í•´ë„ ë¬´ì‹œë¨
metrics = {
    "value_loss": 0.5,
    "grad_norm": 100.0,      # ë¬´ì‹œ
    "value_variance": 0.01   # ë¬´ì‹œ
}
result = early_stop.should_stop(metrics)
# ì˜ˆìƒ: False (ì•„ì§ patience ì•ˆ ì°¨ì„œ)
```

**Scenario 2: loss_only ëª¨ë“œ (ìˆ˜ë ´)**
```python
# patience=10, min_delta=1e-4
for i in range(15):
    metrics = {"value_loss": 0.5}  # ê°œì„  ì—†ìŒ
    if early_stop.should_stop(metrics):
        print(f"Stopped at step {i}")
        break
# ì˜ˆìƒ: step 10ì—ì„œ ì¤‘ë‹¨
```

**Scenario 3: any ëª¨ë“œ (ì„¤ì • ëˆ„ë½, ë°©ì–´ ì½”ë“œ í…ŒìŠ¤íŠ¸)**
```python
config = {
    "enabled": True,
    "mode": "any",
    "patience": 10,
    # variance_min/max ì—†ìŒ!
}
early_stop = ValueHeadEarlyStopping(config)

metrics = {
    "value_loss": 0.5,
    "value_variance": 0.01  # variance_min/max Noneì´ì§€ë§Œ ì—ëŸ¬ ì—†ì–´ì•¼ í•¨
}
result = early_stop.should_stop(metrics)
# ì˜ˆìƒ: ì—ëŸ¬ ì—†ì´ ì •ìƒ ë™ì‘
```

**Scenario 4: any ëª¨ë“œ (ì •ìƒ ë™ì‘)**
```python
config = {
    "enabled": True,
    "mode": "any",
    "patience": 10,
    "variance_min": 0.1,
    "variance_max": 5.0,
}
early_stop = ValueHeadEarlyStopping(config)

# Variance out of range
metrics = {
    "value_loss": 0.5,
    "value_variance": 0.05  # < 0.1
}
result = early_stop.should_stop(metrics)
# ì˜ˆìƒ: True (variance invalid)
```

**ì‹¤í–‰ ëª…ë ¹:**
```bash
PYTHONPATH=. python -m pytest tests/test_early_stopping.py::TestValueHeadEarlyStopping -v
```

#### 3.2 í†µí•© í…ŒìŠ¤íŠ¸

**ì›ë˜ ì‹¤íŒ¨í–ˆë˜ ëª…ë ¹ì–´ ì¬ì‹¤í–‰:**
```bash
PYTHONPATH=. python -m src.cli.train \
    --config tests/configs/config.local_test.yaml \
    --recipe tests/configs/recipe.critic_wmtp.yaml \
    --run-name test_critic_fixed \
    --tags test,fix,early-stopping \
    --verbose
```

**ì˜ˆìƒ ê²°ê³¼:**
```
ğŸ”¬ Starting Critic-WMTP Stage 1: Value Head Pretraining
Starting Stage 1: Value Head Pretraining
  - Hidden size: 768
  - Learning rate: 0.0001
  - Max steps: 30
  - Early stopping enabled (mode=loss_only)

Epoch 1/3
Training â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100%
  Step 100: Loss = 0.xxxx

âœ… Stage 1 Training Complete
  - Final avg loss: 0.xxxx
  - Total steps: 30
  - Value Head saved to: ./checkpoints/critic/test_critic_fixed/value_head_stage1.pt
```

#### 3.3 ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] loss_only ëª¨ë“œì—ì„œ variance/gradient í•¨ìˆ˜ê°€ í˜¸ì¶œë˜ì§€ ì•ŠëŠ”ê°€?
- [ ] loss_only ëª¨ë“œì—ì„œ patience ê¸°ë°˜ ì¡°ê¸° ì¢…ë£Œê°€ ì •ìƒ ë™ì‘í•˜ëŠ”ê°€?
- [ ] any ëª¨ë“œì—ì„œ ëª¨ë“  ì²´í¬ê°€ ì •ìƒ ë™ì‘í•˜ëŠ”ê°€?
- [ ] all ëª¨ë“œì—ì„œ ëª¨ë“  ì²´í¬ê°€ ì •ìƒ ë™ì‘í•˜ëŠ”ê°€?
- [ ] variance_min/maxê°€ Noneì¼ ë•Œ ì—ëŸ¬ê°€ ë°œìƒí•˜ì§€ ì•ŠëŠ”ê°€?
- [ ] ê¸°ì¡´ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë“¤ì´ ëª¨ë‘ í†µê³¼í•˜ëŠ”ê°€?
- [ ] Production recipeë¡œë„ ì •ìƒ ë™ì‘í•˜ëŠ”ê°€?

---

## 5. ê°œë°œ ì›ì¹™ ì¤€ìˆ˜ ì²´í¬ë¦¬ìŠ¤íŠ¸

### [ì›ì¹™ 1] ì•/ë’¤ íë¦„ ë¶„ì„
- âœ… `should_stop()` â†’ `_check_*` ë©”ì„œë“œ í˜¸ì¶œ íë¦„ ë¶„ì„ ì™„ë£Œ
- âœ… `critic_head_pretrainer.py`ì—ì„œ early_stopping ì‚¬ìš© íŒ¨í„´ í™•ì¸
- âœ… Recipe ì„¤ì • â†’ Config â†’ EarlyStopping ì´ˆê¸°í™” íë¦„ íŒŒì•…

### [ì›ì¹™ 2] ê¸°ì¡´ êµ¬ì¡° ì¡´ì¤‘ ë° ì¼ê´€ì„±
- âœ… 3ê°€ì§€ ëª¨ë“œ (`any`, `all`, `loss_only`) ëª¨ë‘ ìœ ì§€
- âœ… `_check_gradient_instability()`ì˜ None ì²´í¬ íŒ¨í„´ì„ `_check_variance_invalid()`ì— ì ìš©
- âœ… ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜, ë°˜í™˜ê°’, ì—ëŸ¬ ë©”ì‹œì§€ í¬ë§· ë³´ì¡´
- âœ… ê¸°ì¡´ any/all ëª¨ë“œ ë¡œì§ ë¬´ë³€ê²½

### [ì›ì¹™ 3] ì‚­ì œ/ì¬ì‘ì„± íŒë‹¨
- âœ… ì™„ì „í•œ ì¬ì‘ì„± ë¶ˆí•„ìš”, ë¡œì§ ìµœì í™”ë§Œ ìˆ˜í–‰
- âœ… loss_only ë¸”ë¡ ì¤‘ë³µ ì œê±° (ì¡°ê¸° ë¶„ê¸°ë¡œ ì´ë™)
- âœ… í•µì‹¬ ë¡œì§ì€ ìœ ì§€, ì‹¤í–‰ ìˆœì„œë§Œ ì¡°ì •

### [ì›ì¹™ 4] ì½”ë“œ í’ˆì§ˆ
#### [ì›ì¹™ 4-1] í˜¸í™˜ì„± ë° ë„¤ì´ë°
- âœ… `should_stop(metrics)` ì‹œê·¸ë‹ˆì²˜ ìœ ì§€ (í˜¸ì¶œë¶€ ë³€ê²½ ë¶ˆí•„ìš”)
- âœ… `stop_reason`, `should_stop_flag` ë³€ìˆ˜ëª… ìœ ì§€
- âœ… ë©”íŠ¸ë¦­ í‚¤ (`value_loss`, `grad_norm`, `value_variance`) ì¼ê´€ì„± ìœ ì§€

#### [ì›ì¹™ 4-2] ë©”ì„œë“œ ê³„ì¸µ
- âœ… ìƒˆë¡œìš´ wrapper ë©”ì„œë“œ ì¶”ê°€ ì—†ìŒ
- âœ… ê¸°ì¡´ `_check_*` ë©”ì„œë“œ í™œìš©
- âœ… ê³¼ë„í•œ ê³„ì¸µí™” ì—†ìŒ

#### [ì›ì¹™ 4-3] ì£¼ì„ ì‘ì„±
- âœ… "Phase", "Version", "v2.0" ê°™ì€ ì„ì‹œ ì£¼ì„ ì—†ìŒ
- âœ… ì½”ë“œ ë™ì‘ì— ëŒ€í•œ í•µì‹¬ ì„¤ëª…ë§Œ í¬í•¨
- âœ… Docstring ìœ ì§€ ë° í•„ìš” ì‹œ ì—…ë°ì´íŠ¸

### [ì›ì¹™ 5] ê²€í†  ë° ë³´ê³ 
- âœ… Phaseë³„ êµ¬í˜„ ì™„ë£Œ í›„ ì‚¬ìš©ì ìŠ¹ì¸ ëŒ€ê¸°
- âœ… ê³„íšì„œì™€ ë¹„êµí•˜ì—¬ ê°ê´€ì  ë³´ê³ 
- âœ… ì„±ê³¼ ê³¼ì¥ ì—†ì´ ì‹¤ì œ ë³€ê²½ ì‚¬í•­ë§Œ ê¸°ìˆ 

### [ì›ì¹™ 6] ì˜ì¡´ì„± ê´€ë¦¬
- âœ… ì½”ë“œ ìˆ˜ì •ë§Œìœ¼ë¡œ í•´ê²° (ì˜ì¡´ì„± ë³€ê²½ ë¶ˆí•„ìš”)
- âœ… Python í‘œì¤€ ê¸°ëŠ¥ë§Œ ì‚¬ìš©
- âœ… ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€ ì—†ìŒ

---

## 6. ì˜ˆìƒ ê²°ê³¼

### ìˆ˜ì • ì „
```
ğŸ”¬ Starting Critic-WMTP Stage 1: Value Head Pretraining
...
Training â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”   0% -:--:--

âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: '<=' not supported between instances of 'NoneType' and 'float'
```

### ìˆ˜ì • í›„ (loss_only ëª¨ë“œ)
```
ğŸ”¬ Starting Critic-WMTP Stage 1: Value Head Pretraining
Starting Stage 1: Value Head Pretraining
  - Hidden size: 768
  - Learning rate: 0.0001
  - Max steps: 30
  - Early stopping enabled (mode=loss_only)

Epoch 1/3
Training â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100%
  Step 10: Loss = 0.5123
  ...

âš  Early stopping: Stage 1 early stop (loss_only mode): loss converged (0.5123, patience=10)

âœ… Stage 1 Training Complete
  - Final avg loss: 0.5123
  - Total steps: 25
  - Value Head saved to: ./checkpoints/critic/.../value_head_stage1.pt
  - Early stopped: Stage 1 early stop (loss_only mode): loss converged (0.5123, patience=10)
```

### ìˆ˜ì • í›„ (any ëª¨ë“œ, ì„¤ì • ëˆ„ë½)
```
# variance_min/max ì—†ì–´ë„ ì—ëŸ¬ ì—†ìŒ
âœ… Stage 1 Training Complete
  - Final avg loss: 0.xxxx
  - Total steps: 30
```

---

## 7. ì‹¤í–‰ ê³„íš

### Step 1: Phase 1 êµ¬í˜„
1. `should_stop()` ë©”ì„œë“œ ë¦¬íŒ©í† ë§
2. ë¡œì»¬ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
3. **ì‚¬ìš©ì ìŠ¹ì¸ ëŒ€ê¸°** â†’ ê²°ê³¼ ë³´ê³ 

### Step 2: Phase 2 êµ¬í˜„
1. `_check_variance_invalid()` ë°©ì–´ ì½”ë“œ ì¶”ê°€
2. ë¡œì»¬ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì¬ì‹¤í–‰
3. **ì‚¬ìš©ì ìŠ¹ì¸ ëŒ€ê¸°** â†’ ê²°ê³¼ ë³´ê³ 

### Step 3: Phase 3 ê²€ì¦
1. ì „ì²´ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
2. í†µí•© í…ŒìŠ¤íŠ¸ (ì›ë˜ ì‹¤íŒ¨ ëª…ë ¹ì–´)
3. Production recipe í…ŒìŠ¤íŠ¸
4. **ìµœì¢… ê²°ê³¼ ë³´ê³ **

---

## 8. ìš”ì•½

### í•µì‹¬ ë³€ê²½
- `should_stop()`: loss_only ëª¨ë“œ ì¡°ê¸° ë¶„ê¸° ì¶”ê°€
- `_check_variance_invalid()`: variance_min/max None ì²´í¬ ì¶”ê°€

### í•´ê²°ë˜ëŠ” ë¬¸ì œ
- âœ… loss_only ëª¨ë“œì—ì„œ TypeError í•´ê²°
- âœ… ë¶ˆí•„ìš”í•œ ì²´í¬ í•¨ìˆ˜ í˜¸ì¶œ ì œê±°
- âœ… ì½”ë“œ ì˜ë„ì™€ ì‹¤ì œ ë™ì‘ ì¼ì¹˜
- âœ… Recipeì—ì„œ ì„¤ì • ëˆ„ë½í•´ë„ ì•ˆì „

### ì‚¬ìš©ì íŒë‹¨ ì§€ì§€
- âœ… loss_onlyê°€ Stage 1ì— ê°€ì¥ ì í•©
- âœ… Variance/Gradient ì²´í¬ê°€ ì˜¤íˆë ¤ ë°©í•´ë¨
- âœ… Loss convergenceê°€ ê°€ì¥ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì§€í‘œ

### ê°œë°œ ì›ì¹™ 100% ì¤€ìˆ˜
- âœ… ê¸°ì¡´ êµ¬ì¡° ì¡´ì¤‘ ë° ë¶„ì„
- âœ… ì¼ê´€ëœ íŒ¨í„´ ì ìš©
- âœ… ìµœì†Œ ìˆ˜ì • ì›ì¹™
- âœ… Phaseë³„ ìŠ¹ì¸ ê¸°ë°˜ ì§„í–‰
