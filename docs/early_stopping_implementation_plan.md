# Early Stopping Implementation Plan for WMTP Training (v2.0)

## ë³€ê²½ ì´ë ¥

**v2.0 (2025-10-01)**: ì „ë©´ ì¬ì„¤ê³„
- PretrainConfigë¥¼ ìµœìƒìœ„ ë ˆë²¨ë¡œ ë¶„ë¦¬ (train.stage1, critic.pretrainer í†µí•©)
- Early stopping ëª¨ë“œë¥¼ "any" ë°©ì‹ìœ¼ë¡œ ë³€ê²½ (ë” ì‹¤ìš©ì )
- Gradient normì„ ìœˆë„ìš° ê¸°ë°˜ ë¹„ìœ¨ ì²´í¬ë¡œ ê°œì„ 
- API ì‚¬ìš© íŒ¨í„´ ëª…í™•í™” (should_stop() ë°˜í™˜ê°’ ì²˜ë¦¬)
- ë¶„ì‚° í•™ìŠµ ë° ì²´í¬í¬ì¸íŠ¸ í†µí•© ê°•í™”

**v1.0**: ì´ˆê¸° ê³„íš (Stage1Config ê¸°ë°˜)

---

## 1. ê°œìš”

### ëª©í‘œ

WMTP í•™ìŠµì˜ **Pretraining (Stage 1)**ê³¼ **Main Training (Stage 2)**ì— í•™ìˆ ì ìœ¼ë¡œ íƒ€ë‹¹í•œ Early Stoppingì„ êµ¬í˜„í•˜ì—¬:

1. **ê³¼ì í•© ë°©ì§€**: Loss ìˆ˜ë ´ ê°ì§€ë¡œ ë¶ˆí•„ìš”í•œ í•™ìŠµ ì¤‘ë‹¨
2. **í•™ìŠµ ì‹œê°„ ìµœì í™”**: íš¨ìœ¨ì ì¸ ìì› í™œìš©
3. **ì—°êµ¬ ë¦¬ìŠ¤í¬ ì™„í™”**: Critic í‘œë¥˜Â·ê³ ë¶„ì‚°, Gradient ë¶ˆì•ˆì •ì„± ì¡°ê¸° ê°ì§€

### í•µì‹¬ ë³€ê²½ì‚¬í•­

#### 1. í†µí•© Pretraining ì„¤ì •
```yaml
# ê¸°ì¡´ (í˜¼ë€ìŠ¤ëŸ¬ìš´ êµ¬ì¡°)
train:
  stage1: ...  # ë˜ëŠ”
critic:
  pretrainer: ...

# ê°œì„  (ëª…í™•í•œ êµ¬ì¡°)
pretrain:  # ìµœìƒìœ„ ë ˆë²¨, trainê³¼ ë™ì¼ ì¸µìœ„
  enabled: true
  num_epochs: 3
  max_steps: 30
  early_stopping:
    mode: "any"  # í•˜ë‚˜ë¼ë„ ì¡°ê±´ ë§Œì¡± ì‹œ ì¤‘ë‹¨
    ...
```

#### 2. ì‹¤ìš©ì ì¸ ANY ëª¨ë“œ
- **ê¸°ì¡´**: loss_converged **AND** grad_stable **AND** variance_valid (ë§¤ìš° ë³´ìˆ˜ì )
- **ê°œì„ **: loss_converged **OR** grad_unstable **OR** variance_invalid (ì‹¤ìš©ì )
- ê° ì¡°ê±´ì´ ë…ë¦½ì ìœ¼ë¡œ ì¤‘ë‹¨ íŠ¸ë¦¬ê±° ê°€ëŠ¥

#### 3. ìœˆë„ìš° ê¸°ë°˜ Gradient ì•ˆì •ì„±
- **ê¸°ì¡´**: ì—°ì† patience íšŸìˆ˜ ì´ˆê³¼ (ì¼ì‹œì  ìŠ¤íŒŒì´í¬ì— ì·¨ì•½)
- **ê°œì„ **: ìœˆë„ìš° ë‚´ ì´ˆê³¼ ë¹„ìœ¨ (ì˜ˆ: ìµœê·¼ 10íšŒ ì¤‘ 70% ì´ìƒ)
- ì¼ì‹œì  ë³€ë™ì— ê°•ì¸í•œ íŒë‹¨

#### 4. ëª…í™•í•œ API íŒ¨í„´
```python
# ì˜¬ë°”ë¥¸ ì‚¬ìš©ë²•
if early_stopping.should_stop(metrics):
    reason = early_stopping.stop_reason  # ì†ì„±ìœ¼ë¡œ ì ‘ê·¼
    console.print(f"[yellow]Early stopping: {reason}[/yellow]")
    break
```

### ì—°êµ¬ ì œì•ˆì„œì™€ì˜ ì •í•©ì„±

ë³¸ ê³„íšì€ WMTP_í•™ìˆ _ì—°êµ¬ì œì•ˆì„œ.mdì˜ í•µì‹¬ ëª©í‘œë¥¼ ì§ì ‘ ì§€ì›í•©ë‹ˆë‹¤:

- **Line 103**: "Critic í‘œí˜„ ì•ˆì •í™”: íˆë“  ìƒíƒœ ë¶„í¬ ë³€í™” ì–µì œ"
  â†’ Variance range ì²´í¬ë¡œ í‘œí˜„ í‘œë¥˜ ì¡°ê¸° ê°ì§€

- **Line 104**: "ê·¸ë˜ë””ì–¸íŠ¸ ì¤‘ìš”ë„ ì•ˆì •í™”: EMA ëˆ„ì , outlier í´ë¦¬í•‘"
  â†’ ìœˆë„ìš° ê¸°ë°˜ gradient norm ëª¨ë‹ˆí„°ë§ìœ¼ë¡œ ë¶ˆì•ˆì •ì„± ê°ì§€

- **Line 112**: "Critic í‘œë¥˜Â·ê³ ë¶„ì‚°: ê°€ì¹˜ í¸í–¥Â·ë¶„ì‚° ì¦ê°€"
  â†’ ë‹¤ì¤‘ ê¸°ì¤€ early stoppingìœ¼ë¡œ ë¦¬ìŠ¤í¬ ì™„í™”

- **Line 16**: "ì¤‘ìš” í† í°ì— ê³„ì‚°ì„ ì§‘ì¤‘í•˜ëŠ” WMTP"
  â†’ ë¶ˆí•„ìš”í•œ í•™ìŠµ ì¡°ê¸° ì¤‘ë‹¨ìœ¼ë¡œ ìì› íš¨ìœ¨í™”

---

## 2. í˜„ì¬ êµ¬ì¡° ë¶„ì„

### Pretraining (Stage 1) - critic_head_pretrainer.py

**í˜„ì¬ ì¢…ë£Œ ì¡°ê±´**:
```python
for epoch in range(self.num_epochs):  # Line 199
    for step, batch in enumerate(train_loader):
        if step >= self.max_steps:  # Line 203
            break
```

**ë¬¸ì œì **:
- âŒ Value loss ìˆ˜ë ´ ë¬´ì‹œ
- âŒ Gradient norm í­ì£¼ ê°ì§€ ì—†ìŒ (Line 288ì—ì„œ ê²½ê³ ë§Œ)
- âŒ ì˜ˆì¸¡ variance ë²”ìœ„ ì²´í¬ ì—†ìŒ

**í™œìš© ê°€ëŠ¥í•œ ì •ë³´**:
- `loss.item()`: MSE loss (Line 274)
- `total_norm`: Gradient L2 norm (Line 281-286)
- `pred_values`: Value predictions (Line 271) â†’ variance ê³„ì‚° ê°€ëŠ¥

### Main Training (Stage 2) - base_wmtp_trainer.py

**í˜„ì¬ ì¢…ë£Œ ì¡°ê±´**:
```python
for step, batch in enumerate(dataloader):  # Line 405
    if max_steps is not None and current_step >= max_steps:  # Line 440
        break
```

**ë¬¸ì œì **:
- âŒ Loss ì •ì²´ ê°ì§€ ì—†ìŒ
- âœ… ëª¨ë“  ì•Œê³ ë¦¬ì¦˜ (baseline, critic, rho1) ê³µí†µ êµ¬ì¡°

**í™œìš© ê°€ëŠ¥í•œ ì •ë³´**:
- `train_step()` â†’ `dict[str, Any]` (Line 413-414)
- ê° ì•Œê³ ë¦¬ì¦˜ë³„ metrics ë°˜í™˜ (loss, wmtp_loss ë“±)

---

## 3. Early Stopping ì „ëµ

### Pretraining (Stage 1) ê¸°ì¤€

#### 1. Loss Convergence (í•„ìˆ˜)
- **Metric**: `loss` or `value_loss`
- **ê¸°ì¤€**: ìµœê·¼ N steps ë™ì•ˆ loss ê°œì„ ì´ `min_delta` ë¯¸ë§Œ
- **ì„¤ì •**: `patience=10`, `min_delta=1e-4`
- **ì´ìœ **: Value headê°€ ë” ì´ìƒ ë³´ìƒ ì˜ˆì¸¡ ëŠ¥ë ¥ì„ ê°œì„ í•˜ì§€ ëª»í•¨

#### 2. Gradient Instability (ì¤‘ìš”)
- **Metric**: `grad_norm` (L2 norm)
- **ê¸°ì¤€**: ìœˆë„ìš° ë‚´ ì´ˆê³¼ ë¹„ìœ¨ì´ threshold_ratio ì´ìƒ
- **ì„¤ì •**:
  - `grad_norm_threshold=50.0` (ê¸°ì¡´ ê²½ê³  ìˆ˜ì¤€)
  - `grad_norm_window_size=10` (ìµœê·¼ 10 ìŠ¤í…)
  - `grad_norm_threshold_ratio=0.7` (70% ì´ìƒ ì´ˆê³¼)
- **ì´ìœ **: ì—°êµ¬ì œì•ˆì„œ Line 104 "ê·¸ë˜ë””ì–¸íŠ¸ ì¤‘ìš”ë„ ì•ˆì •í™”" ë‹¬ì„±
- **ê°œì„ ì **: ì¼ì‹œì  ìŠ¤íŒŒì´í¬ì— ê°•ì¸, ì „ì²´ íŒ¨í„´ íŒŒì•…

#### 3. Variance Out of Range (ì•ˆì •ì„±)
- **Metric**: `value_variance` (pred_values.var())
- **ê¸°ì¤€**: ë¶„ì‚°ì´ `[variance_min, variance_max]` ë²”ìœ„ ì´íƒˆ
- **ì„¤ì •**: `variance_min=0.1`, `variance_max=5.0`
- **ì´ìœ **:
  - ë„ˆë¬´ ì‘ìœ¼ë©´ uninformative (ëª¨ë“  í† í° ë™ì¼ ì¤‘ìš”ë„)
  - ë„ˆë¬´ í¬ë©´ unstable (í‘œí˜„ í‘œë¥˜)

#### 4. Max Steps (ì•ˆì „ì¥ì¹˜)
- **ê¸°ì¤€**: `max_steps` ë„ë‹¬ OR `num_epochs` ì™„ë£Œ
- **ì´ìœ **: ë¬´í•œ í•™ìŠµ ë°©ì§€

#### ì¤‘ë‹¨ ëª¨ë“œ (mode)
- `"any"` (ê¶Œì¥): ìœ„ ì¡°ê±´ ì¤‘ **í•˜ë‚˜ë¼ë„** ë§Œì¡±í•˜ë©´ ì¤‘ë‹¨ (ì‹¤ìš©ì )
- `"all"`: **ëª¨ë‘** ë§Œì¡±í•´ì•¼ ì¤‘ë‹¨ (ë§¤ìš° ë³´ìˆ˜ì )
- `"loss_only"`: Loss convergenceë§Œ ì²´í¬

### Main Training (Stage 2) ê¸°ì¤€

#### 1. Loss Convergence (í•„ìˆ˜)
- **Metric**: `loss` or `wmtp_loss` (algoë³„ ìë™ ê°ì§€)
- **ê¸°ì¤€**: ìµœê·¼ N steps ë™ì•ˆ loss ê°œì„ ì´ `min_delta` ë¯¸ë§Œ
- **ì„¤ì •**: `patience=100`, `min_delta=1e-5`
- **ì´ìœ **: Main modelì´ ë” ì´ìƒ í•™ìŠµí•˜ì§€ ëª»í•¨

#### 2. Max Steps (ì•ˆì „ì¥ì¹˜)
- **ê¸°ì¤€**: `max_steps` ë„ë‹¬
- **ì´ìœ **: ë¬´í•œ í•™ìŠµ ë°©ì§€

---

## 4. êµ¬í˜„ ê³„íš

### Phase 1: Core Early Stopping Utility ê°œì„ 

**íŒŒì¼**: `src/utils/early_stopping.py`

**ë³€ê²½ì‚¬í•­**:

1. **BaseEarlyStopping**: ìœ ì§€ (ê¸°ì¡´ êµ¬ì¡° ìš°ìˆ˜)

2. **LossEarlyStopping**: ìœ ì§€ (Stage 2ìš©, ë³€ê²½ ë¶ˆí•„ìš”)

3. **ValueHeadEarlyStopping**: ì „ë©´ ê°œì„ 

**ì£¼ìš” ê°œì„  ë‚´ìš©**:
```python
class ValueHeadEarlyStopping(BaseEarlyStopping):
    def __init__(self, config: dict[str, Any] | None = None):
        super().__init__(config)

        # Mode ì¶”ê°€
        self.mode = self.config.get("mode", "any")  # "any" | "all" | "loss_only"

        # ìœˆë„ìš° ê¸°ë°˜ gradient ì²´í¬
        self.grad_norm_window_size = self.config.get("grad_norm_window_size", 10)
        self.grad_norm_threshold_ratio = self.config.get("grad_norm_threshold_ratio", 0.7)

        from collections import deque
        self.grad_norm_history: deque = deque(maxlen=self.grad_norm_window_size)

    def should_stop(self, metrics: dict[str, float]) -> bool:
        # ê° ì¡°ê±´ ë…ë¦½ì ìœ¼ë¡œ ì²´í¬
        loss_converged = self._check_loss_convergence(...)
        grad_unstable = self._check_gradient_instability(...)
        variance_invalid = self._check_variance_invalid(...)

        # ëª¨ë“œì— ë”°ë¥¸ ì¤‘ë‹¨ ê²°ì •
        if self.mode == "any":
            should_stop = loss_converged or grad_unstable or variance_invalid
        elif self.mode == "all":
            should_stop = loss_converged and not grad_unstable and not variance_invalid
        else:  # "loss_only"
            should_stop = loss_converged

        return should_stop

    def _check_gradient_instability(self, grad_norm: float | None) -> bool:
        """ìœˆë„ìš° ê¸°ë°˜ gradient ë¶ˆì•ˆì •ì„± ì²´í¬."""
        if grad_norm is None or self.grad_norm_threshold is None:
            return False

        self.grad_norm_history.append(grad_norm > self.grad_norm_threshold)

        if len(self.grad_norm_history) < self.grad_norm_window_size:
            return False

        unstable_ratio = sum(self.grad_norm_history) / len(self.grad_norm_history)
        return unstable_ratio >= self.grad_norm_threshold_ratio
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] `mode` íŒŒë¼ë¯¸í„° ì¶”ê°€
- [ ] ìœˆë„ìš° ê¸°ë°˜ gradient ì²´í¬ êµ¬í˜„
- [ ] ANY/ALL/LOSS_ONLY ë¡œì§ êµ¬í˜„
- [ ] State ê´€ë¦¬ì— `grad_norm_history` ì¶”ê°€
- [ ] ì¤‘ë‹¨ ì´ìœ  ë©”ì‹œì§€ ê°œì„ 

---

### Phase 2: Schema ì „ë©´ ì¬ì„¤ê³„

**íŒŒì¼**: `src/settings/recipe_schema.py`

#### 2.1 EarlyStoppingConfig ì¶”ê°€

```python
class EarlyStoppingConfig(BaseModel):
    """Early stopping configuration."""

    enabled: bool = Field(default=False)

    # Common settings
    patience: int = Field(default=100, ge=1)
    min_delta: float = Field(default=1e-4, gt=0)
    monitor: str = Field(default="loss")

    # Pretraining specific
    mode: Literal["any", "all", "loss_only"] = Field(default="any")

    # Gradient stability (window-based)
    grad_norm_threshold: float | None = Field(default=None)
    grad_norm_window_size: int = Field(default=10, ge=1)
    grad_norm_threshold_ratio: float = Field(default=0.7, gt=0, le=1)

    # Variance range
    variance_min: float | None = Field(default=None)
    variance_max: float | None = Field(default=None)
```

#### 2.2 PretrainConfig ì¶”ê°€ (ìµœìƒìœ„ ë ˆë²¨)

```python
class PretrainConfig(BaseModel):
    """Pretraining configuration (Stage 1)."""

    enabled: bool = Field(default=True)

    # Training parameters
    num_epochs: int = Field(default=3, ge=1)
    max_steps: int = Field(default=2000, ge=1)
    lr: float = Field(default=1e-4, gt=0)

    # Output
    save_value_head: bool = Field(default=True)

    # Early stopping
    early_stopping: EarlyStoppingConfig | None = Field(default=None)

    # Note: GAE parameters (gamma, gae_lambda) are in critic section
    # and will be used by pretrainer via component_factory
```

#### 2.3 Recipe ìˆ˜ì •

```python
class Recipe(BaseModel):
    run: Run
    pretrain: PretrainConfig | None = Field(default=None)  # ì¶”ê°€
    train: Train  # stage1 í•„ë“œ ì œê±°ë¨
    optim: Optim
    data: Data
    loss: Loss
    critic: Critic | None
    rho1: Rho1 | None
    eval: Eval
```

#### 2.4 Train ìˆ˜ì •

```python
class Train(BaseModel):
    algo: Literal["baseline-mtp", "critic-wmtp", "rho1-wmtp"]
    full_finetune: bool = True
    max_steps: int | None = None
    eval_interval: int = 500
    save_interval: int = 1000
    # stage1 ì œê±°ë¨
    early_stopping: EarlyStoppingConfig | None = Field(default=None)  # ì¶”ê°€
```

#### 2.5 Stage1Config ì œê±°

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] `EarlyStoppingConfig` í´ë˜ìŠ¤ ì •ì˜
- [ ] `PretrainConfig` í´ë˜ìŠ¤ ì •ì˜
- [ ] `Recipe`ì— `pretrain` í•„ë“œ ì¶”ê°€
- [ ] `Train`ì—ì„œ `stage1` ì œê±°
- [ ] `Train`ì— `early_stopping` ì¶”ê°€
- [ ] `Stage1Config` í´ë˜ìŠ¤ ì œê±°
- [ ] ìŠ¤í‚¤ë§ˆ í…ŒìŠ¤íŠ¸

---

### Phase 3: Pretrainer Integration

**íŒŒì¼**: `src/components/trainer/critic_head_pretrainer.py`

#### 3.1 __init__ ìˆ˜ì •

```python
def __init__(self, config: dict[str, Any] | None = None):
    super().__init__(config)

    # ê¸°ì¡´ íŒŒë¼ë¯¸í„°
    self.lr = self.config.get("lr", 1e-4)
    self.num_epochs = self.config.get("num_epochs", 3)
    self.max_steps = self.config.get("max_steps", 1000)
    self.gamma = self.config.get("gamma", 0.99)
    self.gae_lambda = self.config.get("gae_lambda", 0.95)

    # Early stopping ì¶”ê°€
    self.early_stopping_config = self.config.get("early_stopping")
```

#### 3.2 run() ìˆ˜ì •

```python
def run(self, ctx: dict[str, Any]) -> dict[str, Any]:
    # ... ê¸°ì¡´ ì´ˆê¸°í™” ...

    # Early stopping ì´ˆê¸°í™”
    early_stopping = None
    if self.early_stopping_config and self.early_stopping_config.get("enabled", False):
        from src.utils.early_stopping import ValueHeadEarlyStopping

        early_stopping = ValueHeadEarlyStopping(self.early_stopping_config)
        mode = self.early_stopping_config.get("mode", "any")
        console.print(f"[cyan]Early stopping enabled (mode={mode})[/cyan]")

    # Training loop
    for epoch in range(self.num_epochs):
        for step, batch in enumerate(train_loader):
            if step >= self.max_steps:
                break

            # ... ê¸°ì¡´ í•™ìŠµ ë¡œì§ ...

            # Forward & Backward
            pred_values = self.value_head(hs_flat)
            loss = loss_fn(pred_values, vt_flat)
            optimizer.zero_grad(set_to_none=True)
            loss.backward()

            # Gradient norm ê³„ì‚° (ê¸°ì¡´)
            total_norm = ...

            # Variance ê³„ì‚° ì¶”ê°€
            pred_variance = pred_values.var().item()

            optimizer.step()

            # Early stopping ì²´í¬
            if early_stopping:
                metrics = {
                    "value_loss": loss.item(),
                    "grad_norm": total_norm,
                    "value_variance": pred_variance,
                }

                if early_stopping.should_stop(metrics):
                    reason = early_stopping.stop_reason
                    console.print(f"[yellow]âš  Early stopping: {reason}[/yellow]")
                    break

        # ì™¸ë¶€ loop ì¢…ë£Œ
        if early_stopping and early_stopping.should_stop_flag:
            break

    # ... ì €ì¥ ë¡œì§ ...

    return {
        "saved": save_location,
        "final_loss": avg_final_loss,
        "total_steps": step_count,
        "early_stopped": early_stopping.should_stop_flag if early_stopping else False,
        "stop_reason": early_stopping.stop_reason if early_stopping else None,
    }
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] `__init__`ì— early_stopping_config ì¶”ê°€
- [ ] `run()`ì— early_stopping ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
- [ ] Variance ê³„ì‚° ì¶”ê°€
- [ ] Early stopping ì²´í¬ í†µí•©
- [ ] Nested loop ì¢…ë£Œ ì²˜ë¦¬
- [ ] ë°˜í™˜ê°’ì— early_stopped ì •ë³´ ì¶”ê°€

---

### Phase 4: Main Trainer Integration

**íŒŒì¼**: `src/components/trainer/base_wmtp_trainer.py`

#### 4.1 __init__ ìˆ˜ì •

```python
def __init__(self, config: dict[str, Any] | None = None):
    super().__init__(config)
    # ... ê¸°ì¡´ ì´ˆê¸°í™” ...
    self.early_stopping = None  # setup()ì—ì„œ ì´ˆê¸°í™”
```

#### 4.2 setup() ìˆ˜ì •

```python
def setup(self, ctx: dict[str, Any]) -> None:
    super().setup(ctx)
    # ... ê¸°ì¡´ setup ...

    # Early stopping ì´ˆê¸°í™”
    recipe = ctx.get("recipe")
    if recipe and hasattr(recipe.train, "early_stopping"):
        es_config = recipe.train.early_stopping
        if es_config and es_config.enabled:
            from src.utils.early_stopping import LossEarlyStopping

            es_config_dict = (
                es_config.model_dump()
                if hasattr(es_config, "model_dump")
                else es_config
            )

            self.early_stopping = LossEarlyStopping(es_config_dict)
            console.print(f"[cyan]Early stopping enabled (monitor={es_config.monitor})[/cyan]")

    # ì²´í¬í¬ì¸íŠ¸ ë³µì›
    checkpoint_data = ctx.get("checkpoint_data")
    if checkpoint_data and self.early_stopping:
        es_state = checkpoint_data.get("early_stopping_state")
        if es_state:
            self.early_stopping.load_state(es_state)
            console.print("[cyan]Early stopping state restored[/cyan]")
```

#### 4.3 run() ìˆ˜ì •

```python
def run(self, ctx: dict[str, Any]) -> dict[str, Any]:
    # ... ê¸°ì¡´ ì´ˆê¸°í™” ...

    for step, batch in enumerate(dataloader):
        current_step = step + 1

        if current_step <= self.start_step:
            continue

        # í›ˆë ¨ ìŠ¤í…
        out = self.train_step(batch)
        metrics = out

        # Early stopping ì²´í¬
        if self.early_stopping:
            should_stop = self.early_stopping.should_stop(metrics)

            # ë¶„ì‚° í•™ìŠµ: rank 0 ê²°ì • ë¸Œë¡œë“œìºìŠ¤íŠ¸
            if torch.distributed.is_available() and torch.distributed.is_initialized():
                should_stop_tensor = torch.tensor(
                    [should_stop],
                    dtype=torch.bool,
                    device=self.device
                )
                torch.distributed.broadcast(should_stop_tensor, src=0)
                should_stop = should_stop_tensor.item()

            if should_stop:
                reason = self.early_stopping.stop_reason
                console.print(f"[yellow]âš  Early stopping: {reason}[/yellow]")

                # MLflow ë¡œê¹…
                if self.mlflow:
                    self.mlflow.log_metrics({
                        "early_stopping/final_step": current_step,
                        "early_stopping/best_value": self.early_stopping.best_value,
                        "early_stopping/counter": self.early_stopping.counter,
                    })
                break

        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        if current_step % self.save_interval == 0:
            checkpoint_path = self._save_checkpoint(epoch, current_step, metrics)
            # ...

        # Max steps ì²´í¬
        if max_steps is not None and current_step >= max_steps:
            break

    # ... ìµœì¢… ì €ì¥ ...
    return metrics
```

#### 4.4 _save_checkpoint() ìˆ˜ì •

```python
def _save_checkpoint(self, epoch: int, step: int, metrics: dict) -> str:
    # ... ê¸°ì¡´ ì½”ë“œ ...

    # Early stopping ìƒíƒœ í¬í•¨
    es_state = self.early_stopping.get_state() if self.early_stopping else None

    self.dist_manager.save_checkpoint(
        # ... ê¸°ì¡´ íŒŒë¼ë¯¸í„° ...
        early_stopping_state=es_state,
    )

    return checkpoint_path
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] `__init__`ì— early_stopping ë³€ìˆ˜
- [ ] `setup()`ì— early_stopping ìƒì„±
- [ ] ì²´í¬í¬ì¸íŠ¸ ë³µì› ì‹œ ìƒíƒœ ë¡œë“œ
- [ ] `run()`ì— early stopping ì²´í¬
- [ ] ë¶„ì‚° í•™ìŠµ ë¸Œë¡œë“œìºìŠ¤íŠ¸
- [ ] MLflow ë¡œê¹…
- [ ] ì²´í¬í¬ì¸íŠ¸ì— ìƒíƒœ ì €ì¥

---

### Phase 5: Pipeline & Factory ìˆ˜ì •

#### 5.1 training_pipeline.py

**íŒŒì¼**: `src/pipelines/training_pipeline.py`

```python
def run_training_pipeline(config: Config, recipe: Recipe, ...):
    # ... ê¸°ì¡´ ì´ˆê¸°í™” ...

    # Step 10: Pretraining (recipe.pretrain ì‚¬ìš©)
    value_head_path = None

    if (recipe.train.algo == "critic-wmtp" and
        recipe.pretrain and
        recipe.pretrain.enabled and
        rm_model is not None and
        not dry_run):

        console.print("[cyan]ğŸ”¬ Starting Pretraining (Stage 1)[/cyan]")

        pretrainer = ComponentFactory.create_pretrainer(recipe)
        pretrainer.setup({})

        stage1_result = pretrainer.run({
            "base_model": base,
            "rm_model": rm_model,
            "train_dataloader": train_dl,
            "run_name": recipe.run.name or "default",
        })

        if stage1_result.get("saved"):
            value_head_path = stage1_result["saved"]
            console.print(f"[green]âœ… Pretraining complete: {value_head_path}[/green]")

        if stage1_result.get("early_stopped"):
            console.print(f"[yellow]âš  Pretraining early stopped: {stage1_result.get('stop_reason')}[/yellow]")

    # ... ë‚˜ë¨¸ì§€ ì½”ë“œ ...
```

#### 5.2 component_factory.py

**íŒŒì¼**: `src/factory/component_factory.py`

```python
@staticmethod
def create_pretrainer(recipe: Recipe) -> Any:
    algo = recipe.train.algo

    if algo == "critic-wmtp":
        if not recipe.pretrain:
            raise ValueError("critic-wmtp requires pretrain configuration")

        pretrainer_config = {
            # Pretrain ì„¹ì…˜
            "num_epochs": recipe.pretrain.num_epochs,
            "max_steps": recipe.pretrain.max_steps,
            "lr": recipe.pretrain.lr,
            "gamma": recipe.pretrain.gamma,
            "gae_lambda": recipe.pretrain.gae_lambda,

            # Loss ì„¹ì…˜
            "temperature": recipe.loss.weight_temperature,

            # Critic ì„¹ì…˜
            "target": recipe.critic.target,
            "token_spread": recipe.critic.token_spread,
            "delta_mode": recipe.critic.delta_mode,
            "normalize": recipe.critic.normalize,
            "value_coef": recipe.critic.auxiliary_loss_coef,

            # Early stopping
            "early_stopping": (
                recipe.pretrain.early_stopping.model_dump()
                if recipe.pretrain.early_stopping
                else None
            ),
        }

        from src.components.registry import pretrainer_registry
        return pretrainer_registry.create("critic-head-pretrainer", pretrainer_config)
    else:
        raise ValueError(f"Pretraining not supported for: {algo}")
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] `training_pipeline.py`: recipe.pretrain ì‚¬ìš©
- [ ] `component_factory.py`: create_pretrainer() ìˆ˜ì •
- [ ] Early stopping ê²°ê³¼ ì²˜ë¦¬

---

### Phase 6: Testing & Documentation

#### 6.1 í…ŒìŠ¤íŠ¸ YAML ì—…ë°ì´íŠ¸

**íŒŒì¼**: `tests/configs/recipe.critic_wmtp.yaml`

```yaml
run:
  name: "m3_test_critic_wmtp"
  tags: ["test", "m3", "critic", "wmtp"]

# Pretraining (ìµœìƒìœ„ ë ˆë²¨)
pretrain:
  enabled: true
  num_epochs: 3
  max_steps: 30
  lr: 1e-4
  save_value_head: true

  early_stopping:
    enabled: true
    mode: "any"
    patience: 10
    min_delta: 1e-4
    monitor: "value_loss"
    grad_norm_threshold: 50.0
    grad_norm_window_size: 10
    grad_norm_threshold_ratio: 0.7
    variance_min: 0.1
    variance_max: 5.0

train:
  algo: "critic-wmtp"
  full_finetune: true
  max_steps: 2

  early_stopping:
    enabled: false
    patience: 100
    min_delta: 1e-5
    monitor: "wmtp_loss"

optim:
  optimizer: "adamw"
  lr: 5.0e-5
  weight_decay: 0.01
  betas: [0.9, 0.999]
  grad_clip: 1.0
  scheduler: "constant"
  warmup_ratio: 0.0

data:
  train:
    sources: ["mbpp"]
    max_length: 128
    batch_size: 1
    pack_sequences: false
  eval:
    sources: ["mbpp"]
    max_length: 128
    batch_size: 1
    pack_sequences: false

loss:
  weight_norm: "mean1.0_clip"
  lambda: 0.3
  weight_temperature: 0.7
  epsilon: 0.05
  max_weight: 3.0

critic:
  target: "rm_sequence"
  token_spread: "gae"
  delta_mode: "td"
  normalize: "zscore"
  discount_lambda: 0.95
  gamma: 0.99
  gae_lambda: 0.95
  auxiliary_loss_coef: 0.1
  value_lr: 5e-5
  use_pseudo_rewards: true

eval:
  protocol: "meta-mtp"
  sampling:
    temperature: 0.7
    top_p: 0.9
    n: 1
  metrics:
    - "mbpp_exact"
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] í…ŒìŠ¤íŠ¸ YAML ì—…ë°ì´íŠ¸
- [ ] í†µí•© í…ŒìŠ¤íŠ¸ ì‘ì„±
- [ ] ë¬¸ì„œ ìµœì¢… ê²€í† 

---

## 5. ì˜ˆìƒ íš¨ê³¼

### í•™ìˆ ì  íƒ€ë‹¹ì„±
- âœ… ì—°êµ¬ì œì•ˆì„œ Line 112 "Critic í‘œë¥˜Â·ê³ ë¶„ì‚°" ë¦¬ìŠ¤í¬ ì™„í™”
- âœ… ì—°êµ¬ì œì•ˆì„œ Line 104 "ê·¸ë˜ë””ì–¸íŠ¸ ì¤‘ìš”ë„ ì•ˆì •í™”" ë‹¬ì„±
- âœ… Loss ìˆ˜ë ´ ê¸°ë°˜ ì¡°ê¸° ì¢…ë£Œë¡œ ê³¼ì í•© ë°©ì§€

### ì‹¤ìš©ì  ì´ì 
- âœ… í•™ìŠµ ì‹œê°„ ì ˆì•½: ë¶ˆí•„ìš”í•œ epoch ì œê±°
- âœ… ìì› íš¨ìœ¨í™”: GPU ì‚¬ìš© ì‹œê°„ ê°ì†Œ
- âœ… ì•ˆì •ì„± í–¥ìƒ: Gradient í­ì£¼ ì¡°ê¸° ê°ì§€
- âœ… ìœ ì—°ì„±: ANY/ALL/LOSS_ONLY ëª¨ë“œ ì„ íƒ

### ì¬í˜„ì„±
- âœ… YAML ì„¤ì •ìœ¼ë¡œ ì¡°ê¸° ì¢…ë£Œ ê¸°ì¤€ ëª…ì‹œ
- âœ… MLflowì— early_stopping ë©”íŠ¸ë¦­ ë¡œê¹…
- âœ… ì²´í¬í¬ì¸íŠ¸ì— ìƒíƒœ ì €ì¥
- âœ… ë¶„ì‚° í•™ìŠµì—ì„œ ì¼ê´€ëœ ì¤‘ë‹¨ ê²°ì •

---

## 6. ë¦¬ìŠ¤í¬ ë° ì™„í™”

### ë¦¬ìŠ¤í¬ 1: False Positive

**ë¬¸ì œ**: patienceê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ì¼ì‹œì  ì •ì²´ì—ì„œ ì¤‘ë‹¨

**ì™„í™”**:
- ë³´ìˆ˜ì  ê¸°ë³¸ê°’ (patience=10 for Stage 1, 100 for Stage 2)
- min_deltaë¥¼ ì¶©ë¶„íˆ ì‘ê²Œ ì„¤ì • (1e-4)
- í…ŒìŠ¤íŠ¸ í™˜ê²½ì—ì„œ ê²€ì¦

### ë¦¬ìŠ¤í¬ 2: Hyperparameter Tuning

**ë¬¸ì œ**: ì—¬ëŸ¬ íŒŒë¼ë¯¸í„° íŠœë‹ í•„ìš”

**ì™„í™”**:
- ì—°êµ¬ì œì•ˆì„œ ê¸°ë°˜ í•©ë¦¬ì  ê¸°ë³¸ê°’
- enabled=falseë¡œ ê¸°ì¡´ ë™ì‘ ìœ ì§€
- PPO ì»¤ë®¤ë‹ˆí‹° ê¶Œì¥ê°’ ì°¸ì¡°

### ë¦¬ìŠ¤í¬ 3: Mode ì„ íƒ

**ë¬¸ì œ**: ANY/ALL/LOSS_ONLY ì„ íƒ í˜¼ë€

**ì™„í™”**:
- ANYë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì • (ê°€ì¥ ì‹¤ìš©ì )
- ë¬¸ì„œì— ê° ëª¨ë“œ íŠ¹ì„± ì„¤ëª…
- ì•Œê³ ë¦¬ì¦˜ë³„ ê¶Œì¥ ëª¨ë“œ ì œì‹œ

### ë¦¬ìŠ¤í¬ 4: ë¶„ì‚° í•™ìŠµ ë™ê¸°í™”

**ë¬¸ì œ**: Rank ê°„ ì¤‘ë‹¨ ê²°ì • ë¶ˆì¼ì¹˜

**ì™„í™”**:
- Rank 0 ê²°ì • ë¸Œë¡œë“œìºìŠ¤íŠ¸
- torch.distributed.barrier() ì‚¬ìš©
- ë¶„ì‚° í•™ìŠµ í…ŒìŠ¤íŠ¸

---

## 7. ì „ì²´ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Phase 1: Early Stopping Utility
- [ ] ValueHeadEarlyStoppingì— mode ì¶”ê°€
- [ ] ìœˆë„ìš° ê¸°ë°˜ gradient ì²´í¬ êµ¬í˜„
- [ ] ANY/ALL/LOSS_ONLY ë¡œì§ êµ¬í˜„
- [ ] State ê´€ë¦¬ì— grad_norm_history ì¶”ê°€
- [ ] ì¤‘ë‹¨ ì´ìœ  ë©”ì‹œì§€ ê°œì„ 

### Phase 2: Schema
- [ ] EarlyStoppingConfig í´ë˜ìŠ¤ ì •ì˜
- [ ] PretrainConfig í´ë˜ìŠ¤ ì •ì˜
- [ ] Recipeì— pretrain ì¶”ê°€
- [ ] Trainì—ì„œ stage1 ì œê±°
- [ ] Trainì— early_stopping ì¶”ê°€
- [ ] Stage1Config ì œê±°
- [ ] ìŠ¤í‚¤ë§ˆ í…ŒìŠ¤íŠ¸

### Phase 3: Pretrainer
- [ ] __init__ì— early_stopping_config
- [ ] run()ì— early_stopping ìƒì„±
- [ ] Variance ê³„ì‚° ì¶”ê°€
- [ ] Early stopping ì²´í¬ í†µí•©
- [ ] Nested loop ì¢…ë£Œ
- [ ] ë°˜í™˜ê°’ì— early_stopped ì¶”ê°€

### Phase 4: Main Trainer
- [ ] __init__ì— early_stopping ë³€ìˆ˜
- [ ] setup()ì— early_stopping ìƒì„±
- [ ] ì²´í¬í¬ì¸íŠ¸ ìƒíƒœ ë¡œë“œ
- [ ] run()ì— early stopping ì²´í¬
- [ ] ë¶„ì‚° í•™ìŠµ ë¸Œë¡œë“œìºìŠ¤íŠ¸
- [ ] MLflow ë¡œê¹…
- [ ] ì²´í¬í¬ì¸íŠ¸ ìƒíƒœ ì €ì¥

### Phase 5: Pipeline & Factory
- [ ] training_pipeline.py ìˆ˜ì •
- [ ] component_factory.py ìˆ˜ì •
- [ ] Early stopping ê²°ê³¼ ì²˜ë¦¬

### Phase 6: Testing
- [ ] í…ŒìŠ¤íŠ¸ YAML ì—…ë°ì´íŠ¸
- [ ] í†µí•© í…ŒìŠ¤íŠ¸ ì‘ì„±
- [ ] ë¬¸ì„œ ìµœì¢… ê²€í† 

---

## 8. ê°œë°œ ì›ì¹™ ì¤€ìˆ˜

**ì›ì¹™1**: âœ… ê¸°ì¡´ êµ¬ì¡° íŒŒì•… ì™„ë£Œ

**ì›ì¹™2**: âœ… ì¤‘ë³µ ì—†ëŠ” ì¼ê´€ëœ êµ¬ì¡°
- PretrainConfigë¡œ í†µí•© (train.stage1, critic.pretrainer ì œê±°)
- EarlyStopping ìœ í‹¸ë¦¬í‹° ê³µí†µ ì‚¬ìš©

**ì›ì¹™3**: âœ… ê¸°ì¡´ ì½”ë“œ ì‚­ì œ ê³„íš ìˆ˜ë¦½
- train.stage1, critic.pretrainer â†’ pretrainìœ¼ë¡œ ëŒ€ì²´
- í•˜ìœ„ í˜¸í™˜ì„± ë¬´ì‹œ (ì‚¬ìš©ì ìŠ¹ì¸)

**ì›ì¹™4**: âœ… ê¹¨ë—í•œ ì¬ì‘ì„± ì¤€ë¹„
- ë¶ˆí•„ìš”í•œ ì¤‘ë³µ ì œê±°
- ëª…í™•í•œ êµ¬ì¡°ë¡œ ì¬ì„¤ê³„

**ì›ì¹™5**: âœ… ê³„íšì„œ ì‘ì„± í›„ ìŠ¹ì¸ ìš”ì²­

**ì›ì¹™6**: âœ… ì˜ì¡´ì„± ë¬¸ì œ ì—†ìŒ
- PyTorch, collections (í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬)
- ì¶”ê°€ íŒ¨í‚¤ì§€ ë¶ˆí•„ìš”

---

## 9. êµ¬í˜„ ìˆœì„œ

1. Phase 1: Early Stopping Utility ê°œì„  (2-3ì‹œê°„)
2. Phase 2: Schema ì¬ì„¤ê³„ (1-2ì‹œê°„)
3. Phase 3: Pretrainer Integration (2-3ì‹œê°„)
4. Phase 4: Main Trainer Integration (2-3ì‹œê°„)
5. Phase 5: Pipeline & Factory ìˆ˜ì • (1-2ì‹œê°„)
6. Phase 6: Testing & Documentation (2-3ì‹œê°„)

**ì´ ì˜ˆìƒ ì‹œê°„**: 10-16ì‹œê°„

---

**ë¬¸ì„œ ë²„ì „**: 2.0
**ì‘ì„±ì¼**: 2025-10-01
**ì£¼ìš” ê°œì„ **:
- PretrainConfig ìµœìƒìœ„ ë ˆë²¨ ë¶„ë¦¬
- ANY ëª¨ë“œë¡œ ì‹¤ìš©ì„± ê°œì„ 
- ìœˆë„ìš° ê¸°ë°˜ gradient ì•ˆì •í™”
- API ëª…í™•í™” ë° ë¶„ì‚° í•™ìŠµ í†µí•©
- ì²´í¬í¬ì¸íŠ¸ ìƒíƒœ ê´€ë¦¬ ê°•í™”
