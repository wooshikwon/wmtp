# WMTP MPS (Metal Performance Shaders) ìµœì í™” ê³„íšì„œ
## Apple Silicon MacBook ì§€ì›ì„ ìœ„í•œ 4D í…ì„œ ì•„í‚¤í…ì²˜ ê°œì„ 

---

## 1. ë°°ê²½ ë° í˜„í™© ë¶„ì„

### 1.1 ë¬¸ì œ ì •ì˜
- **í˜„ìƒ**: MacBook M3ì—ì„œ WMTP ì‹¤ì œ í•™ìŠµ ì‹œ ë¬´í•œ ë¸”ë¡œí‚¹ (Dry-runì€ ì •ìƒ)
- **ì›ì¸**: MPS ë°±ì—”ë“œì˜ 4D í…ì„œ [B, S, H, V] ì²˜ë¦¬ ë¯¸ì„±ìˆ™
- **ì˜í–¥**: ê°œë°œìì˜ ë¡œì»¬ í…ŒìŠ¤íŠ¸/ë””ë²„ê¹… ë¶ˆê°€ëŠ¥

### 1.2 í˜„ì¬ ì•„í‚¤í…ì²˜ êµ¬ì¡° (í•„ìˆ˜1: êµ¬ì¡° íŒŒì•…)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   modeling.py   â”‚ â† torch.stack(dim=2)ë¡œ 4D í…ì„œ ìƒì„±
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ base_trainer.py â”‚ â† compute_weighted_mtp_loss()ì—ì„œ 4Dâ†’2D flatten
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Algorithm       â”‚
â”‚ Trainers        â”‚ â† baseline/critic/rho1ì´ ê³µí†µ í•¨ìˆ˜ ì‚¬ìš©
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.3 í•µì‹¬ ë¸”ë¡œí‚¹ ì§€ì  ì‹ë³„

| íŒŒì¼ | ìœ„ì¹˜ | ë¬¸ì œ ì—°ì‚° | ì‹¬ê°ë„ |
|------|------|-----------|--------|
| modeling.py | L94 | `torch.stack(all_logits, dim=2)` | ğŸ”´ Critical |
| base_wmtp_trainer.py | L78-87 | `view(B*S*H, V)` â†’ CE â†’ `view(B,S,H)` | ğŸ”´ Critical |
| ë‹¤ìˆ˜ trainer | ì—¬ëŸ¬ ê³³ | `unsqueeze(-1).expand(-1,-1,H)` | ğŸŸ¡ Medium |

---

## 2. ê°œë°œ ì›ì¹™ ì¤€ìˆ˜ ê²€í† 

### 2.1 ê¸°ì¡´ êµ¬ì¡° ì¡´ì¤‘ (í•„ìˆ˜2)
- âœ… `compute_weighted_mtp_loss()`ëŠ” ëª¨ë“  ì•Œê³ ë¦¬ì¦˜ì˜ í•µì‹¬ â†’ ìœ ì§€
- âœ… 4D í…ì„œ [B,S,H,V]ëŠ” WMTPì˜ ì •ì²´ì„± â†’ ìœ ì§€
- âœ… ì¤‘ë³µ ì œê±°: MPS/CUDA ë¡œì§ì„ ë‹¨ì¼ í•¨ìˆ˜ë¡œ í†µí•©

### 2.2 ì‚­ì œ/ì¬ì‘ì„± íŒë‹¨ (í•„ìˆ˜3-4)
- âŒ ì „ë©´ ì¬ì‘ì„± ë¶ˆí•„ìš”: ì¡°ê±´ë¶€ ë¶„ê¸°ë¡œ í•´ê²° ê°€ëŠ¥
- âœ… í•˜ìœ„ í˜¸í™˜ì„± ë¬´ì‹œ: MPS ìµœì í™”ëŠ” ìƒˆë¡œìš´ ê²½ë¡œë¡œ ì¶”ê°€

### 2.3 ì˜ì¡´ì„± í™œìš© (í•„ìˆ˜6)
- PyTorch â‰¥ 2.0 required (torch.compile ì§€ì›)
- MPS availability check via `torch.backends.mps.is_available()`

---

## 3. Phaseë³„ ìˆ˜ì • ê³„íš

### ğŸ¯ Phase 0: í˜„í™© í™•ì¸ ë° í…ŒìŠ¤íŠ¸ ì¸í”„ë¼ êµ¬ì¶• (2ì‹œê°„)

#### ëª©í‘œ
- MPS ë¸”ë¡œí‚¹ ì¬í˜„ ê°€ëŠ¥í•œ ìµœì†Œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì‘ì„±
- ì„±ëŠ¥ ì¸¡ì • ë² ì´ìŠ¤ë¼ì¸ í™•ë¦½

#### ì‘ì—… ë‚´ìš©
1. **MPS í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±**: `tests/test_mps_compatibility.py`
```python
# MPS ë¸”ë¡œí‚¹ ì¬í˜„ í…ŒìŠ¤íŠ¸
def test_mps_4d_tensor_operations():
    """MPSì—ì„œ 4D í…ì„œ ì—°ì‚° ë¸”ë¡œí‚¹ í…ŒìŠ¤íŠ¸"""
    device = torch.device("mps")
    # 4D tensor operations...
    assert execution_time < 5.0  # 5ì´ˆ ì´ë‚´ ì™„ë£Œ
```

2. **ë²¤ì¹˜ë§ˆí¬ ìŠ¤í¬ë¦½íŠ¸**: `benchmarks/mps_vs_cpu.py`
```python
# MPS vs CPU ì„±ëŠ¥ ë¹„êµ
# í˜„ì¬ ìƒíƒœ (ë¸”ë¡œí‚¹) vs ìˆ˜ì • í›„ ì¸¡ì •
```

#### ì„±ê³µ ê¸°ì¤€
- [ ] MPS ë¸”ë¡œí‚¹ 100% ì¬í˜„
- [ ] CPU ëŒ€ë¹„ ì„±ëŠ¥ ì¸¡ì • ì™„ë£Œ

---

### ğŸ”§ Phase 1: ìµœì†Œ ì¹¨ìŠµ ìˆ˜ì • - Contiguous ë©”ëª¨ë¦¬ ë³´ì¥ (2ì‹œê°„)

#### ëª©í‘œ
- ìµœì†Œí•œì˜ ì½”ë“œ ë³€ê²½ìœ¼ë¡œ MPS ì‘ë™ í™•ì¸
- ê¸°ì¡´ CUDA ì„±ëŠ¥ì— ì˜í–¥ ìµœì†Œí™”

#### ìˆ˜ì • íŒŒì¼ ë° ë‚´ìš©

**1. modeling.py ìˆ˜ì •**
```python
# tests/tiny_models/distilgpt2-mtp/modeling.py L94
# ë³€ê²½ ì „:
mtp_logits = torch.stack(all_logits, dim=2)

# ë³€ê²½ í›„:
mtp_logits = torch.stack(all_logits, dim=2).contiguous()
```

**2. base_wmtp_trainer.py ìˆ˜ì •**
```python
# src/components/trainer/base_wmtp_trainer.py L78
# ë³€ê²½ ì „:
logits_flat = logits.view(B * S * H, V)

# ë³€ê²½ í›„:
logits_flat = logits.contiguous().view(B * S * H, V)
```

#### ì„±ê³µ ê¸°ì¤€
- [ ] MPSì—ì„œ 1 step í•™ìŠµ ì™„ë£Œ
- [ ] CUDA ì„±ëŠ¥ ì €í•˜ < 1%

---

### ğŸš€ Phase 2: í•µì‹¬ ë¸”ë¡œí‚¹ í•´ì†Œ - ì¡°ê±´ë¶€ ìµœì í™” ê²½ë¡œ (4ì‹œê°„)

#### ëª©í‘œ
- MPS ì „ìš© ìµœì í™” ê²½ë¡œ êµ¬í˜„
- ì„¤ì • ê¸°ë°˜ ìë™ ê²½ë¡œ ì„ íƒ

#### ìˆ˜ì • ë‚´ìš©

**1. MPS ìµœì í™” ìœ í‹¸ë¦¬í‹° ìƒì„±**: `src/utils/mps_optimizer.py`
```python
"""MPS ë°±ì—”ë“œ ìµœì í™” ìœ í‹¸ë¦¬í‹°"""

class MPSOptimizer:
    """MPS íŠ¹í™” í…ì„œ ì—°ì‚° ìµœì í™”"""

    @staticmethod
    def is_mps_available() -> bool:
        """MPS ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        return (
            torch.backends.mps.is_available() and
            torch.backends.mps.is_built()
        )

    @staticmethod
    def optimize_4d_stack(tensors: List[torch.Tensor], dim: int) -> torch.Tensor:
        """MPS ìµœì í™”ëœ 4D ìŠ¤íƒ ì—°ì‚°"""
        if dim == 2 and MPSOptimizer.is_mps_available():
            # MPS ìµœì í™”: cat + reshape
            B, S, V = tensors[0].shape
            H = len(tensors)

            # Method 1: Direct assignment (most MPS-friendly)
            result = torch.zeros(B, S, H, V, device=tensors[0].device)
            for i, t in enumerate(tensors):
                result[:, :, i, :] = t
            return result.contiguous()
        else:
            # CUDA/CPU: ê¸°ì¡´ ë°©ì‹
            return torch.stack(tensors, dim=dim)
```

**2. compute_weighted_mtp_loss ê°œì„ **
```python
# src/components/trainer/base_wmtp_trainer.py

def compute_weighted_mtp_loss(
    logits: torch.Tensor,
    target_labels: torch.Tensor,
    head_weights: torch.Tensor,
    ignore_index: int = -100,
    selection_mask: torch.Tensor | None = None,
    use_mps_optimization: bool | None = None,  # ìƒˆ íŒŒë¼ë¯¸í„°
) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """MPS ìµœì í™” ì˜µì…˜ì´ ì¶”ê°€ëœ WMTP ì†ì‹¤ ê³„ì‚°"""

    from src.utils.mps_optimizer import MPSOptimizer

    # MPS ìµœì í™” ìë™ ê°ì§€
    if use_mps_optimization is None:
        use_mps_optimization = MPSOptimizer.is_mps_available()

    B, S, H, V = logits.shape

    if use_mps_optimization:
        # MPS ìµœì í™” ê²½ë¡œ: í—¤ë“œë³„ 3D ì²˜ë¦¬
        ce_list = []
        for h in range(H):
            logits_h = logits[:, :, h, :].contiguous()
            labels_h = target_labels[:, :, h].contiguous()

            ce_h = F.cross_entropy(
                logits_h.transpose(1, 2),
                labels_h,
                ignore_index=ignore_index,
                reduction='none'
            )
            ce_list.append(ce_h)

        ce_per_head = torch.stack(ce_list, dim=2).contiguous()
    else:
        # ê¸°ì¡´ CUDA ìµœì í™” ê²½ë¡œ (4D flatten)
        logits_flat = logits.view(B * S * H, V)
        target_flat = target_labels.view(B * S * H)

        ce_flat = F.cross_entropy(
            logits_flat, target_flat,
            ignore_index=ignore_index,
            reduction='none'
        )
        ce_per_head = ce_flat.view(B, S, H)

    # ì´í›„ ë¡œì§ì€ ë™ì¼...
    return final_loss, token_valid_mask, ce_per_head
```

**3. modeling.py MPS ìµœì í™”**
```python
# tests/tiny_models/distilgpt2-mtp/modeling.py
def forward(self, ...):
    # ...
    from src.utils.mps_optimizer import MPSOptimizer

    # MPS ìµœì í™”ëœ ìŠ¤íƒ
    mtp_logits = MPSOptimizer.optimize_4d_stack(all_logits, dim=2)
    # ...
```

#### ì„±ê³µ ê¸°ì¤€
- [ ] MPSì—ì„œ ì „ì²´ í•™ìŠµ ì™„ë£Œ (max_steps=100)
- [ ] MPS ì„±ëŠ¥: CPU ëŒ€ë¹„ 2x ì´ìƒ
- [ ] CUDA ì„±ëŠ¥ ì˜í–¥: Â±1% ì´ë‚´

---

### âš™ï¸ Phase 3: ì „ì²´ ìµœì í™” ë° ì„¤ì • í†µí•© (4ì‹œê°„)

#### ëª©í‘œ
- ì„¤ì • íŒŒì¼ ê¸°ë°˜ MPS ìµœì í™” í† ê¸€
- torch.compile() í†µí•©
- ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì¶”ê°€

#### ìˆ˜ì • ë‚´ìš©

**1. ì„¤ì • ìŠ¤í‚¤ë§ˆ í™•ì¥**: `src/settings/config_schema.py`
```python
class DeviceConfig(BaseModel):
    compute_backend: Literal["cuda", "mps", "cpu"] = "cuda"
    mixed_precision: str = "bf16"
    mps_optimization: bool = True  # ìƒˆ í•„ë“œ
    use_torch_compile: bool = False  # ìƒˆ í•„ë“œ
```

**2. torch.compile í†µí•©** (PyTorch 2.0+)
```python
# src/components/trainer/base_wmtp_trainer.py
if self.config.get("use_torch_compile", False):
    compute_weighted_mtp_loss = torch.compile(
        compute_weighted_mtp_loss,
        backend="inductor",
        mode="reduce-overhead"
    )
```

**3. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì¶”ê°€**
```python
# MLflow ë©”íŠ¸ë¦­ì— MPS ì„±ëŠ¥ ì§€í‘œ ì¶”ê°€
if self.device.type == "mps":
    metrics["device/mps_memory_allocated"] = torch.mps.current_allocated_memory()
    metrics["device/mps_optimization_enabled"] = use_mps_optimization
```

#### ì„±ê³µ ê¸°ì¤€
- [ ] ì„¤ì • ê¸°ë°˜ MPS ìµœì í™” on/off ê°€ëŠ¥
- [ ] torch.compile ì ìš© ì‹œ ì¶”ê°€ 10% ì„±ëŠ¥ í–¥ìƒ
- [ ] MLflowì—ì„œ MPS ë©”íŠ¸ë¦­ í™•ì¸ ê°€ëŠ¥

---

## 4. ê²€ì¦ ê³„íš

### 4.1 ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
```bash
# MPS í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸
pytest tests/test_mps_compatibility.py -v

# ì„±ëŠ¥ íšŒê·€ í…ŒìŠ¤íŠ¸
pytest tests/test_performance_regression.py -v
```

### 4.2 í†µí•© í…ŒìŠ¤íŠ¸
```bash
# M3 MacBook ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸
python tests/script/test_m3_pipeline.py \
    --config tests/configs/config.local_test.yaml \
    --recipe tests/configs/recipe.mtp_baseline.yaml
```

### 4.3 ë²¤ì¹˜ë§ˆí¬
```bash
# MPS vs CPU vs CUDA ì„±ëŠ¥ ë¹„êµ
python benchmarks/compare_backends.py --report
```

---

## 5. ìœ„í—˜ ìš”ì†Œ ë° ëŒ€ì‘

| ìœ„í—˜ | í™•ë¥  | ì˜í–¥ | ëŒ€ì‘ ë°©ì•ˆ |
|------|------|------|-----------|
| CUDA ì„±ëŠ¥ ì €í•˜ | ë‚®ìŒ | ë†’ìŒ | ì¡°ê±´ë¶€ ë¶„ê¸°ë¡œ ê²©ë¦¬ |
| MPS ë²„ê·¸ | ì¤‘ê°„ | ì¤‘ê°„ | PyTorch ì—…ë°ì´íŠ¸ ì¶”ì  |
| ì½”ë“œ ë³µì¡ë„ ì¦ê°€ | ë†’ìŒ | ë‚®ìŒ | ìœ í‹¸ë¦¬í‹° í´ë˜ìŠ¤ë¡œ ìº¡ìŠí™” |

---

## 6. ì¼ì • ë° ë§ˆì¼ìŠ¤í†¤

| Phase | ì˜ˆìƒ ì‹œê°„ | ì™„ë£Œ ê¸°ì¤€ | ë‹´ë‹¹ì |
|-------|-----------|-----------|--------|
| Phase 0 | 2ì‹œê°„ | í…ŒìŠ¤íŠ¸ ì¸í”„ë¼ êµ¬ì¶• | - |
| Phase 1 | 2ì‹œê°„ | ìµœì†Œ ì‘ë™ í™•ì¸ | - |
| Phase 2 | 4ì‹œê°„ | MPS ìµœì í™” ì™„ë£Œ | - |
| Phase 3 | 4ì‹œê°„ | ì „ì²´ í†µí•© ì™„ë£Œ | - |
| **í•©ê³„** | **12ì‹œê°„** | **MPS ì™„ì „ ì§€ì›** | - |

---

## 7. ì„±ê³¼ ì¸¡ì • (í•„ìˆ˜5: ê°ê´€ì  í‰ê°€)

### 7.1 ì •ëŸ‰ì  ì§€í‘œ
- **MPS ì‘ë™ë¥ **: 0% â†’ 100% (ë¸”ë¡œí‚¹ í•´ì†Œ)
- **MPS ì„±ëŠ¥**: CPU ëŒ€ë¹„ 2-3x í–¥ìƒ ëª©í‘œ
- **CUDA ì˜í–¥**: Â±1% ì´ë‚´ ìœ ì§€
- **ì½”ë“œ ì¦ê°€ëŸ‰**: +200ì¤„ ì´ë‚´

### 7.2 ì •ì„±ì  ì„±ê³¼
- MacBook ê°œë°œìì˜ ë¡œì»¬ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥
- ì˜¤í”ˆì†ŒìŠ¤ ì ‘ê·¼ì„± í–¥ìƒ
- PyTorch MPS ìƒíƒœê³„ ê¸°ì—¬

---

## 8. ê²°ë¡ 

ë³¸ ê³„íšì€ WMTPì˜ 4D í…ì„œ ì•„í‚¤í…ì²˜ë¥¼ ìœ ì§€í•˜ë©´ì„œ MPS í˜¸í™˜ì„±ì„ í™•ë³´í•˜ëŠ” ì‹¤ìš©ì  ì ‘ê·¼ë²•ì…ë‹ˆë‹¤. ê¸°ì¡´ CUDA ì„±ëŠ¥ì— ì˜í–¥ ì—†ì´ Apple Silicon ì§€ì›ì„ ì¶”ê°€í•˜ì—¬ ê°œë°œì ê²½í—˜ì„ í¬ê²Œ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**í•µì‹¬ ì„±ê³µ ìš”ì†Œ**:
1. ì¡°ê±´ë¶€ ìµœì í™”ë¡œ ìœ„í—˜ ìµœì†Œí™”
2. ë‹¨ê³„ì  ì ‘ê·¼ìœ¼ë¡œ ì•ˆì •ì„± í™•ë³´
3. ê¸°ì¡´ êµ¬ì¡° ì¡´ì¤‘ìœ¼ë¡œ ìœ ì§€ë³´ìˆ˜ì„± ìœ ì§€

---

*ì‘ì„±ì¼: 2024-09-27*
*ë²„ì „: 1.0*
*ìƒíƒœ: ê²€í†  ëŒ€ê¸°*
