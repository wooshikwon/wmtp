#!/usr/bin/env python3
"""
ë©”íƒ€ë°ì´í„°ë§Œ ìƒì„±í•˜ì—¬ S3ì— ì—…ë¡œë“œ
(ëŒ€ìš©ëŸ‰ íŒŒì¼ ë³µì‚¬ ì—†ì´ í‘œì¤€í™” êµ¬ì¡°ë§Œ ìƒì„±)
"""

import json
import os
from datetime import datetime
from pathlib import Path

import boto3
import yaml
from dotenv import load_dotenv
from rich.console import Console

console = Console()


def create_and_upload_metadata_only():
    """ë©”íƒ€ë°ì´í„°ë§Œ ìƒì„±í•˜ì—¬ ë¹ ë¥´ê²Œ í…ŒìŠ¤íŠ¸"""

    load_dotenv()

    s3_client = boto3.client(
        "s3",
        aws_access_key_id=os.getenv("AWS_ACCESS_KEY_ID"),
        aws_secret_access_key=os.getenv("AWS_SECRET_ACCESS_KEY"),
        region_name="eu-north-1"
    )

    # Sheared-LLaMA ë©”íƒ€ë°ì´í„°
    metadata = {
        "wmtp_type": "base_model",
        "training_algorithm": "sheared_training",
        "horizon": 4,
        "n_heads": 4,
        "base_architecture": "llama",
        "model_size": "2.7b",
        "storage_version": "2.0",
        "created_by": "metadata_only_test",
        "standardization_date": datetime.now().isoformat(),
        "original_format": "sharded_pytorch_bins",
        "source_path": "s3://wmtp/models/Sheared-LLaMA-2.7B",
        "loading_strategy": {
            "loader_type": "huggingface",
            "model_class_name": None,
            "custom_module_file": None,
            "transformers_class": "AutoModelForCausalLM",
            "state_dict_mapping": {
                "remove_prefix": None,
                "add_prefix": None,
                "key_transforms": {}
            },
            "required_files": [
                "config.json",
                "pytorch_model-00001-of-00002.bin",
                "pytorch_model-00002-of-00002.bin",
                "pytorch_model.bin.index.json",
                "tokenizer.model",
                "metadata.json"
            ]
        },
        "algorithm_compatibility": ["baseline-mtp", "rho1-wmtp"]
    }

    # ë¡œì»¬ì— ì„ì‹œ ì €ì¥
    temp_dir = Path("/tmp/metadata_test")
    temp_dir.mkdir(exist_ok=True)

    # metadata.json
    metadata_file = temp_dir / "metadata.json"
    with open(metadata_file, "w") as f:
        json.dump(metadata, f, indent=2)

    # MLflow íŒŒì¼ë“¤
    mlflow_dir = temp_dir / "mlflow_model"
    mlflow_dir.mkdir(exist_ok=True)

    mlmodel = {
        "artifact_path": "model",
        "flavors": {
            "python_function": {
                "env": {"conda": "conda.yaml", "virtualenv": "python_env.yaml"},
                "loader_module": "mlflow.transformers",
                "model_path": "model",
                "python_version": "3.11.7"
            },
            "transformers": {
                "code": None,
                "framework": "pt",
                "model_path": "model",
                "task": "text-generation"
            }
        },
        "mlflow_version": "2.15.1",
        "model_size_bytes": 10794508288,  # 10GB
        "model_uuid": "sheared-llama-2.7b",
        "signature": {
            "inputs": '[{"type": "string"}]',
            "outputs": '[{"type": "string"}]'
        },
        "utc_time_created": datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f")
    }

    with open(mlflow_dir / "MLmodel", "w") as f:
        yaml.dump(mlmodel, f, default_flow_style=False, sort_keys=False)

    with open(mlflow_dir / "requirements.txt", "w") as f:
        f.write("transformers\ntorch")

    conda_env = {
        "name": "sheared_llama_env",
        "channels": ["defaults"],
        "dependencies": ["python=3.11.7", {"pip": ["transformers", "torch"]}]
    }

    with open(mlflow_dir / "conda.yaml", "w") as f:
        yaml.dump(conda_env, f, default_flow_style=False, sort_keys=False)

    # S3ì— ë©”íƒ€ë°ì´í„°ë§Œ ì—…ë¡œë“œ
    console.print("[yellow]ğŸ“¤ ë©”íƒ€ë°ì´í„° ì—…ë¡œë“œ ì¤‘...[/yellow]")

    target_prefix = "models_v2/sheared-llama-2.7b-test"

    # metadata.json
    s3_client.upload_file(
        str(metadata_file),
        "wmtp",
        f"{target_prefix}/metadata.json"
    )
    console.print("âœ… metadata.json ì—…ë¡œë“œ ì™„ë£Œ")

    # MLflow íŒŒì¼ë“¤
    for file_name in ["MLmodel", "requirements.txt", "conda.yaml"]:
        local_file = mlflow_dir / file_name
        s3_client.upload_file(
            str(local_file),
            "wmtp",
            f"{target_prefix}/mlflow_model/{file_name}"
        )
        console.print(f"âœ… {file_name} ì—…ë¡œë“œ ì™„ë£Œ")

    # ê¸°ì¡´ íŒŒì¼ë“¤ì— ëŒ€í•œ symbolic link ì •ë³´ ìƒì„±
    link_info = {
        "model_files_location": "s3://wmtp/models/Sheared-LLaMA-2.7B/",
        "note": "ì‹¤ì œ ëª¨ë¸ íŒŒì¼ì€ ì›ë³¸ ìœ„ì¹˜ì—ì„œ ì§ì ‘ ì°¸ì¡°"
    }

    link_file = temp_dir / "link_info.json"
    with open(link_file, "w") as f:
        json.dump(link_info, f, indent=2)

    s3_client.upload_file(
        str(link_file),
        "wmtp",
        f"{target_prefix}/link_info.json"
    )

    console.print(f"\n[green]âœ… ë©”íƒ€ë°ì´í„° ìƒì„± ì™„ë£Œ![/green]")
    console.print(f"ìœ„ì¹˜: s3://wmtp/{target_prefix}/")
    console.print("ì‹¤ì œ ëª¨ë¸ íŒŒì¼ì€ ì›ë³¸ ìœ„ì¹˜ì—ì„œ ì°¸ì¡°")

    return f"s3://wmtp/{target_prefix}/"


if __name__ == "__main__":
    path = create_and_upload_metadata_only()
    console.print(f"\n[bold green]í…ŒìŠ¤íŠ¸ìš© ë©”íƒ€ë°ì´í„° ê²½ë¡œ:[/bold green]")
    console.print(path)