#!/usr/bin/env python3
"""
AWS S3 ëŒ€ìš©ëŸ‰ íŒŒì¼ ìµœì í™” ë‹¤ìš´ë¡œë”

10GB+ ëª¨ë¸ íŒŒì¼ë“¤ì„ ìœ„í•œ ë©€í‹°íŒŒíŠ¸ ë³‘ë ¬ ë‹¤ìš´ë¡œë“œ ìµœì í™”
- TransferConfigë¡œ ë³‘ë ¬ ë‹¤ìš´ë¡œë“œ ì„¤ì •
- ì§„í–‰ë¥  í‘œì‹œ
- ë„¤íŠ¸ì›Œí¬ ì¬ì‹œë„ ë¡œì§
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì²˜ë¦¬
"""

import os
import tempfile
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path

import boto3
from boto3.s3.transfer import TransferConfig
from botocore.exceptions import ClientError
from rich.console import Console
from rich.progress import (
    BarColumn,
    DownloadColumn,
    Progress,
    TaskID,
    TextColumn,
    TimeRemainingColumn,
    TransferSpeedColumn,
)

console = Console()


class OptimizedS3Downloader:
    """S3 ëŒ€ìš©ëŸ‰ íŒŒì¼ ìµœì í™” ë‹¤ìš´ë¡œë”"""

    def __init__(
        self, aws_access_key: str, aws_secret_key: str, region: str = "eu-north-1"
    ):
        self.s3_client = boto3.client(
            "s3",
            aws_access_key_id=aws_access_key,
            aws_secret_access_key=aws_secret_key,
            region_name=region,
        )
        self.bucket = "wmtp"

        # ëŒ€ìš©ëŸ‰ íŒŒì¼ìš© ìµœì í™” ì„¤ì •
        self.transfer_config = TransferConfig(
            multipart_threshold=1024 * 25,  # 25MB ì´ìƒë¶€í„° ë©€í‹°íŒŒíŠ¸
            max_concurrency=10,  # ìµœëŒ€ 10ê°œ ë³‘ë ¬ ì—°ê²°
            multipart_chunksize=1024 * 25,  # 25MB ì²­í¬
            use_threads=True,  # ìŠ¤ë ˆë“œ í’€ ì‚¬ìš©
            num_download_attempts=3,  # ì¬ì‹œë„ 3íšŒ
            max_io_queue=100,  # I/O í í¬ê¸°
            io_chunksize=1024 * 256,  # 256KB I/O ì²­í¬ (ë©”ëª¨ë¦¬ íš¨ìœ¨)
        )

        console.print("[green]âœ… S3 ìµœì í™” ë‹¤ìš´ë¡œë” ì´ˆê¸°í™”[/green]")
        console.print("  ë©€í‹°íŒŒíŠ¸ ì„ê³„ì : 25MB")
        console.print("  ìµœëŒ€ ë³‘ë ¬ ì—°ê²°: 10")
        console.print("  ì²­í¬ í¬ê¸°: 25MB")

    def get_file_list(self, prefix: str) -> list[tuple[str, int]]:
        """S3 íŒŒì¼ ë¦¬ìŠ¤íŠ¸ì™€ í¬ê¸° ê°€ì ¸ì˜¤ê¸°"""
        console.print(f"[yellow]S3 íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ì¡°íšŒ: {prefix}[/yellow]")

        files = []
        paginator = self.s3_client.get_paginator("list_objects_v2")

        for page in paginator.paginate(Bucket=self.bucket, Prefix=prefix):
            if "Contents" not in page:
                continue

            for obj in page["Contents"]:
                key = obj["Key"]
                size = obj["Size"]
                files.append((key, size))

                # íŒŒì¼ í¬ê¸°ë¥¼ ì‚¬ëŒì´ ì½ê¸° ì‰¬ìš´ í˜•íƒœë¡œ í‘œì‹œ
                if size > 1024**3:  # GB
                    size_str = f"{size / (1024**3):.1f}GB"
                elif size > 1024**2:  # MB
                    size_str = f"{size / (1024**2):.1f}MB"
                else:  # KB
                    size_str = f"{size / 1024:.1f}KB"

                console.print(f"  ğŸ“ {key.split('/')[-1]} ({size_str})")

        return files

    def download_file_optimized(
        self,
        s3_key: str,
        local_path: Path,
        file_size: int,
        progress: Progress,
        task_id: TaskID,
    ):
        """ìµœì í™”ëœ ë‹¨ì¼ íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
        try:
            # íŒŒì¼ í¬ê¸°ê°€ í´ ê²½ìš° ë©€í‹°íŒŒíŠ¸ ë‹¤ìš´ë¡œë“œ ìë™ ì ìš©
            local_path.parent.mkdir(parents=True, exist_ok=True)

            # ì§„í–‰ë¥  ì½œë°± í•¨ìˆ˜
            def progress_callback(bytes_transferred):
                progress.update(task_id, advance=bytes_transferred)

            # S3 ë‹¤ìš´ë¡œë“œ (boto3ê°€ ìë™ìœ¼ë¡œ ë©€í‹°íŒŒíŠ¸ íŒë‹¨)
            self.s3_client.download_file(
                self.bucket,
                s3_key,
                str(local_path),
                Config=self.transfer_config,
                Callback=progress_callback,
            )

            return True, None

        except ClientError as e:
            error_msg = f"S3 Client Error: {e}"
            return False, error_msg
        except Exception as e:
            error_msg = f"Download Error: {e}"
            return False, error_msg

    def download_model_parallel(self, source_prefix: str, local_dir: Path) -> bool:
        """ë³‘ë ¬ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"""
        console.print("[bold blue]ğŸš€ ìµœì í™”ëœ S3 ë‹¤ìš´ë¡œë“œ ì‹œì‘[/bold blue]")
        console.print(f"ì†ŒìŠ¤: s3://{self.bucket}/{source_prefix}")
        console.print(f"ëŒ€ìƒ: {local_dir}")

        # íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        files = self.get_file_list(source_prefix)
        if not files:
            console.print("[red]âŒ ë‹¤ìš´ë¡œë“œí•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤[/red]")
            return False

        # ì´ íŒŒì¼ í¬ê¸° ê³„ì‚°
        total_size = sum(size for _, size in files)
        console.print(
            f"[cyan]ì´ ë‹¤ìš´ë¡œë“œ í¬ê¸°: {total_size / (1024**3):.2f}GB ({len(files)}ê°œ íŒŒì¼)[/cyan]"
        )

        # ì§„í–‰ë¥  í‘œì‹œê¸° ì„¤ì •
        with Progress(
            TextColumn("[bold blue]{task.description}"),
            BarColumn(),
            "[progress.percentage]{task.percentage:>3.1f}%",
            DownloadColumn(),
            TransferSpeedColumn(),
            TimeRemainingColumn(),
            console=console,
            refresh_per_second=4,  # ì´ˆë‹¹ 4íšŒ ê°±ì‹ 
        ) as progress:
            # ê° íŒŒì¼ë³„ ë‹¤ìš´ë¡œë“œ íƒœìŠ¤í¬ ìƒì„±
            download_tasks = []
            for s3_key, file_size in files:
                file_name = s3_key.split("/")[-1]
                local_path = local_dir / s3_key.replace(source_prefix, "").lstrip("/")

                task_id = progress.add_task(f"ğŸ“¥ {file_name}", total=file_size)

                download_tasks.append((s3_key, local_path, file_size, task_id))

            # ëŒ€ìš©ëŸ‰ íŒŒì¼ì€ ìˆœì°¨ ë‹¤ìš´ë¡œë“œ, ì‘ì€ íŒŒì¼ë“¤ì€ ë³‘ë ¬ ì²˜ë¦¬
            large_files = [
                (key, path, size, task_id)
                for key, path, size, task_id in download_tasks
                if size > 100 * 1024 * 1024
            ]  # 100MB ì´ìƒ
            small_files = [
                (key, path, size, task_id)
                for key, path, size, task_id in download_tasks
                if size <= 100 * 1024 * 1024
            ]

            success_count = 0

            # ëŒ€ìš©ëŸ‰ íŒŒì¼ ìˆœì°¨ ì²˜ë¦¬ (ë©€í‹°íŒŒíŠ¸ ìë™ ì ìš©)
            for s3_key, local_path, file_size, task_id in large_files:
                success, error = self.download_file_optimized(
                    s3_key, local_path, file_size, progress, task_id
                )
                if success:
                    success_count += 1
                    progress.update(task_id, completed=file_size)
                else:
                    console.print(f"[red]âŒ ì‹¤íŒ¨: {s3_key} - {error}[/red]")

            # ì‘ì€ íŒŒì¼ë“¤ ë³‘ë ¬ ì²˜ë¦¬ (ìµœëŒ€ 5ê°œ ë™ì‹œ)
            if small_files:
                with ThreadPoolExecutor(max_workers=5) as executor:
                    future_to_task = {}
                    for s3_key, local_path, file_size, task_id in small_files:
                        future = executor.submit(
                            self.download_file_optimized,
                            s3_key,
                            local_path,
                            file_size,
                            progress,
                            task_id,
                        )
                        future_to_task[future] = (s3_key, task_id, file_size)

                    for future in as_completed(future_to_task):
                        s3_key, task_id, file_size = future_to_task[future]
                        try:
                            success, error = future.result()
                            if success:
                                success_count += 1
                                progress.update(task_id, completed=file_size)
                            else:
                                console.print(f"[red]âŒ ì‹¤íŒ¨: {s3_key} - {error}[/red]")
                        except Exception as e:
                            console.print(f"[red]âŒ ì˜ˆì™¸: {s3_key} - {e}[/red]")

        # ê²°ê³¼ ìš”ì•½
        total_files = len(files)
        if success_count == total_files:
            console.print(
                f"[bold green]ğŸ‰ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {success_count}/{total_files} íŒŒì¼ ì„±ê³µ[/bold green]"
            )
            return True
        else:
            console.print(
                f"[yellow]âš ï¸ ë¶€ë¶„ ì„±ê³µ: {success_count}/{total_files} íŒŒì¼ ì™„ë£Œ[/yellow]"
            )
            return success_count > 0


def test_sheared_llama_download():
    """Sheared LLaMA ë‹¤ìš´ë¡œë“œ í…ŒìŠ¤íŠ¸"""
    console.print("[bold blue]ğŸ§ª Sheared LLaMA 2.7B ìµœì í™” ë‹¤ìš´ë¡œë“œ í…ŒìŠ¤íŠ¸[/bold blue]")

    # AWS ì¸ì¦ ì •ë³´ - í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ
    downloader = OptimizedS3Downloader(
        os.getenv("AWS_ACCESS_KEY_ID"), os.getenv("AWS_SECRET_ACCESS_KEY")
    )

    with tempfile.TemporaryDirectory() as temp_dir:
        temp_path = Path(temp_dir)
        local_dir = temp_path / "sheared_llama"

        console.print(f"[cyan]ì„ì‹œ ë””ë ‰í† ë¦¬: {local_dir}[/cyan]")

        # ìµœì í™”ëœ ë‹¤ìš´ë¡œë“œ ì‹¤í–‰
        success = downloader.download_model_parallel(
            "models/Sheared-LLaMA-2.7B/", local_dir
        )

        if success:
            # ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ í™•ì¸
            console.print("[green]ğŸ“‚ ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ë“¤:[/green]")
            for file_path in local_dir.rglob("*"):
                if file_path.is_file():
                    size = file_path.stat().st_size
                    if size > 1024**3:
                        size_str = f"{size / (1024**3):.2f}GB"
                    elif size > 1024**2:
                        size_str = f"{size / (1024**2):.1f}MB"
                    else:
                        size_str = f"{size / 1024:.1f}KB"
                    console.print(f"  âœ… {file_path.name} ({size_str})")

            console.print(
                f"[bold green]âœ… ë‹¤ìš´ë¡œë“œ ì„±ê³µ! ì„ì‹œí´ë”: {local_dir}[/bold green]"
            )
            console.print("[yellow]ğŸ’¡ ì‹¤ì œ ë³€í™˜ ì‘ì—…ì„ ê³„ì† ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ?[/yellow]")
        else:
            console.print("[red]âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨[/red]")


if __name__ == "__main__":
    # í™˜ê²½ë³€ìˆ˜ ì„¤ì • - .env íŒŒì¼ì—ì„œ ë¡œë“œ
    from dotenv import load_dotenv

    load_dotenv()

    test_sheared_llama_download()
