#!/usr/bin/env python3
"""
S3 Model Pipeline - ëŒ€ìš©ëŸ‰ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ, í‘œì¤€í™”, ì—…ë¡œë“œ í†µí•© ë„êµ¬

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” S3ì—ì„œ ëŒ€ìš©ëŸ‰ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ê³ , HuggingFace í‘œì¤€ êµ¬ì¡°ë¡œ ë³€í™˜í•œ ë’¤,
ë‹¤ì‹œ S3ì— ì—…ë¡œë“œí•˜ëŠ” ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ì œê³µí•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- 10GB+ ëŒ€ìš©ëŸ‰ íŒŒì¼ ìµœì í™” ë©€í‹°íŒŒíŠ¸ ë‹¤ìš´ë¡œë“œ
- HuggingFace + MLflow í•˜ì´ë¸Œë¦¬ë“œ í‘œì¤€ êµ¬ì¡°ë¡œ ë³€í™˜
- safetensors ë³´ì•ˆ í˜•ì‹ ì‚¬ìš©
- S3 ì—…ë¡œë“œ with ì§„í–‰ë¥  í‘œì‹œ

í‘œì¤€ ì¶œë ¥ êµ¬ì¡°:
model_name/
â”œâ”€â”€ model.safetensors           # HF í‘œì¤€ (ë³´ì•ˆ)
â”œâ”€â”€ config.json                # HF í‘œì¤€ + WMTP í™•ì¥
â”œâ”€â”€ tokenizer.json             # HF í‘œì¤€
â”œâ”€â”€ tokenizer_config.json      # HF í‘œì¤€
â”œâ”€â”€ special_tokens_map.json    # HF í‘œì¤€
â”œâ”€â”€ metadata.json              # WMTP ì „ìš© ë©”íƒ€ë°ì´í„°
â””â”€â”€ mlflow_model/              # MLflow í˜¸í™˜ì„±
    â”œâ”€â”€ MLmodel               # MLflow ë©”íƒ€ë°ì´í„°
    â”œâ”€â”€ requirements.txt      # ì˜ì¡´ì„±
    â””â”€â”€ conda.yaml           # í™˜ê²½
"""

import json
import os
import shutil
import tempfile
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from pathlib import Path
from typing import Any, Optional

import boto3
import torch
import yaml
from boto3.s3.transfer import TransferConfig
from botocore.exceptions import ClientError
from dotenv import load_dotenv
from rich.console import Console
from rich.progress import (
    BarColumn,
    DownloadColumn,
    Progress,
    TaskID,
    TextColumn,
    TimeRemainingColumn,
    TransferSpeedColumn,
)
from safetensors.torch import save_file
from transformers import AutoModelForCausalLM

console = Console()


class S3ModelPipeline:
    """S3 ëª¨ë¸ ë‹¤ìš´ë¡œë“œ, í‘œì¤€í™”, ì—…ë¡œë“œ í†µí•© íŒŒì´í”„ë¼ì¸"""

    def __init__(
        self,
        aws_access_key: Optional[str] = None,
        aws_secret_key: Optional[str] = None,
        region: str = "eu-north-1",
    ):
        # í™˜ê²½ë³€ìˆ˜ì—ì„œ AWS ì¸ì¦ ì •ë³´ ë¡œë“œ
        if not aws_access_key:
            aws_access_key = os.getenv("AWS_ACCESS_KEY_ID")
        if not aws_secret_key:
            aws_secret_key = os.getenv("AWS_SECRET_ACCESS_KEY")

        if not aws_access_key or not aws_secret_key:
            raise ValueError("AWS credentials not found in environment variables")

        self.s3_client = boto3.client(
            "s3",
            aws_access_key_id=aws_access_key,
            aws_secret_access_key=aws_secret_key,
            region_name=region,
        )
        self.bucket = "wmtp"

        # ëŒ€ìš©ëŸ‰ íŒŒì¼ìš© ìµœì í™” ì „ì†¡ ì„¤ì • (ì†ë„ ê°œì„ )
        self.transfer_config = TransferConfig(
            multipart_threshold=1024 * 5,  # 5MB ì´ìƒë¶€í„° ë©€í‹°íŒŒíŠ¸ (ë” ë¹ ë¥¸ ë³‘ë ¬í™”)
            max_concurrency=20,  # ìµœëŒ€ 20ê°œ ë³‘ë ¬ ì—°ê²° (2ë°° ì¦ê°€)
            multipart_chunksize=1024 * 10,  # 10MB ì²­í¬ (ë” ì‘ì€ ì²­í¬ë¡œ ë³‘ë ¬ì„± ì¦ê°€)
            use_threads=True,  # ìŠ¤ë ˆë“œ í’€ ì‚¬ìš©
            num_download_attempts=3,  # ì¬ì‹œë„ 3íšŒ
            max_io_queue=1000,  # I/O í í¬ê¸° 10ë°° ì¦ê°€
            io_chunksize=1024 * 1024,  # 1MB I/O ì²­í¬ (ì²˜ë¦¬ëŸ‰ ì¦ê°€)
        )

        console.print("[green]âœ… S3 Model Pipeline ì´ˆê¸°í™” ì™„ë£Œ[/green]")

    def process_model(
        self,
        source_s3_path: str,
        target_s3_path: str,
        wmtp_type: str = "base_model",
        model_info: Optional[dict[str, Any]] = None,
    ) -> bool:
        """
        S3 ëª¨ë¸ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        1. S3ì—ì„œ ë‹¤ìš´ë¡œë“œ
        2. í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        3. S3ì— ì—…ë¡œë“œ
        """
        console.print(f"[bold blue]ğŸš€ ëª¨ë¸ íŒŒì´í”„ë¼ì¸ ì‹œì‘[/bold blue]")
        console.print(f"ì†ŒìŠ¤: {source_s3_path}")
        console.print(f"ëŒ€ìƒ: {target_s3_path}")
        console.print(f"íƒ€ì…: {wmtp_type}")

        if model_info is None:
            model_info = {}

        try:
            with tempfile.TemporaryDirectory() as temp_dir:
                temp_path = Path(temp_dir)
                download_path = temp_path / "downloaded"
                standardized_path = temp_path / "standardized"

                # 1. S3ì—ì„œ ë‹¤ìš´ë¡œë“œ (ìµœì í™”ëœ ë©€í‹°íŒŒíŠ¸)
                console.print("\n[yellow]ğŸ“¥ Step 1/3: S3 ë‹¤ìš´ë¡œë“œ ì¤‘...[/yellow]")
                if not self._download_from_s3(source_s3_path, download_path):
                    return False

                # 2. í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                console.print("\n[yellow]ğŸ”„ Step 2/3: í‘œì¤€ í˜•ì‹ ë³€í™˜ ì¤‘...[/yellow]")
                self._convert_to_standard(
                    download_path, standardized_path, wmtp_type, model_info
                )

                # 3. S3ì— ì—…ë¡œë“œ
                console.print("\n[yellow]ğŸ“¤ Step 3/3: S3 ì—…ë¡œë“œ ì¤‘...[/yellow]")
                self._upload_to_s3(standardized_path, target_s3_path)

            console.print(
                f"\n[bold green]âœ… íŒŒì´í”„ë¼ì¸ ì™„ë£Œ: {target_s3_path}[/bold green]"
            )
            return True

        except Exception as e:
            console.print(f"\n[red]âŒ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {e}[/red]")
            return False

    def _download_from_s3(self, s3_path: str, local_path: Path) -> bool:
        """S3ì—ì„œ ìµœì í™”ëœ ë©€í‹°íŒŒíŠ¸ ë‹¤ìš´ë¡œë“œ"""
        local_path.mkdir(parents=True, exist_ok=True)
        prefix = s3_path.replace(f"s3://{self.bucket}/", "").rstrip("/")

        # íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ì¡°íšŒ
        files = []
        paginator = self.s3_client.get_paginator("list_objects_v2")

        for page in paginator.paginate(Bucket=self.bucket, Prefix=prefix):
            if "Contents" not in page:
                continue

            for obj in page["Contents"]:
                key = obj["Key"]
                size = obj["Size"]
                # ë””ë ‰í† ë¦¬ê°€ ì•„ë‹Œ ì‹¤ì œ íŒŒì¼ë§Œ
                if not key.endswith("/"):
                    files.append((key, size))

        if not files:
            console.print("[red]âŒ ë‹¤ìš´ë¡œë“œí•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤[/red]")
            return False

        # ì´ í¬ê¸° ê³„ì‚°
        total_size = sum(size for _, size in files)
        console.print(
            f"ì´ {len(files)}ê°œ íŒŒì¼, {total_size / (1024**3):.2f}GB ë‹¤ìš´ë¡œë“œ"
        )

        # ì§„í–‰ë¥  í‘œì‹œì™€ í•¨ê»˜ ë‹¤ìš´ë¡œë“œ
        with Progress(
            TextColumn("[bold blue]{task.description}"),
            BarColumn(),
            "[progress.percentage]{task.percentage:>3.1f}%",
            DownloadColumn(),
            TransferSpeedColumn(),
            TimeRemainingColumn(),
            console=console,
        ) as progress:
            # ëŒ€ìš©ëŸ‰ íŒŒì¼ê³¼ ì‘ì€ íŒŒì¼ ë¶„ë¦¬
            large_files = [(k, s) for k, s in files if s > 100 * 1024 * 1024]
            small_files = [(k, s) for k, s in files if s <= 100 * 1024 * 1024]

            # ëŒ€ìš©ëŸ‰ íŒŒì¼ ìˆœì°¨ ë‹¤ìš´ë¡œë“œ (ë©€í‹°íŒŒíŠ¸)
            for key, size in large_files:
                file_name = key.split("/")[-1]
                task_id = progress.add_task(f"ğŸ“¥ {file_name}", total=size)

                local_file = local_path / key.replace(prefix + "/", "")
                local_file.parent.mkdir(parents=True, exist_ok=True)

                def progress_callback(bytes_transferred):
                    progress.update(task_id, advance=bytes_transferred)

                try:
                    self.s3_client.download_file(
                        self.bucket,
                        key,
                        str(local_file),
                        Config=self.transfer_config,
                        Callback=progress_callback,
                    )
                    progress.update(task_id, completed=size)
                except Exception as e:
                    console.print(f"[red]âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ {file_name}: {e}[/red]")
                    return False

            # ì‘ì€ íŒŒì¼ ë³‘ë ¬ ë‹¤ìš´ë¡œë“œ
            if small_files:
                with ThreadPoolExecutor(max_workers=5) as executor:
                    futures = []
                    for key, size in small_files:
                        file_name = key.split("/")[-1]
                        task_id = progress.add_task(f"ğŸ“¥ {file_name}", total=size)

                        local_file = local_path / key.replace(prefix + "/", "")
                        local_file.parent.mkdir(parents=True, exist_ok=True)

                        future = executor.submit(
                            self._download_single_file, key, local_file, size, progress, task_id
                        )
                        futures.append(future)

                    for future in as_completed(futures):
                        if not future.result():
                            return False

        console.print("[green]âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ[/green]")
        return True

    def _download_single_file(
        self, key: str, local_file: Path, size: int, progress: Progress, task_id: TaskID
    ) -> bool:
        """ë‹¨ì¼ íŒŒì¼ ë‹¤ìš´ë¡œë“œ (ì‘ì€ íŒŒì¼ìš©)"""
        try:

            def progress_callback(bytes_transferred):
                progress.update(task_id, advance=bytes_transferred)

            self.s3_client.download_file(
                self.bucket,
                key,
                str(local_file),
                Config=self.transfer_config,
                Callback=progress_callback,
            )
            progress.update(task_id, completed=size)
            return True
        except Exception as e:
            console.print(f"[red]âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ {key}: {e}[/red]")
            return False

    def _convert_to_standard(
        self,
        source_path: Path,
        target_path: Path,
        wmtp_type: str,
        model_info: dict[str, Any],
    ):
        """ëª¨ë¸ì„ HuggingFace í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        target_path.mkdir(parents=True, exist_ok=True)

        # 1. ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ safetensorsë¡œ ë³€í™˜
        self._convert_weights_to_safetensors(source_path, target_path, wmtp_type)

        # 2. ì„¤ì • íŒŒì¼ ë³µì‚¬ ë° í‘œì¤€í™”
        self._standardize_configs(source_path, target_path)

        # 3. WMTP ë©”íƒ€ë°ì´í„° ìƒì„±
        self._create_wmtp_metadata(target_path, wmtp_type, model_info)

        # 4. MLflow í˜¸í™˜ì„± íŒŒì¼ ìƒì„±
        self._create_mlflow_files(target_path, wmtp_type, model_info)

        console.print("[green]âœ… í‘œì¤€ í˜•ì‹ ë³€í™˜ ì™„ë£Œ[/green]")

    def _convert_weights_to_safetensors(
        self, source_path: Path, target_path: Path, wmtp_type: str
    ):
        """ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ safetensors í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        console.print("ëª¨ë¸ ê°€ì¤‘ì¹˜ ë³€í™˜ ì¤‘...")

        # ì´ë¯¸ safetensorsì¸ ê²½ìš°
        if (source_path / "model.safetensors").exists():
            shutil.copy2(
                source_path / "model.safetensors", target_path / "model.safetensors"
            )
            console.print("âœ… safetensors í˜•ì‹ ë³µì‚¬ ì™„ë£Œ")
            return

        # Meta consolidated.pth ë³€í™˜
        if wmtp_type == "mtp_native" and (source_path / "consolidated.pth").exists():
            checkpoint = torch.load(
                source_path / "consolidated.pth", map_location="cpu", weights_only=True
            )
            save_file(checkpoint, target_path / "model.safetensors")
            console.print("âœ… consolidated.pth â†’ safetensors ë³€í™˜ ì™„ë£Œ")
            return

        # ë‹¨ì¼ pytorch_model.bin ë³€í™˜
        if (source_path / "pytorch_model.bin").exists():
            try:
                # HuggingFace ëª¨ë¸ë¡œ ë¡œë“œ í›„ ì €ì¥
                model = AutoModelForCausalLM.from_pretrained(
                    source_path, torch_dtype=torch.float32, trust_remote_code=True
                )
                model.save_pretrained(target_path, safe_serialization=True)

                # pytorch_model.bin ì œê±°
                bin_file = target_path / "pytorch_model.bin"
                if bin_file.exists():
                    bin_file.unlink()

                console.print("âœ… pytorch_model.bin â†’ safetensors ë³€í™˜ ì™„ë£Œ")
            except Exception:
                # ì§ì ‘ ë³€í™˜
                state_dict = torch.load(
                    source_path / "pytorch_model.bin", map_location="cpu", weights_only=True
                )
                save_file(state_dict, target_path / "model.safetensors")
                console.print("âœ… pytorch_model.bin â†’ safetensors ì§ì ‘ ë³€í™˜ ì™„ë£Œ")
            return

        # ë¶„í• ëœ pytorch_model-*.bin ë³‘í•© ë° ë³€í™˜
        sharded_files = list(source_path.glob("pytorch_model-*.bin"))
        if sharded_files:
            # ì¸ë±ìŠ¤ íŒŒì¼ ì½ê¸°
            index_file = source_path / "pytorch_model.bin.index.json"
            if index_file.exists():
                with open(index_file) as f:
                    index_data = json.load(f)

                merged_state_dict = {}
                weight_map = index_data["weight_map"]

                # ëª¨ë“  shard ë³‘í•©
                for shard_file in set(weight_map.values()):
                    shard_path = source_path / shard_file
                    if shard_path.exists():
                        shard_dict = torch.load(
                            shard_path, map_location="cpu", weights_only=True
                        )
                        merged_state_dict.update(shard_dict)

                save_file(merged_state_dict, target_path / "model.safetensors")
                console.print("âœ… ë¶„í•  ëª¨ë¸ â†’ safetensors ë³‘í•© ë³€í™˜ ì™„ë£Œ")
                return

        console.print("[yellow]âš ï¸ ì§€ì›ë˜ëŠ” ëª¨ë¸ ê°€ì¤‘ì¹˜ í˜•ì‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤[/yellow]")

    def _standardize_configs(self, source_path: Path, target_path: Path):
        """ì„¤ì • íŒŒì¼ë“¤ ë³µì‚¬ ë° í‘œì¤€í™”"""
        config_files = [
            "config.json",
            "tokenizer.json",
            "tokenizer_config.json",
            "special_tokens_map.json",
            "generation_config.json",
            "vocab.json",  # GPT-2 ê³„ì—´
            "merges.txt",  # GPT-2 ê³„ì—´
            "tokenizer.model",  # LLaMA ê³„ì—´
            "modeling.py",  # MTP ëª¨ë¸ìš© ì»¤ìŠ¤í…€ ëª¨ë“ˆ
        ]

        for file_name in config_files:
            source_file = source_path / file_name
            if source_file.exists():
                shutil.copy2(source_file, target_path / file_name)

        console.print("âœ… ì„¤ì • íŒŒì¼ í‘œì¤€í™” ì™„ë£Œ")

    def _create_wmtp_metadata(
        self, target_path: Path, wmtp_type: str, model_info: dict[str, Any]
    ):
        """WMTP ë©”íƒ€ë°ì´í„° ìƒì„± (ìƒˆë¡œìš´ í‘œì¤€ êµ¬ì¡°)"""

        # loading_strategy ê²°ì •
        loading_strategy = self._determine_loading_strategy(
            wmtp_type, model_info, target_path
        )

        # algorithm_compatibility ê²°ì •
        algorithm_compatibility = self._determine_algorithm_compatibility(
            wmtp_type, model_info
        )

        metadata = {
            "wmtp_type": wmtp_type,
            "training_algorithm": model_info.get("training_algorithm", "unknown"),
            "horizon": model_info.get("horizon", 4),
            "n_heads": model_info.get("n_heads", 4),
            "base_architecture": model_info.get("architecture", "unknown"),
            "model_size": model_info.get("size", "unknown"),
            "storage_version": "2.0",
            "created_by": "s3_model_pipeline",
            "standardization_date": datetime.now().isoformat(),
            "original_format": model_info.get("original_format", "unknown"),
            "source_path": model_info.get("source_path", "unknown"),
            "loading_strategy": loading_strategy,
            "algorithm_compatibility": algorithm_compatibility,
        }

        with open(target_path / "metadata.json", "w") as f:
            json.dump(metadata, f, indent=2)

        console.print("âœ… WMTP ë©”íƒ€ë°ì´í„° ìƒì„± ì™„ë£Œ")

    def _determine_loading_strategy(
        self, wmtp_type: str, model_info: dict[str, Any], target_path: Path
    ) -> dict[str, Any]:
        """ëª¨ë¸ íƒ€ì…ê³¼ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ loading_strategy ê²°ì •"""

        # MTP ëª¨ë¸ì¸ì§€ í™•ì¸
        is_mtp = (
            model_info.get("training_algorithm") == "mtp"
            or wmtp_type == "mtp_native"
            or (target_path / "modeling.py").exists()
        )

        if is_mtp:
            # MTP ëª¨ë¸ìš© ì»¤ìŠ¤í…€ ë¡œë”© ì „ëµ
            return {
                "loader_type": "custom_mtp",
                "model_class_name": "GPTMTPForCausalLM",
                "custom_module_file": "modeling.py",
                "transformers_class": None,
                "state_dict_mapping": {
                    "remove_prefix": "base_model." if wmtp_type == "mtp_native" else None,
                    "add_prefix": None,
                    "key_transforms": {},
                },
                "required_files": [
                    "config.json",
                    "model.safetensors",
                    "modeling.py",
                    "metadata.json",
                ],
            }
        else:
            # ì¼ë°˜ HuggingFace ëª¨ë¸ ë¡œë”© ì „ëµ
            transformers_class = "AutoModelForCausalLM"
            if wmtp_type == "reward_model":
                transformers_class = "AutoModel"

            return {
                "loader_type": "huggingface",
                "model_class_name": None,
                "custom_module_file": None,
                "transformers_class": transformers_class,
                "state_dict_mapping": {
                    "remove_prefix": None,
                    "add_prefix": None,
                    "key_transforms": {},
                },
                "required_files": ["config.json", "model.safetensors", "metadata.json"],
            }

    def _determine_algorithm_compatibility(
        self, wmtp_type: str, model_info: dict[str, Any]
    ) -> list[str]:
        """ëª¨ë¸ íƒ€ì…ê³¼ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ algorithm_compatibility ê²°ì •"""

        training_algo = model_info.get("training_algorithm", "")

        # MTP ëª¨ë¸ë“¤
        if training_algo == "mtp" or wmtp_type == "mtp_native":
            return ["baseline-mtp", "critic-wmtp", "rho1-wmtp"]

        # Reward ëª¨ë¸
        if wmtp_type == "reward_model" or training_algo == "reward_modeling":
            return ["critic-wmtp", "rho1-wmtp"]

        # Base/Reference ëª¨ë¸
        if wmtp_type in ["base_model", "reference_model"]:
            # Sheared LLaMA ê°™ì€ íŠ¹ìˆ˜ ëª¨ë¸
            if training_algo == "sheared_training":
                return ["baseline-mtp", "rho1-wmtp"]
            # ì¼ë°˜ base ëª¨ë¸
            return ["rho1-wmtp"]

        # ê¸°ë³¸ê°’
        return ["rho1-wmtp"]

    def _create_mlflow_files(
        self, target_path: Path, wmtp_type: str, model_info: dict[str, Any]
    ):
        """MLflow í˜¸í™˜ì„± íŒŒì¼ ìƒì„±"""
        mlflow_dir = target_path / "mlflow_model"
        mlflow_dir.mkdir(exist_ok=True)

        # ëª¨ë¸ í¬ê¸° ê³„ì‚° (ì‹¤ì œ safetensors íŒŒì¼ í¬ê¸°)
        model_file = target_path / "model.safetensors"
        model_size_bytes = model_file.stat().st_size if model_file.exists() else 0

        # MLmodel íŒŒì¼ (ê¸°ì¡´ tiny_modelsì™€ ë™ì¼í•œ êµ¬ì¡°)
        mlmodel_content = {
            "artifact_path": "model",
            "flavors": {
                "python_function": {
                    "env": {
                        "conda": "conda.yaml",
                        "virtualenv": "python_env.yaml",
                    },
                    "loader_module": "mlflow.transformers",
                    "model_path": "model",
                    "python_version": "3.11.7",
                },
                "transformers": {
                    "code": None,
                    "framework": "pt",
                    "model_path": "model",
                    "task": "text-generation",
                },
            },
            "mlflow_version": "2.15.1",
            "model_size_bytes": model_size_bytes,
            "model_uuid": "12345678901234567890123456789013",
            "run_id": None,
            "signature": {
                "inputs": '[{"type": "string"}]',
                "outputs": '[{"type": "string"}]',
            },
            "utc_time_created": datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f"),
        }

        with open(mlflow_dir / "MLmodel", "w") as f:
            yaml.dump(mlmodel_content, f, default_flow_style=False, sort_keys=False)

        # requirements.txt
        requirements = ["transformers", "torch"]

        with open(mlflow_dir / "requirements.txt", "w") as f:
            f.write("\n".join(requirements))

        # conda.yaml
        conda_env = {
            "name": f"wmtp_{wmtp_type}_env",
            "channels": ["defaults"],
            "dependencies": ["python=3.11.7", {"pip": requirements}],
        }

        with open(mlflow_dir / "conda.yaml", "w") as f:
            yaml.dump(conda_env, f, default_flow_style=False, sort_keys=False)

        console.print("âœ… MLflow í˜¸í™˜ì„± íŒŒì¼ ìƒì„± ì™„ë£Œ")

    def _upload_to_s3(self, local_path: Path, s3_path: str):
        """í‘œì¤€í™”ëœ ëª¨ë¸ì„ S3ì— ì—…ë¡œë“œ"""
        prefix = s3_path.replace(f"s3://{self.bucket}/", "").rstrip("/")

        # ì—…ë¡œë“œí•  íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘
        files_to_upload = []
        total_size = 0

        for file_path in local_path.rglob("*"):
            if file_path.is_file():
                size = file_path.stat().st_size
                files_to_upload.append((file_path, size))
                total_size += size

        console.print(
            f"ì´ {len(files_to_upload)}ê°œ íŒŒì¼, {total_size / (1024**3):.2f}GB ì—…ë¡œë“œ"
        )

        # ì§„í–‰ë¥  í‘œì‹œì™€ í•¨ê»˜ ì—…ë¡œë“œ
        with Progress(
            TextColumn("[bold blue]{task.description}"),
            BarColumn(),
            "[progress.percentage]{task.percentage:>3.1f}%",
            TransferSpeedColumn(),
            console=console,
        ) as progress:
            for file_path, size in files_to_upload:
                relative_path = file_path.relative_to(local_path)
                s3_key = f"{prefix}/{relative_path}".replace("\\", "/")
                file_name = file_path.name

                task_id = progress.add_task(f"ğŸ“¤ {file_name}", total=size)

                def progress_callback(bytes_transferred):
                    progress.update(task_id, advance=bytes_transferred)

                try:
                    self.s3_client.upload_file(
                        str(file_path),
                        self.bucket,
                        s3_key,
                        Config=self.transfer_config,
                        Callback=progress_callback,
                    )
                    progress.update(task_id, completed=size)
                except Exception as e:
                    console.print(f"[red]âŒ ì—…ë¡œë“œ ì‹¤íŒ¨ {file_name}: {e}[/red]")
                    raise

        console.print("[green]âœ… S3 ì—…ë¡œë“œ ì™„ë£Œ[/green]")


def test_sheared_llama_pipeline():
    """Sheared-LLaMA 2.7B ëª¨ë¸ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸"""
    console.print(
        "[bold blue]ğŸ§ª Sheared-LLaMA 2.7B íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸[/bold blue]\n"
    )

    # S3 íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    pipeline = S3ModelPipeline()

    # Sheared-LLaMA ëª¨ë¸ ì²˜ë¦¬
    model_info = {
        "architecture": "llama",
        "size": "2.7b",
        "training_algorithm": "sheared_training",
        "original_format": "sharded_pytorch_bins",
        "source_path": "s3://wmtp/models/Sheared-LLaMA-2.7B/",
    }

    success = pipeline.process_model(
        source_s3_path="s3://wmtp/models/Sheared-LLaMA-2.7B/",
        target_s3_path="s3://wmtp/models_v2/sheared-llama-2.7b-standard/",
        wmtp_type="base_model",
        model_info=model_info,
    )

    if success:
        console.print("\n[bold green]ğŸ‰ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì„±ê³µ![/bold green]")
        console.print("í‘œì¤€í™”ëœ ëª¨ë¸ì´ S3ì— ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        console.print("\n[red]âŒ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨[/red]")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
    load_dotenv()

    # ì‹¤í–‰í•  ëª¨ë¸ ì •ì˜
    models_to_process = [
        {
            "source": "s3://wmtp/models/Sheared-LLaMA-2.7B/",
            "target": "s3://wmtp/models_v2/sheared-llama-2.7b/",
            "wmtp_type": "base_model",
            "info": {
                "architecture": "llama",
                "size": "2.7b",
                "training_algorithm": "sheared_training",
                "original_format": "sharded_pytorch_bins",
            },
        },
        {
            "source": "s3://wmtp/models/7b_1t_4/",
            "target": "s3://wmtp/models_v2/llama-7b-mtp/",
            "wmtp_type": "mtp_native",
            "info": {
                "architecture": "llama",
                "size": "7b",
                "training_algorithm": "mtp",
                "horizon": 4,
                "n_heads": 4,
                "original_format": "consolidated_pth",
            },
        },
        {
            "source": "s3://wmtp/models/Starling-RM-7B-alpha/",
            "target": "s3://wmtp/models_v2/starling-rm-7b/",
            "wmtp_type": "reward_model",
            "info": {
                "architecture": "llama",
                "size": "7b",
                "training_algorithm": "reward_modeling",
                "original_format": "pytorch_bin",
            },
        },
    ]

    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    pipeline = S3ModelPipeline()

    console.print(
        f"[bold blue]ğŸš€ {len(models_to_process)}ê°œ ëª¨ë¸ ì²˜ë¦¬ ì‹œì‘[/bold blue]\n"
    )

    for i, model in enumerate(models_to_process, 1):
        console.print(f"\n[cyan]â”â”â” ëª¨ë¸ {i}/{len(models_to_process)} â”â”â”[/cyan]")

        success = pipeline.process_model(
            source_s3_path=model["source"],
            target_s3_path=model["target"],
            wmtp_type=model["wmtp_type"],
            model_info=model["info"],
        )

        if not success:
            console.print(f"[red]ëª¨ë¸ ì²˜ë¦¬ ì‹¤íŒ¨: {model['source']}[/red]")

    console.print("\n[bold green]ğŸ‰ ëª¨ë“  ëª¨ë¸ ì²˜ë¦¬ ì™„ë£Œ![/bold green]")


if __name__ == "__main__":
    import sys

    if len(sys.argv) > 1 and sys.argv[1] == "test":
        # í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì‹¤í–‰
        test_sheared_llama_pipeline()
    else:
        # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        main()