#!/usr/bin/env python3
"""
S3 ëª¨ë“  ëª¨ë¸ í‘œì¤€í™” - ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ìš©
Sheared-LLaMA-2.7B, 7b_1t_4, Starling-RM-7B-alpha ëª¨ë‘ ì²˜ë¦¬
"""

import json
import os
import sys
import time
from datetime import datetime
from pathlib import Path

import boto3
import yaml
from dotenv import load_dotenv

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()


class S3ModelStandardizer:
    """S3 ëª¨ë¸ í‘œì¤€í™” (ìµœì í™” ë²„ì „)"""

    def __init__(self):
        self.s3_client = boto3.client(
            "s3",
            aws_access_key_id=os.getenv("AWS_ACCESS_KEY_ID"),
            aws_secret_access_key=os.getenv("AWS_SECRET_ACCESS_KEY"),
            region_name="eu-north-1",
        )
        self.bucket = "wmtp"
        print(f"[{datetime.now()}] S3 Standardizer ì´ˆê¸°í™” ì™„ë£Œ", flush=True)

    def standardize_model(self, source_prefix: str, target_prefix: str, model_config: dict):
        """ë‹¨ì¼ ëª¨ë¸ í‘œì¤€í™”"""
        print(f"\n[{datetime.now()}] ========================================", flush=True)
        print(f"ëª¨ë¸ í‘œì¤€í™” ì‹œì‘: {model_config['name']}", flush=True)
        print(f"ì†ŒìŠ¤: s3://{self.bucket}/{source_prefix}", flush=True)
        print(f"ëŒ€ìƒ: s3://{self.bucket}/{target_prefix}", flush=True)

        try:
            # 1. S3 íŒŒì¼ ë³µì‚¬
            print(f"[{datetime.now()}] S3 íŒŒì¼ ë³µì‚¬ ì‹œì‘...", flush=True)
            self._copy_s3_files(source_prefix, target_prefix)

            # 2. ë©”íƒ€ë°ì´í„° ìƒì„± ë° ì—…ë¡œë“œ
            print(f"[{datetime.now()}] ë©”íƒ€ë°ì´í„° ìƒì„±...", flush=True)
            self._create_metadata(target_prefix, model_config)

            print(f"[{datetime.now()}] âœ… {model_config['name']} í‘œì¤€í™” ì™„ë£Œ!", flush=True)
            return True

        except Exception as e:
            print(f"[{datetime.now()}] âŒ ì˜¤ë¥˜ ë°œìƒ: {e}", flush=True)
            return False

    def _copy_s3_files(self, source_prefix: str, target_prefix: str):
        """S3 íŒŒì¼ ë³µì‚¬ (ë³‘ë ¬ ì²˜ë¦¬)"""
        paginator = self.s3_client.get_paginator("list_objects_v2")

        # íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        files_to_copy = []
        for page in paginator.paginate(Bucket=self.bucket, Prefix=source_prefix):
            if "Contents" not in page:
                continue
            for obj in page["Contents"]:
                if not obj["Key"].endswith("/"):
                    files_to_copy.append((obj["Key"], obj["Size"]))

        total_files = len(files_to_copy)
        total_size = sum(size for _, size in files_to_copy) / (1024**3)  # GB
        print(f"[{datetime.now()}] ì´ {total_files}ê°œ íŒŒì¼, {total_size:.2f}GB", flush=True)

        # íŒŒì¼ ë³µì‚¬
        for i, (source_key, size) in enumerate(files_to_copy, 1):
            file_name = source_key.split("/")[-1]
            target_key = source_key.replace(source_prefix, target_prefix)

            # ì§„í–‰ ìƒí™© ì¶œë ¥
            if i % 5 == 0 or size > 100 * 1024 * 1024:  # 5ê°œë§ˆë‹¤ ë˜ëŠ” 100MB ì´ìƒ
                print(f"[{datetime.now()}] [{i}/{total_files}] {file_name} ({size/(1024**2):.1f}MB) ë³µì‚¬ ì¤‘...", flush=True)

            try:
                # S3 ì„œë²„ì‚¬ì´ë“œ ë³µì‚¬
                copy_source = {"Bucket": self.bucket, "Key": source_key}

                if size > 5 * 1024**3:  # 5GB ì´ìƒ
                    # ë©€í‹°íŒŒíŠ¸ ë³µì‚¬ (ëŒ€ìš©ëŸ‰)
                    self._multipart_copy(copy_source, target_key, size)
                else:
                    # ì¼ë°˜ ë³µì‚¬
                    self.s3_client.copy_object(
                        CopySource=copy_source,
                        Bucket=self.bucket,
                        Key=target_key,
                        MetadataDirective='COPY',
                        TaggingDirective='COPY'
                    )

            except Exception as e:
                print(f"[{datetime.now()}] âš ï¸ {file_name} ë³µì‚¬ ì‹¤íŒ¨: {e}", flush=True)
                # ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰

        print(f"[{datetime.now()}] S3 íŒŒì¼ ë³µì‚¬ ì™„ë£Œ", flush=True)

    def _multipart_copy(self, copy_source: dict, target_key: str, size: int):
        """ë©€í‹°íŒŒíŠ¸ ë³µì‚¬ (5GB ì´ìƒ)"""
        # ë©€í‹°íŒŒíŠ¸ ì—…ë¡œë“œ ì‹œì‘
        mpu = self.s3_client.create_multipart_upload(Bucket=self.bucket, Key=target_key)
        mpu_id = mpu["UploadId"]

        # 1GB ì²­í¬
        chunk_size = 1024 * 1024 * 1024
        parts = []

        try:
            for i, offset in enumerate(range(0, size, chunk_size), 1):
                end_byte = min(offset + chunk_size - 1, size - 1)

                part = self.s3_client.upload_part_copy(
                    Bucket=self.bucket,
                    Key=target_key,
                    CopySource=copy_source,
                    CopySourceRange=f"bytes={offset}-{end_byte}",
                    PartNumber=i,
                    UploadId=mpu_id,
                )

                parts.append({
                    "ETag": part["CopyPartResult"]["ETag"],
                    "PartNumber": i
                })

            # ì™„ë£Œ
            self.s3_client.complete_multipart_upload(
                Bucket=self.bucket,
                Key=target_key,
                UploadId=mpu_id,
                MultipartUpload={"Parts": parts},
            )

        except Exception as e:
            self.s3_client.abort_multipart_upload(
                Bucket=self.bucket, Key=target_key, UploadId=mpu_id
            )
            raise e

    def _create_metadata(self, target_prefix: str, model_config: dict):
        """ë©”íƒ€ë°ì´í„° ìƒì„± ë° ì—…ë¡œë“œ"""

        # metadata.json
        metadata = {
            "wmtp_type": model_config["wmtp_type"],
            "training_algorithm": model_config["training_algorithm"],
            "horizon": model_config.get("horizon", 4),
            "n_heads": model_config.get("n_heads", 4),
            "base_architecture": model_config["architecture"],
            "model_size": model_config["size"],
            "storage_version": "2.0",
            "created_by": "s3_standardize_all",
            "standardization_date": datetime.now().isoformat(),
            "original_format": model_config["original_format"],
            "source_path": f"s3://{self.bucket}/{target_prefix}",
            "loading_strategy": self._get_loading_strategy(model_config),
            "algorithm_compatibility": self._get_algorithm_compatibility(model_config),
        }

        # JSONì„ ë¬¸ìì—´ë¡œ ë³€í™˜
        metadata_json = json.dumps(metadata, indent=2)

        # S3ì— ì§ì ‘ ì—…ë¡œë“œ
        self.s3_client.put_object(
            Bucket=self.bucket,
            Key=f"{target_prefix}/metadata.json",
            Body=metadata_json,
            ContentType="application/json"
        )

        # MLflow íŒŒì¼ë“¤
        self._create_mlflow_files(target_prefix, model_config)

        print(f"[{datetime.now()}] ë©”íƒ€ë°ì´í„° ì—…ë¡œë“œ ì™„ë£Œ", flush=True)

    def _get_loading_strategy(self, config: dict) -> dict:
        """ë¡œë”© ì „ëµ ê²°ì •"""
        if config["training_algorithm"] == "mtp" or config["wmtp_type"] == "mtp_native":
            return {
                "loader_type": "custom_mtp",
                "model_class_name": "GPTMTPForCausalLM",
                "custom_module_file": "modeling.py",
                "transformers_class": None,
                "state_dict_mapping": {
                    "remove_prefix": "base_model." if config["wmtp_type"] == "mtp_native" else None,
                    "add_prefix": None,
                    "key_transforms": {}
                },
                "required_files": ["config.json", "model.safetensors", "modeling.py", "metadata.json"]
            }
        else:
            transformers_class = "AutoModel" if config["wmtp_type"] == "reward_model" else "AutoModelForCausalLM"
            return {
                "loader_type": "huggingface",
                "model_class_name": None,
                "custom_module_file": None,
                "transformers_class": transformers_class,
                "state_dict_mapping": {
                    "remove_prefix": None,
                    "add_prefix": None,
                    "key_transforms": {}
                },
                "required_files": ["config.json", "metadata.json"]
            }

    def _get_algorithm_compatibility(self, config: dict) -> list:
        """ì•Œê³ ë¦¬ì¦˜ í˜¸í™˜ì„±"""
        if config["training_algorithm"] == "mtp" or config["wmtp_type"] == "mtp_native":
            return ["baseline-mtp", "critic-wmtp", "rho1-wmtp"]
        elif config["wmtp_type"] == "reward_model":
            return ["critic-wmtp", "rho1-wmtp"]
        elif config["training_algorithm"] == "sheared_training":
            return ["baseline-mtp", "rho1-wmtp"]
        else:
            return ["rho1-wmtp"]

    def _create_mlflow_files(self, target_prefix: str, config: dict):
        """MLflow íŒŒì¼ ìƒì„±"""

        mlmodel = {
            "artifact_path": "model",
            "flavors": {
                "python_function": {
                    "env": {"conda": "conda.yaml", "virtualenv": "python_env.yaml"},
                    "loader_module": "mlflow.transformers",
                    "model_path": "model",
                    "python_version": "3.11.7"
                },
                "transformers": {
                    "code": None,
                    "framework": "pt",
                    "model_path": "model",
                    "task": "text-generation"
                }
            },
            "mlflow_version": "2.15.1",
            "model_uuid": f"{config['name']}",
            "signature": {
                "inputs": '[{"type": "string"}]',
                "outputs": '[{"type": "string"}]'
            },
            "utc_time_created": datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f")
        }

        # YAML ë¬¸ìì—´ë¡œ ë³€í™˜
        mlmodel_yaml = yaml.dump(mlmodel, default_flow_style=False, sort_keys=False)

        # S3 ì—…ë¡œë“œ
        self.s3_client.put_object(
            Bucket=self.bucket,
            Key=f"{target_prefix}/mlflow_model/MLmodel",
            Body=mlmodel_yaml,
            ContentType="text/plain"
        )

        # requirements.txt
        self.s3_client.put_object(
            Bucket=self.bucket,
            Key=f"{target_prefix}/mlflow_model/requirements.txt",
            Body="transformers\ntorch",
            ContentType="text/plain"
        )

        # conda.yaml
        conda_env = {
            "name": f"wmtp_{config['name']}_env",
            "channels": ["defaults"],
            "dependencies": ["python=3.11.7", {"pip": ["transformers", "torch"]}]
        }

        conda_yaml = yaml.dump(conda_env, default_flow_style=False, sort_keys=False)

        self.s3_client.put_object(
            Bucket=self.bucket,
            Key=f"{target_prefix}/mlflow_model/conda.yaml",
            Body=conda_yaml,
            ContentType="text/plain"
        )


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ - ëª¨ë“  ëª¨ë¸ ì²˜ë¦¬"""

    print(f"[{datetime.now()}] =====================================", flush=True)
    print(f"S3 ëª¨ë¸ í‘œì¤€í™” ì‹œì‘ (3ê°œ ëª¨ë¸)", flush=True)
    print(f"=====================================", flush=True)

    standardizer = S3ModelStandardizer()

    # ì²˜ë¦¬í•  ëª¨ë¸ ì •ì˜
    models = [
        {
            "name": "sheared-llama-2.7b",
            "source": "models/Sheared-LLaMA-2.7B",
            "target": "models_v2/sheared-llama-2.7b",
            "wmtp_type": "base_model",
            "architecture": "llama",
            "size": "2.7b",
            "training_algorithm": "sheared_training",
            "original_format": "sharded_pytorch_bins",
        },
        {
            "name": "llama-7b-mtp",
            "source": "models/7b_1t_4",
            "target": "models_v2/llama-7b-mtp",
            "wmtp_type": "mtp_native",
            "architecture": "llama",
            "size": "7b",
            "training_algorithm": "mtp",
            "horizon": 4,
            "n_heads": 4,
            "original_format": "consolidated_pth",
        },
        {
            "name": "starling-rm-7b",
            "source": "models/Starling-RM-7B-alpha",
            "target": "models_v2/starling-rm-7b",
            "wmtp_type": "reward_model",
            "architecture": "llama",
            "size": "7b",
            "training_algorithm": "reward_modeling",
            "original_format": "pytorch_bin",
        },
    ]

    # ê° ëª¨ë¸ ì²˜ë¦¬
    success_count = 0
    for i, model in enumerate(models, 1):
        print(f"\n[{datetime.now()}] ëª¨ë¸ {i}/{len(models)}: {model['name']}", flush=True)

        success = standardizer.standardize_model(
            source_prefix=model["source"],
            target_prefix=model["target"],
            model_config=model
        )

        if success:
            success_count += 1
            print(f"[{datetime.now()}] âœ… {model['name']} ì™„ë£Œ", flush=True)
        else:
            print(f"[{datetime.now()}] âŒ {model['name']} ì‹¤íŒ¨", flush=True)

    # ìµœì¢… ê²°ê³¼
    print(f"\n[{datetime.now()}] =====================================", flush=True)
    print(f"ìµœì¢… ê²°ê³¼: {success_count}/{len(models)} ëª¨ë¸ í‘œì¤€í™” ì™„ë£Œ", flush=True)
    print(f"=====================================", flush=True)

    if success_count == len(models):
        print("ğŸ‰ ëª¨ë“  ëª¨ë¸ í‘œì¤€í™” ì„±ê³µ!", flush=True)
        sys.exit(0)
    else:
        print("âš ï¸ ì¼ë¶€ ëª¨ë¸ í‘œì¤€í™” ì‹¤íŒ¨", flush=True)
        sys.exit(1)


if __name__ == "__main__":
    main()