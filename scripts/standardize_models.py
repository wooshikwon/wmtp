#!/usr/bin/env python3
"""
WMTP Model Storage Standardization Tool

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë‹¤ì–‘í•œ í˜•ì‹ì˜ ëª¨ë¸ë“¤ì„ WMTP í‘œì¤€ ì €ì¥ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
HuggingFace + MLflow í•˜ì´ë¸Œë¦¬ë“œ êµ¬ì¡°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤.

í‘œì¤€ êµ¬ì¡°:
model_name/
â”œâ”€â”€ model.safetensors           # HF í‘œì¤€ (ë³´ì•ˆ)
â”œâ”€â”€ config.json                # HF í‘œì¤€ + WMTP í™•ì¥
â”œâ”€â”€ tokenizer.json             # HF í‘œì¤€
â”œâ”€â”€ tokenizer_config.json      # HF í‘œì¤€
â”œâ”€â”€ special_tokens_map.json    # HF í‘œì¤€
â”œâ”€â”€ wmtp_metadata.json         # WMTP ì „ìš© ë©”íƒ€ë°ì´í„°
â””â”€â”€ mlflow_model/              # MLflow í˜¸í™˜ì„±
    â”œâ”€â”€ MLmodel               # MLflow ë©”íƒ€ë°ì´í„°
    â”œâ”€â”€ requirements.txt      # ì˜ì¡´ì„±
    â””â”€â”€ conda.yaml           # í™˜ê²½
"""

import json
import shutil
import tempfile
from datetime import datetime
from pathlib import Path
from typing import Any

import boto3
import torch
from rich.console import Console
from rich.progress import Progress
from transformers import (
    AutoModelForCausalLM,
)

console = Console()


class WMTPModelStandardizer:
    """WMTP ëª¨ë¸ í‘œì¤€í™” ë„êµ¬"""

    def __init__(
        self, aws_access_key: str, aws_secret_key: str, region: str = "eu-north-1"
    ):
        self.s3_client = boto3.client(
            "s3",
            aws_access_key_id=aws_access_key,
            aws_secret_access_key=aws_secret_key,
            region_name=region,
        )
        self.bucket = "wmtp"

    def standardize_s3_model(
        self,
        source_path: str,
        target_path: str,
        wmtp_type: str,
        model_info: dict[str, Any],
    ):
        """S3 ëª¨ë¸ì„ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        console.print(
            f"[blue]Standardizing S3 model: {source_path} â†’ {target_path}[/blue]"
        )

        with tempfile.TemporaryDirectory() as temp_dir:
            temp_path = Path(temp_dir)

            # 1. ì›ë³¸ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
            self._download_s3_model(source_path, temp_path / "original")

            # 2. í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            standardized_path = temp_path / "standardized"
            self._convert_to_standard_format(
                temp_path / "original", standardized_path, wmtp_type, model_info
            )

            # 3. S3ì— ì—…ë¡œë“œ
            self._upload_standardized_model(standardized_path, target_path)

        console.print(f"[green]âœ… S3 model standardized: {target_path}[/green]")

    def standardize_local_model(
        self,
        source_path: Path,
        target_path: Path,
        wmtp_type: str,
        model_info: dict[str, Any],
    ):
        """ë¡œì»¬ ëª¨ë¸ì„ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        console.print(
            f"[blue]Standardizing local model: {source_path} â†’ {target_path}[/blue]"
        )

        target_path.mkdir(parents=True, exist_ok=True)

        self._convert_to_standard_format(
            source_path, target_path, wmtp_type, model_info
        )

        console.print(f"[green]âœ… Local model standardized: {target_path}[/green]")

    def _download_s3_model(self, s3_path: str, local_path: Path):
        """S3ì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"""
        local_path.mkdir(parents=True, exist_ok=True)

        # S3 ê°ì²´ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        prefix = s3_path.replace("s3://wmtp/", "")

        with Progress() as progress:
            task = progress.add_task("Downloading from S3...", total=None)

            paginator = self.s3_client.get_paginator("list_objects_v2")
            for page in paginator.paginate(Bucket=self.bucket, Prefix=prefix):
                if "Contents" not in page:
                    continue

                for obj in page["Contents"]:
                    key = obj["Key"]
                    local_file = local_path / key.replace(prefix, "").lstrip("/")
                    local_file.parent.mkdir(parents=True, exist_ok=True)

                    self.s3_client.download_file(self.bucket, key, str(local_file))

            progress.update(task, completed=True)

    def _convert_to_standard_format(
        self,
        source_path: Path,
        target_path: Path,
        wmtp_type: str,
        model_info: dict[str, Any],
    ):
        """ëª¨ë¸ì„ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        target_path.mkdir(parents=True, exist_ok=True)

        # 1. ëª¨ë¸ ê°€ì¤‘ì¹˜ ë³€í™˜ (safetensors)
        self._convert_model_weights(source_path, target_path, wmtp_type)

        # 2. ì„¤ì • íŒŒì¼ í‘œì¤€í™”
        self._standardize_config_files(source_path, target_path)

        # 3. WMTP ë©”íƒ€ë°ì´í„° ìƒì„±
        self._generate_wmtp_metadata(target_path, wmtp_type, model_info)

        # 4. MLflow í˜¸í™˜ì„± íŒŒì¼ ìƒì„±
        self._generate_mlflow_files(target_path, wmtp_type, model_info)

    def _convert_model_weights(
        self, source_path: Path, target_path: Path, wmtp_type: str
    ):
        """ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ safetensorsë¡œ ë³€í™˜"""
        console.print("[yellow]Converting model weights to safetensors...[/yellow]")

        if wmtp_type == "mtp_native":
            # Meta consolidated.pth ë³€í™˜
            consolidated_file = source_path / "consolidated.pth"
            if consolidated_file.exists():
                self._convert_consolidated_pth(consolidated_file, target_path)
            else:
                raise FileNotFoundError(f"consolidated.pth not found in {source_path}")

        elif (source_path / "pytorch_model.bin").exists():
            # HuggingFace pytorch_model.bin ë³€í™˜
            self._convert_pytorch_bin(source_path, target_path)

        elif any(source_path.glob("pytorch_model-*.bin")):
            # ë¶„í• ëœ HuggingFace ëª¨ë¸ ë³€í™˜
            self._convert_sharded_pytorch_bins(source_path, target_path)

        elif (source_path / "model.safetensors").exists():
            # ì´ë¯¸ safetensors í˜•ì‹
            shutil.copy2(
                source_path / "model.safetensors", target_path / "model.safetensors"
            )

        else:
            raise ValueError(f"No supported model weights found in {source_path}")

    def _convert_consolidated_pth(self, pth_file: Path, target_path: Path):
        """Meta consolidated.pthë¥¼ safetensorsë¡œ ë³€í™˜"""
        console.print("[yellow]Converting consolidated.pth to safetensors...[/yellow]")

        # PyTorch íŒŒì¼ ë¡œë“œ
        checkpoint = torch.load(pth_file, map_location="cpu", weights_only=True)

        # safetensorsë¡œ ì €ì¥
        from safetensors.torch import save_file

        save_file(checkpoint, target_path / "model.safetensors")

        console.print("[green]âœ… Converted consolidated.pth to safetensors[/green]")

    def _convert_pytorch_bin(self, source_path: Path, target_path: Path):
        """ë‹¨ì¼ pytorch_model.binì„ safetensorsë¡œ ë³€í™˜"""
        console.print("[yellow]Converting pytorch_model.bin to safetensors...[/yellow]")

        try:
            # HuggingFace ëª¨ë¸ë¡œ ë¡œë“œ í›„ safetensorsë¡œ ì €ì¥
            model = AutoModelForCausalLM.from_pretrained(
                source_path, torch_dtype=torch.float32, trust_remote_code=True
            )
            model.save_pretrained(target_path, safe_serialization=True)

            # pytorch_model.binì€ ì œê±° (safetensorsë§Œ ìœ ì§€)
            bin_file = target_path / "pytorch_model.bin"
            if bin_file.exists():
                bin_file.unlink()

        except Exception as e:
            console.print(f"[red]Failed to convert via HuggingFace: {e}[/red]")
            console.print("[yellow]Attempting direct conversion...[/yellow]")

            # ì§ì ‘ ë³€í™˜
            state_dict = torch.load(
                source_path / "pytorch_model.bin", map_location="cpu", weights_only=True
            )
            from safetensors.torch import save_file

            save_file(state_dict, target_path / "model.safetensors")

        console.print("[green]âœ… Converted pytorch_model.bin to safetensors[/green]")

    def _convert_sharded_pytorch_bins(self, source_path: Path, target_path: Path):
        """ë¶„í• ëœ pytorch_model-*.binì„ ë‹¨ì¼ safetensorsë¡œ ë³‘í•©"""
        console.print("[yellow]Merging sharded pytorch models...[/yellow]")

        # ì¸ë±ìŠ¤ íŒŒì¼ ì½ê¸°
        index_file = source_path / "pytorch_model.bin.index.json"
        if not index_file.exists():
            raise FileNotFoundError(f"Index file not found: {index_file}")

        with open(index_file) as f:
            index_data = json.load(f)

        # ëª¨ë“  shard ë¡œë“œí•˜ì—¬ ë³‘í•©
        merged_state_dict = {}
        weight_map = index_data["weight_map"]

        for shard_file in set(weight_map.values()):
            shard_path = source_path / shard_file
            shard_state_dict = torch.load(
                shard_path, map_location="cpu", weights_only=True
            )
            merged_state_dict.update(shard_state_dict)

        # ë‹¨ì¼ safetensors íŒŒì¼ë¡œ ì €ì¥
        from safetensors.torch import save_file

        save_file(merged_state_dict, target_path / "model.safetensors")

        console.print("[green]âœ… Merged sharded models into single safetensors[/green]")

    def _standardize_config_files(self, source_path: Path, target_path: Path):
        """ì„¤ì • íŒŒì¼ë“¤ì„ í‘œì¤€í™”"""
        console.print("[yellow]Standardizing config files...[/yellow]")

        # í•„ìˆ˜ HuggingFace íŒŒì¼ë“¤ ë³µì‚¬
        required_files = [
            "config.json",
            "tokenizer.json",
            "tokenizer_config.json",
            "special_tokens_map.json",
            "generation_config.json",  # ì„ íƒì 
            "vocab.json",  # GPT-2 ê³„ì—´
            "merges.txt",  # GPT-2 ê³„ì—´
            "tokenizer.model",  # LLaMA ê³„ì—´
        ]

        for file_name in required_files:
            source_file = source_path / file_name
            if source_file.exists():
                shutil.copy2(source_file, target_path / file_name)

        # config.jsonì— WMTP ì •ë³´ê°€ ìˆë‹¤ë©´ ë³´ì¡´
        config_file = target_path / "config.json"
        if config_file.exists():
            with open(config_file) as f:
                config = json.load(f)

            # WMTP ê´€ë ¨ ì„¤ì • ë³´ì¡´
            if "mtp_config" in config:
                console.print("[green]âœ… Preserved MTP config in config.json[/green]")

        console.print("[green]âœ… Config files standardized[/green]")

    def _generate_wmtp_metadata(
        self, target_path: Path, wmtp_type: str, model_info: dict[str, Any]
    ):
        """WMTP ë©”íƒ€ë°ì´í„° íŒŒì¼ ìƒì„±"""
        console.print("[yellow]Generating WMTP metadata...[/yellow]")

        metadata = {
            "wmtp_type": wmtp_type,
            "training_algorithm": model_info.get("training_algorithm", "unknown"),
            "horizon": model_info.get("horizon", 4),
            "n_heads": model_info.get("n_heads", 4),
            "base_architecture": model_info.get("architecture", "unknown"),
            "model_size": model_info.get("size", "unknown"),
            "storage_version": "2.0",
            "created_by": "wmtp_standardizer",
            "standardization_date": datetime.now().isoformat(),
            "original_format": model_info.get("original_format", "unknown"),
            "conversion_metadata": {
                "converted_from": model_info.get("source_path", "unknown"),
                "conversion_date": datetime.now().isoformat(),
                "converter_version": "wmtp-v2.0",
            },
        }

        with open(target_path / "wmtp_metadata.json", "w") as f:
            json.dump(metadata, f, indent=2)

        console.print("[green]âœ… WMTP metadata generated[/green]")

    def _generate_mlflow_files(
        self, target_path: Path, wmtp_type: str, model_info: dict[str, Any]
    ):
        """MLflow í˜¸í™˜ì„± íŒŒì¼ë“¤ ìƒì„±"""
        console.print("[yellow]Generating MLflow compatibility files...[/yellow]")

        mlflow_dir = target_path / "mlflow_model"
        mlflow_dir.mkdir(exist_ok=True)

        # MLmodel íŒŒì¼
        mlmodel_content = {
            "artifact_path": "model",
            "flavors": {
                "python_function": {
                    "env": "conda.yaml",
                    "loader_module": "mlflow.pytorch",
                    "model_data": "../model.safetensors",
                    "python_version": "3.12",
                },
                "pytorch": {
                    "model_data": "../model.safetensors",
                    "pytorch_version": "2.4.1",
                },
            },
            "model_uuid": f"wmtp_{wmtp_type}_{model_info.get('size', 'unknown')}",
            "mlflow_version": "2.15.1",
            "signature": {
                "inputs": '[{"name": "input_ids", "type": "tensor", "tensor-spec": {"dtype": "int64", "shape": [-1, -1]}}]',
                "outputs": '[{"type": "tensor", "tensor-spec": {"dtype": "float32", "shape": [-1, -1, -1]}}]',
            },
        }

        with open(mlflow_dir / "MLmodel", "w") as f:
            import yaml

            yaml.dump(mlmodel_content, f, default_flow_style=False)

        # requirements.txt
        requirements = [
            "torch>=2.0.0",
            "transformers>=4.40.0",
            "safetensors>=0.4.0",
            "accelerate>=0.20.0",
        ]

        with open(mlflow_dir / "requirements.txt", "w") as f:
            f.write("\n".join(requirements))

        # conda.yaml
        conda_env = {
            "channels": ["conda-forge", "pytorch"],
            "dependencies": ["python=3.12", "pytorch>=2.0.0", {"pip": requirements}],
            "name": f"wmtp_{wmtp_type}_env",
        }

        with open(mlflow_dir / "conda.yaml", "w") as f:
            import yaml

            yaml.dump(conda_env, f, default_flow_style=False)

        console.print("[green]âœ… MLflow compatibility files generated[/green]")

    def _upload_standardized_model(self, local_path: Path, s3_path: str):
        """í‘œì¤€í™”ëœ ëª¨ë¸ì„ S3ì— ì—…ë¡œë“œ"""
        console.print(f"[yellow]Uploading standardized model to {s3_path}...[/yellow]")

        prefix = s3_path.replace("s3://wmtp/", "")

        with Progress() as progress:
            task = progress.add_task("Uploading to S3...", total=None)

            for file_path in local_path.rglob("*"):
                if file_path.is_file():
                    relative_path = file_path.relative_to(local_path)
                    s3_key = f"{prefix}/{relative_path}".replace("\\", "/")

                    self.s3_client.upload_file(str(file_path), self.bucket, s3_key)

            progress.update(task, completed=True)


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    console.print("[bold blue]WMTP Model Storage Standardization[/bold blue]")

    # AWS ì¸ì¦ ì •ë³´ - í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ
    import os

    from dotenv import load_dotenv

    load_dotenv()

    aws_access_key = os.getenv("AWS_ACCESS_KEY_ID")
    aws_secret_key = os.getenv("AWS_SECRET_ACCESS_KEY")

    standardizer = WMTPModelStandardizer(aws_access_key, aws_secret_key)

    # S3 ëª¨ë¸ í‘œì¤€í™” ì •ì˜
    s3_models = [
        {
            "source": "s3://wmtp/models/7b_1t_4/",
            "target": "s3://wmtp/models_v2/llama-7b-mtp/",
            "wmtp_type": "mtp_native",
            "info": {
                "architecture": "llama",
                "size": "7b",
                "training_algorithm": "mtp",
                "horizon": 4,
                "n_heads": 4,
                "original_format": "consolidated_pth",
                "source_path": "s3://wmtp/models/7b_1t_4/",
            },
        },
        {
            "source": "s3://wmtp/models/Starling-RM-7B-alpha/",
            "target": "s3://wmtp/models_v2/starling-rm-7b/",
            "wmtp_type": "reward_model",
            "info": {
                "architecture": "llama",
                "size": "7b",
                "training_algorithm": "reward_modeling",
                "original_format": "pytorch_bin",
                "source_path": "s3://wmtp/models/Starling-RM-7B-alpha/",
            },
        },
        {
            "source": "s3://wmtp/models/Sheared-LLaMA-2.7B/",
            "target": "s3://wmtp/models_v2/sheared-llama-2.7b/",
            "wmtp_type": "base_model",
            "info": {
                "architecture": "llama",
                "size": "2.7b",
                "training_algorithm": "sheared_training",
                "original_format": "sharded_pytorch_bins",
                "source_path": "s3://wmtp/models/Sheared-LLaMA-2.7B/",
            },
        },
    ]

    # S3 ëª¨ë¸ë“¤ í‘œì¤€í™”
    for model in s3_models:
        try:
            standardizer.standardize_s3_model(
                model["source"], model["target"], model["wmtp_type"], model["info"]
            )
        except Exception as e:
            console.print(f"[red]âŒ Failed to standardize {model['source']}: {e}[/red]")

    # ë¡œì»¬ í…ŒìŠ¤íŠ¸ ëª¨ë¸ë“¤ í‘œì¤€í™”
    local_models = [
        {
            "source": Path("tests/tiny_models/distilgpt2-mtp"),
            "target": Path("tests/tiny_models_v2/distilgpt2-mtp"),
            "wmtp_type": "mtp_huggingface",
            "info": {
                "architecture": "gpt2",
                "size": "120m",
                "training_algorithm": "mtp",
                "horizon": 4,
                "n_heads": 4,
                "original_format": "huggingface_safetensors",
            },
        },
        {
            "source": Path("tests/tiny_models/tiny-reward-model"),
            "target": Path("tests/tiny_models_v2/tiny-reward-model"),
            "wmtp_type": "reward_model",
            "info": {
                "architecture": "gpt2",
                "size": "120m",
                "training_algorithm": "reward_modeling",
                "original_format": "huggingface_safetensors",
            },
        },
        {
            "source": Path("tests/tiny_models/distilgpt2"),
            "target": Path("tests/tiny_models_v2/distilgpt2"),
            "wmtp_type": "base_model",
            "info": {
                "architecture": "gpt2",
                "size": "120m",
                "training_algorithm": "base_training",
                "original_format": "huggingface_safetensors",
            },
        },
    ]

    # ë¡œì»¬ ëª¨ë¸ë“¤ í‘œì¤€í™”
    for model in local_models:
        try:
            standardizer.standardize_local_model(
                model["source"], model["target"], model["wmtp_type"], model["info"]
            )
        except Exception as e:
            console.print(f"[red]âŒ Failed to standardize {model['source']}: {e}[/red]")

    console.print("[bold green]ğŸ‰ Model standardization completed![/bold green]")


if __name__ == "__main__":
    main()
