#!/usr/bin/env python3
"""
S3 In-Place í‘œì¤€í™” - ë‹¤ìš´ë¡œë“œ ì—†ì´ S3ì—ì„œ ì§ì ‘ ì²˜ë¦¬

ì£¼ìš” íŠ¹ì§•:
1. ëŒ€ìš©ëŸ‰ íŒŒì¼ì€ S3ì—ì„œ S3ë¡œ ì§ì ‘ ë³µì‚¬ (ë‹¤ìš´ë¡œë“œ ì—†ìŒ)
2. ë©”íƒ€ë°ì´í„°ì™€ ì‘ì€ ì„¤ì • íŒŒì¼ë§Œ ë¡œì»¬ì—ì„œ ìƒì„± í›„ ì—…ë¡œë“œ
3. 10GB ëª¨ë¸ë„ ëª‡ ë¶„ ë‚´ì— ì²˜ë¦¬ ê°€ëŠ¥
"""

import json
import os
import tempfile
from datetime import datetime
from pathlib import Path
from typing import Any, Optional

import boto3
import yaml
from botocore.exceptions import ClientError
from dotenv import load_dotenv
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn

console = Console()


class S3InPlaceStandardizer:
    """S3ì—ì„œ ì§ì ‘ ëª¨ë¸ í‘œì¤€í™” (ë‹¤ìš´ë¡œë“œ ìµœì†Œí™”)"""

    def __init__(
        self,
        aws_access_key: Optional[str] = None,
        aws_secret_key: Optional[str] = None,
        region: str = "eu-north-1",
    ):
        # í™˜ê²½ë³€ìˆ˜ì—ì„œ AWS ì¸ì¦ ì •ë³´ ë¡œë“œ
        if not aws_access_key:
            aws_access_key = os.getenv("AWS_ACCESS_KEY_ID")
        if not aws_secret_key:
            aws_secret_key = os.getenv("AWS_SECRET_ACCESS_KEY")

        if not aws_access_key or not aws_secret_key:
            raise ValueError("AWS credentials not found")

        self.s3_client = boto3.client(
            "s3",
            aws_access_key_id=aws_access_key,
            aws_secret_access_key=aws_secret_key,
            region_name=region,
        )
        self.bucket = "wmtp"

        console.print("[green]âœ… S3 In-Place Standardizer ì´ˆê¸°í™”[/green]")

    def standardize_model(
        self,
        source_prefix: str,
        target_prefix: str,
        wmtp_type: str = "base_model",
        model_info: Optional[dict[str, Any]] = None,
    ) -> bool:
        """
        S3ì—ì„œ ì§ì ‘ ëª¨ë¸ í‘œì¤€í™”
        1. ëŒ€ìš©ëŸ‰ íŒŒì¼ì€ S3 ë‚´ë¶€ ë³µì‚¬
        2. ë©”íƒ€ë°ì´í„°ë§Œ ìƒì„±í•˜ì—¬ ì—…ë¡œë“œ
        """
        console.print(f"\n[bold blue]ğŸš€ S3 In-Place í‘œì¤€í™” ì‹œì‘[/bold blue]")
        console.print(f"ì†ŒìŠ¤: s3://{self.bucket}/{source_prefix}")
        console.print(f"ëŒ€ìƒ: s3://{self.bucket}/{target_prefix}")

        if model_info is None:
            model_info = {}

        try:
            # 1. S3 íŒŒì¼ ëª©ë¡ ì¡°íšŒ
            console.print("\n[yellow]ğŸ“‹ Step 1/3: íŒŒì¼ ëª©ë¡ ì¡°íšŒ[/yellow]")
            files = self._list_s3_files(source_prefix)

            if not files:
                console.print("[red]âŒ ì†ŒìŠ¤ì— íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤[/red]")
                return False

            # íŒŒì¼ ë¶„ë¥˜
            large_files = []  # S3 ë³µì‚¬ ëŒ€ìƒ
            small_files = []  # ë‹¤ìš´ë¡œë“œ í•„ìš” (config ë“±)
            total_size = 0

            for file_key, size in files:
                total_size += size
                file_name = file_key.split("/")[-1]

                # 1MB ì´í•˜ íŒŒì¼ë§Œ ë‹¤ìš´ë¡œë“œ (config, tokenizer ë“±)
                if size <= 1024 * 1024 and file_name in [
                    "config.json",
                    "tokenizer.json",
                    "tokenizer_config.json",
                    "special_tokens_map.json",
                    "generation_config.json",
                    "vocab.json",
                    "merges.txt",
                    "tokenizer.model",
                ]:
                    small_files.append((file_key, size))
                else:
                    large_files.append((file_key, size))

            console.print(
                f"ì´ {len(files)}ê°œ íŒŒì¼, {total_size / (1024**3):.2f}GB"
            )
            console.print(f"  - S3 ë³µì‚¬: {len(large_files)}ê°œ ëŒ€ìš©ëŸ‰ íŒŒì¼")
            console.print(f"  - ë‹¤ìš´ë¡œë“œ: {len(small_files)}ê°œ ì„¤ì • íŒŒì¼")

            # 2. ëŒ€ìš©ëŸ‰ íŒŒì¼ S3 ë‚´ë¶€ ë³µì‚¬
            console.print("\n[yellow]âš¡ Step 2/3: S3 ë‚´ë¶€ ê³ ì† ë³µì‚¬[/yellow]")
            self._copy_large_files_s3_to_s3(
                large_files, source_prefix, target_prefix
            )

            # 3. ì„¤ì • íŒŒì¼ ì²˜ë¦¬ ë° ë©”íƒ€ë°ì´í„° ìƒì„±
            console.print("\n[yellow]ğŸ“ Step 3/3: ë©”íƒ€ë°ì´í„° ìƒì„±[/yellow]")
            with tempfile.TemporaryDirectory() as temp_dir:
                temp_path = Path(temp_dir)

                # ì‘ì€ íŒŒì¼ë“¤ë§Œ ë‹¤ìš´ë¡œë“œ
                for file_key, _ in small_files:
                    self._download_small_file(file_key, temp_path)

                # ë©”íƒ€ë°ì´í„° ìƒì„±
                self._create_and_upload_metadata(
                    temp_path, target_prefix, wmtp_type, model_info
                )

                # ì‘ì€ íŒŒì¼ë“¤ ì—…ë¡œë“œ
                self._upload_small_files(temp_path, target_prefix)

            console.print(
                f"\n[bold green]âœ… í‘œì¤€í™” ì™„ë£Œ! (ë‹¤ìš´ë¡œë“œ ì—†ì´ ì²˜ë¦¬)[/bold green]"
            )
            return True

        except Exception as e:
            console.print(f"\n[red]âŒ í‘œì¤€í™” ì‹¤íŒ¨: {e}[/red]")
            return False

    def _list_s3_files(self, prefix: str) -> list[tuple[str, int]]:
        """S3 íŒŒì¼ ëª©ë¡ ì¡°íšŒ"""
        files = []
        paginator = self.s3_client.get_paginator("list_objects_v2")

        for page in paginator.paginate(Bucket=self.bucket, Prefix=prefix):
            if "Contents" not in page:
                continue
            for obj in page["Contents"]:
                if not obj["Key"].endswith("/"):  # ë””ë ‰í† ë¦¬ ì œì™¸
                    files.append((obj["Key"], obj["Size"]))

        return files

    def _copy_large_files_s3_to_s3(
        self, files: list[tuple[str, int]], source_prefix: str, target_prefix: str
    ):
        """S3 ë‚´ë¶€ì—ì„œ ëŒ€ìš©ëŸ‰ íŒŒì¼ ì§ì ‘ ë³µì‚¬ (ë‹¤ìš´ë¡œë“œ ì—†ìŒ)"""
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            console=console,
        ) as progress:
            for file_key, size in files:
                file_name = file_key.split("/")[-1]
                relative_path = file_key.replace(source_prefix, "").lstrip("/")
                target_key = f"{target_prefix}/{relative_path}"

                # ëŒ€ìš©ëŸ‰ íŒŒì¼ ì´ë¦„ ë³€í™˜ (í•„ìš”ì‹œ)
                if "pytorch_model" in file_name and file_name.endswith(".bin"):
                    # safetensorsë¡œ ë³€í™˜ì€ ë‚˜ì¤‘ì— ë³„ë„ ì²˜ë¦¬
                    # ì§€ê¸ˆì€ ê·¸ëŒ€ë¡œ ë³µì‚¬
                    pass

                task = progress.add_task(
                    f"ë³µì‚¬ ì¤‘: {file_name} ({size / (1024**2):.1f}MB)"
                )

                # S3 ë‚´ë¶€ ë³µì‚¬ (ì„œë²„ì‚¬ì´ë“œ, ë§¤ìš° ë¹ ë¦„)
                copy_source = {"Bucket": self.bucket, "Key": file_key}

                try:
                    # 5GB ì´ìƒì€ ë©€í‹°íŒŒíŠ¸ ë³µì‚¬
                    if size > 5 * 1024**3:
                        self._multipart_copy(copy_source, target_key, size)
                    else:
                        self.s3_client.copy_object(
                            CopySource=copy_source,
                            Bucket=self.bucket,
                            Key=target_key,
                        )

                    progress.update(task, completed=100)
                    console.print(f"  âœ… {file_name} ë³µì‚¬ ì™„ë£Œ")

                except Exception as e:
                    console.print(f"  âŒ {file_name} ë³µì‚¬ ì‹¤íŒ¨: {e}")

    def _multipart_copy(self, copy_source: dict, target_key: str, size: int):
        """5GB ì´ìƒ íŒŒì¼ì˜ ë©€í‹°íŒŒíŠ¸ ë³µì‚¬"""
        # ë©€í‹°íŒŒíŠ¸ ì—…ë¡œë“œ ì‹œì‘
        mpu = self.s3_client.create_multipart_upload(
            Bucket=self.bucket, Key=target_key
        )
        mpu_id = mpu["UploadId"]

        # 500MB ì²­í¬ë¡œ ë¶„í• 
        chunk_size = 500 * 1024 * 1024
        parts = []

        try:
            for i, offset in enumerate(
                range(0, size, chunk_size), 1
            ):
                end_byte = min(offset + chunk_size - 1, size - 1)

                part = self.s3_client.upload_part_copy(
                    Bucket=self.bucket,
                    Key=target_key,
                    CopySource=copy_source,
                    CopySourceRange=f"bytes={offset}-{end_byte}",
                    PartNumber=i,
                    UploadId=mpu_id,
                )

                parts.append({
                    "ETag": part["CopyPartResult"]["ETag"],
                    "PartNumber": i
                })

            # ë©€í‹°íŒŒíŠ¸ ì™„ë£Œ
            self.s3_client.complete_multipart_upload(
                Bucket=self.bucket,
                Key=target_key,
                UploadId=mpu_id,
                MultipartUpload={"Parts": parts},
            )

        except Exception as e:
            # ì‹¤íŒ¨ì‹œ ì •ë¦¬
            self.s3_client.abort_multipart_upload(
                Bucket=self.bucket, Key=target_key, UploadId=mpu_id
            )
            raise e

    def _download_small_file(self, file_key: str, local_path: Path):
        """ì‘ì€ íŒŒì¼ë§Œ ë‹¤ìš´ë¡œë“œ"""
        file_name = file_key.split("/")[-1]
        local_file = local_path / file_name

        try:
            self.s3_client.download_file(self.bucket, file_key, str(local_file))
        except Exception as e:
            console.print(f"[yellow]âš ï¸ {file_name} ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}[/yellow]")

    def _create_and_upload_metadata(
        self,
        temp_path: Path,
        target_prefix: str,
        wmtp_type: str,
        model_info: dict[str, Any],
    ):
        """ë©”íƒ€ë°ì´í„° ìƒì„± ë° ì—…ë¡œë“œ"""

        # loading_strategy ê²°ì •
        loading_strategy = self._determine_loading_strategy(wmtp_type, model_info)

        # algorithm_compatibility ê²°ì •
        algorithm_compatibility = self._determine_algorithm_compatibility(
            wmtp_type, model_info
        )

        # metadata.json ìƒì„±
        metadata = {
            "wmtp_type": wmtp_type,
            "training_algorithm": model_info.get("training_algorithm", "unknown"),
            "horizon": model_info.get("horizon", 4),
            "n_heads": model_info.get("n_heads", 4),
            "base_architecture": model_info.get("architecture", "unknown"),
            "model_size": model_info.get("size", "unknown"),
            "storage_version": "2.0",
            "created_by": "s3_inplace_standardizer",
            "standardization_date": datetime.now().isoformat(),
            "original_format": model_info.get("original_format", "unknown"),
            "source_path": f"s3://{self.bucket}/{target_prefix}",
            "loading_strategy": loading_strategy,
            "algorithm_compatibility": algorithm_compatibility,
        }

        # ë¡œì»¬ì— ì €ì¥
        metadata_file = temp_path / "metadata.json"
        with open(metadata_file, "w") as f:
            json.dump(metadata, f, indent=2)

        # S3ì— ì—…ë¡œë“œ
        self.s3_client.upload_file(
            str(metadata_file),
            self.bucket,
            f"{target_prefix}/metadata.json",
        )

        # MLflow íŒŒì¼ë“¤ ìƒì„± ë° ì—…ë¡œë“œ
        self._create_mlflow_files(temp_path, target_prefix)

        console.print("âœ… ë©”íƒ€ë°ì´í„° ìƒì„± ë° ì—…ë¡œë“œ ì™„ë£Œ")

    def _determine_loading_strategy(
        self, wmtp_type: str, model_info: dict[str, Any]
    ) -> dict[str, Any]:
        """ë¡œë”© ì „ëµ ê²°ì •"""
        is_mtp = (
            model_info.get("training_algorithm") == "mtp" or wmtp_type == "mtp_native"
        )

        if is_mtp:
            return {
                "loader_type": "custom_mtp",
                "model_class_name": "GPTMTPForCausalLM",
                "custom_module_file": "modeling.py",
                "transformers_class": None,
                "state_dict_mapping": {
                    "remove_prefix": "base_model." if wmtp_type == "mtp_native" else None,
                    "add_prefix": None,
                    "key_transforms": {},
                },
                "required_files": [
                    "config.json",
                    "model.safetensors",
                    "modeling.py",
                    "metadata.json",
                ],
            }
        else:
            transformers_class = (
                "AutoModel" if wmtp_type == "reward_model" else "AutoModelForCausalLM"
            )
            return {
                "loader_type": "huggingface",
                "model_class_name": None,
                "custom_module_file": None,
                "transformers_class": transformers_class,
                "state_dict_mapping": {
                    "remove_prefix": None,
                    "add_prefix": None,
                    "key_transforms": {},
                },
                "required_files": ["config.json", "model.safetensors", "metadata.json"],
            }

    def _determine_algorithm_compatibility(
        self, wmtp_type: str, model_info: dict[str, Any]
    ) -> list[str]:
        """ì•Œê³ ë¦¬ì¦˜ í˜¸í™˜ì„± ê²°ì •"""
        training_algo = model_info.get("training_algorithm", "")

        if training_algo == "mtp" or wmtp_type == "mtp_native":
            return ["baseline-mtp", "critic-wmtp", "rho1-wmtp"]
        elif wmtp_type == "reward_model" or training_algo == "reward_modeling":
            return ["critic-wmtp", "rho1-wmtp"]
        elif wmtp_type in ["base_model", "reference_model"]:
            if training_algo == "sheared_training":
                return ["baseline-mtp", "rho1-wmtp"]
            return ["rho1-wmtp"]

        return ["rho1-wmtp"]

    def _create_mlflow_files(self, temp_path: Path, target_prefix: str):
        """MLflow íŒŒì¼ ìƒì„± ë° ì—…ë¡œë“œ"""
        # MLmodel
        mlmodel_content = {
            "artifact_path": "model",
            "flavors": {
                "python_function": {
                    "env": {
                        "conda": "conda.yaml",
                        "virtualenv": "python_env.yaml",
                    },
                    "loader_module": "mlflow.transformers",
                    "model_path": "model",
                    "python_version": "3.11.7",
                },
                "transformers": {
                    "code": None,
                    "framework": "pt",
                    "model_path": "model",
                    "task": "text-generation",
                },
            },
            "mlflow_version": "2.15.1",
            "model_size_bytes": 0,  # ë‚˜ì¤‘ì— ì±„ì›€
            "model_uuid": "12345678901234567890123456789013",
            "run_id": None,
            "signature": {
                "inputs": '[{"type": "string"}]',
                "outputs": '[{"type": "string"}]',
            },
            "utc_time_created": datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f"),
        }

        mlmodel_file = temp_path / "MLmodel"
        with open(mlmodel_file, "w") as f:
            yaml.dump(mlmodel_content, f, default_flow_style=False, sort_keys=False)

        # requirements.txt
        requirements_file = temp_path / "requirements.txt"
        with open(requirements_file, "w") as f:
            f.write("transformers\ntorch")

        # conda.yaml
        conda_env = {
            "name": "wmtp_env",
            "channels": ["defaults"],
            "dependencies": ["python=3.11.7", {"pip": ["transformers", "torch"]}],
        }

        conda_file = temp_path / "conda.yaml"
        with open(conda_file, "w") as f:
            yaml.dump(conda_env, f, default_flow_style=False, sort_keys=False)

        # S3ì— ì—…ë¡œë“œ
        for file_name in ["MLmodel", "requirements.txt", "conda.yaml"]:
            local_file = temp_path / file_name
            if local_file.exists():
                self.s3_client.upload_file(
                    str(local_file),
                    self.bucket,
                    f"{target_prefix}/mlflow_model/{file_name}",
                )

    def _upload_small_files(self, local_path: Path, target_prefix: str):
        """ì‘ì€ ì„¤ì • íŒŒì¼ë“¤ ì—…ë¡œë“œ"""
        for file_path in local_path.glob("*"):
            if file_path.is_file() and file_path.name not in [
                "metadata.json",
                "MLmodel",
                "requirements.txt",
                "conda.yaml",
            ]:
                self.s3_client.upload_file(
                    str(file_path),
                    self.bucket,
                    f"{target_prefix}/{file_path.name}",
                )


def process_sheared_llama():
    """Sheared-LLaMA ëª¨ë¸ ì²˜ë¦¬ (ë‹¤ìš´ë¡œë“œ ì—†ìŒ)"""
    console.print(
        "[bold blue]ğŸš€ Sheared-LLaMA S3 In-Place í‘œì¤€í™”[/bold blue]\n"
    )

    # í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
    load_dotenv()

    standardizer = S3InPlaceStandardizer()

    model_info = {
        "architecture": "llama",
        "size": "2.7b",
        "training_algorithm": "sheared_training",
        "original_format": "sharded_pytorch_bins",
    }

    success = standardizer.standardize_model(
        source_prefix="models/Sheared-LLaMA-2.7B",
        target_prefix="models_v2/sheared-llama-2.7b",
        wmtp_type="base_model",
        model_info=model_info,
    )

    if success:
        console.print("\n[bold green]âœ¨ ì™„ë£Œ! ë‹¤ìš´ë¡œë“œ ì—†ì´ S3ì—ì„œ ì²˜ë¦¬ë¨[/bold green]")
    else:
        console.print("\n[red]âŒ í‘œì¤€í™” ì‹¤íŒ¨[/red]")


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    load_dotenv()

    # ì²˜ë¦¬í•  ëª¨ë¸ë“¤
    models = [
        {
            "source": "models/Sheared-LLaMA-2.7B",
            "target": "models_v2/sheared-llama-2.7b",
            "wmtp_type": "base_model",
            "info": {
                "architecture": "llama",
                "size": "2.7b",
                "training_algorithm": "sheared_training",
                "original_format": "sharded_pytorch_bins",
            },
        },
        {
            "source": "models/7b_1t_4",
            "target": "models_v2/llama-7b-mtp",
            "wmtp_type": "mtp_native",
            "info": {
                "architecture": "llama",
                "size": "7b",
                "training_algorithm": "mtp",
                "horizon": 4,
                "n_heads": 4,
                "original_format": "consolidated_pth",
            },
        },
        {
            "source": "models/Starling-RM-7B-alpha",
            "target": "models_v2/starling-rm-7b",
            "wmtp_type": "reward_model",
            "info": {
                "architecture": "llama",
                "size": "7b",
                "training_algorithm": "reward_modeling",
                "original_format": "pytorch_bin",
            },
        },
    ]

    standardizer = S3InPlaceStandardizer()

    console.print(f"[bold blue]ğŸš€ {len(models)}ê°œ ëª¨ë¸ ì²˜ë¦¬ (ë‹¤ìš´ë¡œë“œ ì—†ìŒ)[/bold blue]\n")

    for i, model in enumerate(models, 1):
        console.print(f"\n[cyan]â”â”â” ëª¨ë¸ {i}/{len(models)} â”â”â”[/cyan]")

        success = standardizer.standardize_model(
            source_prefix=model["source"],
            target_prefix=model["target"],
            wmtp_type=model["wmtp_type"],
            model_info=model["info"],
        )

        if not success:
            console.print(f"[red]ëª¨ë¸ ì²˜ë¦¬ ì‹¤íŒ¨: {model['source']}[/red]")

    console.print("\n[bold green]ğŸ‰ ëª¨ë“  ëª¨ë¸ ì²˜ë¦¬ ì™„ë£Œ![/bold green]")


if __name__ == "__main__":
    import sys

    if len(sys.argv) > 1 and sys.argv[1] == "sheared":
        process_sheared_llama()
    else:
        main()